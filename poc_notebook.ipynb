{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572111a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb6d73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 13:01:17,771 - INFO - \n",
      "--- Starting Pre-Filter for Document Extraction ---\n",
      "2025-07-29 13:01:17,789 - INFO - --- Scaleway OpenAI client initialized successfully ---\n",
      "2025-07-29 13:01:17,790 - INFO - --- Loading documents from './input_documents' ---\n",
      "2025-07-29 13:01:17,801 - INFO - Loaded DOCX file: ./input_documents/Design Document.docx\n",
      "2025-07-29 13:01:17,802 - INFO - --- Prompt sent to LLM ---\n",
      "\n",
      "You are a senior cybersecurity analyst specializing in threat modeling. Your task is to extract structured information from one or more input documents describing a system and transform it into a comprehensive and accurate JSON object representing a Data Flow Diagram (DFD).\n",
      "\n",
      "Your analysis must be meticulous. Follow these reasoning steps precisely:\n",
      "\n",
      "1.  **Identify Core Components**: First, perform a full scan of the document(s) to identify and list all high-level components. This includes:\n",
      "    * `project_name`, `project_version`, and `industry_context`.\n",
      "    * `external_entities`: Any user, actor, or system outside the primary application boundary.\n",
      "    * `processes`: The distinct computational components or services that handle data.\n",
      "    * `assets`: The data stores, such as databases, object storage buckets, or message queues.\n",
      "    * `trust_boundaries`: The defined boundaries separating zones of different trust levels.\n",
      "\n",
      "2.  **Systematically Extract ALL Data Flows**: This is the most critical step. You must identify every single flow of data mentioned or implied in the documents. Scrutinize sections like \"Use Cases,\" \"Data Flow Diagrams,\" \"Architecture,\" and \"Technology Stack\" to find them. Create a data flow entry for each of the following interaction types:\n",
      "    * **External-to-Process**: Flows from an `external_entity` to an internal `process` (e.g., user submitting credentials, uploading a file).\n",
      "    * **Process-to-External**: Flows from an internal `process` to an `external_entity` (e.g., returning results, sending a session token).\n",
      "    * **Process-to-Asset**: Flows where a `process` reads from or writes to a data store `asset` (e.g., \"Authentication Service reads from UsersDB,\" \"Analysis process writes to ResultsDB\"). These are essential and must not be omitted.\n",
      "    * **Process-to-Process**: Flows between internal `processes` (e.g., \"API Gateway routes request to Authentication Service\").\n",
      "\n",
      "3.  **Detail and Classify Each Flow**: For every data flow you identify, you must accurately populate all its attributes: `source`, `destination`, `data_description`, `data_classification`, `protocol`, and `authentication_mechanism`.\n",
      "    * **Data Classification Rules**: Apply strict classification.\n",
      "        * **Confidential**: Use for any data that, if exposed, could harm the organization or its users. This includes, but is not limited to: credentials, session tokens (JWTs), API keys, SAML assertions, Personally Identifiable Information (PII), health information (PHI), financial data, and proprietary business logic.\n",
      "        * **Public**: Use ONLY for data that is explicitly intended for public consumption and carries no security risk if intercepted (e.g., a list of available public APIs). **Authentication-related data is never public.**\n",
      "    * If information for a field (like `protocol` or `authentication_mechanism`) is not explicitly stated, infer it from the context (e.g., a web service likely uses HTTPS, a database connection likely uses JDBC/ODBC over TLS) and make a note of this in the `assumptions` key in the metadata.\n",
      "\n",
      "\n",
      "Input Documents:\n",
      "---\n",
      "Design Document: HealthData Insights Platform\n",
      "This document outlines the design and architecture of the HealthData Insights platform. Its purpose is to provide a comprehensive overview for conducting a thorough threat modeling analysis.\n",
      "1. System Overview ðŸ©º\n",
      "HealthData Insights is a cloud-based Software-as-a-Service (SaaS) platform designed for medical researchers. It allows registered research institutions to upload anonymized patient datasets, process them using built-in analysis tools, and collaborate on findings. The system prioritizes data security, integrity, and compliance with regulations like GDPR and HIPAA.\n",
      "Core functionalities:\n",
      "User and organization management.\n",
      "Secure, encrypted upload and storage of large datasets (e.g., genomic data, clinical trial results).\n",
      "A suite of data analysis tools (e.g., statistical analysis, machine learning model training).\n",
      "Collaboration features for sharing analysis results within a research organization.\n",
      "2. Key Use Cases\n",
      "UC-1: Researcher Registration & Authentication: A new Researcher from a subscribed institution registers for an account. The system verifies their institutional email. Subsequent logins are handled via a multi-factor authentication (MFA) process.\n",
      "UC-2: Data Upload: An authenticated Researcher uploads a large, anonymized dataset in CSV or JSON format from their local machine via a web interface. The data is transferred over HTTPS to the platform's ingestion service.\n",
      "UC-3: Data Processing: A Researcher initiates a data analysis job (e.g., \"Variant Calling\") on an uploaded dataset. A processing worker retrieves the data, performs the computation, and stores the results.\n",
      "UC-4: Results Viewing: A Researcher views the results of a completed analysis job through the web interface. The results are rendered as visualizations and downloadable reports.\n",
      "UC-5: Admin Role Management: An Organizational Admin (a type of Researcher) can invite or remove other researchers from their institution's account.\n",
      "3. Data Flow Diagrams (DFDs)\n",
      "DFD Level 0: Context Diagram\n",
      "A single process, HealthData Insights Platform (1.0), interacts with three external entities:\n",
      "Researcher: Sends login credentials, upload data, and analysis requests. Receives session tokens and analysis results.\n",
      "Organizational Admin: Sends user management requests. Receives confirmation.\n",
      "External Identity Provider (IdP): Interacts with the platform for SAML-based Single Sign-On (SSO) for enterprise clients.\n",
      "DFD Level 1: High-Level Process Breakdown\n",
      "Process 1.1: User Authentication:\n",
      "Input: Credentials from Researcher.\n",
      "Output: Session Token to Researcher.\n",
      "Data Store: Interacts with UsersDB.\n",
      "Process 1.2: Data Ingestion:\n",
      "Input: Anonymized Dataset from Researcher.\n",
      "Output: Dataset write confirmation.\n",
      "Data Store: Writes to RawDataBucket.\n",
      "Process 1.3: Data Analysis:\n",
      "Input: Analysis Job Request from Researcher.\n",
      "Output: Job status update.\n",
      "Data Stores: Reads from RawDataBucket, writes to ProcessedDataBucket and ResultsDB.\n",
      "Process 1.4: Results Presentation:\n",
      "Input: View Results Request from Researcher.\n",
      "Output: Formatted Results to Researcher.\n",
      "Data Store: Reads from ResultsDB.\n",
      "4. Architecture Views ðŸ—ï¸\n",
      "Logical View\n",
      "The system is composed of several microservices orchestrated via an API Gateway.\n",
      "Frontend Web App: A Single Page Application (SPA) built with React that runs in the user's browser. It communicates with the backend via the API Gateway.\n",
      "API Gateway: A central entry point for all client requests. It handles routing, rate limiting, and initial authentication token validation.\n",
      "Authentication Service: Manages user registration, login, MFA, and session token generation (JWTs).\n",
      "Data Ingestion Service: Handles large file uploads, validates file formats, and places data into a temporary storage queue.\n",
      "Analysis Orchestrator Service: Manages the lifecycle of analysis jobs. It listens for new job requests, queues them, and triggers processing workers.\n",
      "Processing Workers: A pool of containerized applications that perform the actual computation. They are ephemeral and scale based on demand. They fetch data from storage, process it, and write the results back.\n",
      "Results Service: Provides endpoints for querying and retrieving processed results and metadata.\n",
      "Physical / Deployment View\n",
      "The entire platform is deployed on Amazon Web Services (AWS) within a single region (eu-central-1), distributed across multiple Availability Zones (AZs) for high availability.\n",
      "VPC (Virtual Private Cloud): The core network boundary.\n",
      "Public Subnet: Contains the Application Load Balancer (ALB), which acts as the public-facing entry point.\n",
      "Private Subnets: Contain all backend services and databases. These services do not have public IP addresses and can only be accessed through the ALB or via internal networking.\n",
      "Compute:\n",
      "Services like the API Gateway, Authentication, Orchestrator, and Results are deployed as containers on AWS Fargate.\n",
      "Processing Workers are also run as Fargate tasks, scaled independently by an SQS queue length metric.\n",
      "Data Stores:\n",
      "UsersDB: An Amazon RDS for PostgreSQL instance (Multi-AZ).\n",
      "RawDataBucket & ProcessedDataBucket: Amazon S3 buckets with encryption at rest (SSE-S3) enabled, versioning, and strict bucket policies. Access is granted via IAM roles.\n",
      "ResultsDB: An Amazon DynamoDB table for storing analysis metadata and results.\n",
      "Networking:\n",
      "All traffic from the internet enters through the ALB, which terminates TLS (HTTPS).\n",
      "Communication between services inside the VPC is done over the internal network.\n",
      "AWS WAF (Web Application Firewall) is configured on the ALB to protect against common web exploits.\n",
      "5. Technology Stack ðŸ’»\n",
      "Export to Sheets\n",
      "6. Trust Boundaries & External Dependencies\n",
      "Trust Boundaries\n",
      "External User Zone -> Application Frontend: The primary trust boundary. All input from the Researcher's browser is untrusted.\n",
      "Internet -> VPC Public Subnet: Traffic entering the ALB. Protected by AWS WAF.\n",
      "VPC Public Subnet -> VPC Private Subnet: All traffic to backend services is routed from the ALB and is considered more trusted, but still requires service-to-service authentication (e.g., via IAM roles).\n",
      "Application -> Data Stores: The boundary between compute services and persistent storage. Access is controlled via strict IAM roles and database credentials.\n",
      "External Dependencies\n",
      "Third-Party Identity Provider (IdP): For enterprise customers using SSO, the platform trusts the SAML assertions provided by the customer's IdP (e.g., Azure AD, Okta). The connection and assertion signatures must be secure.\n",
      "Email Service (e.g., Amazon SES): Used for sending verification and notification emails. The platform trusts this service to deliver emails securely.\n",
      "NPM / PyPI Registries: The software supply chain for third-party libraries used in development is an external dependency.\n",
      "---\n",
      "\n",
      "4.  **Final Review**: Before generating the final output, review the generated list of data flows against the use cases in the source document. Ensure that every major action described in the use cases is represented by one or more data flows in your output.\n",
      "\n",
      "\n",
      "Output ONLY the JSON, with no additional commentary or formatting.\n",
      "\n",
      "2025-07-29 13:01:45,747 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-29 13:02:20,876 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-29 13:02:20,878 - INFO - --- Raw Scaleway Response ---\n",
      "{\"project_name\": \"HealthData Insights Platform\", \"project_version\": \"1.0\", \"industry_context\": \"Medical Research\", \"external_entities\": [\"Researcher\", \"Organizational Admin\", \"External Identity Provider (IdP)\"], \"processes\": [\"User Authentication\", \"Data Ingestion\", \"Data Analysis\", \"Results Presentation\", \"API Gateway\", \"Authentication Service\", \"Data Ingestion Service\", \"Analysis Orchestrator Service\", \"Processing Workers\", \"Results Service\"], \"assets\": [\"UsersDB\", \"RawDataBucket\", \"ProcessedDataBucket\", \"ResultsDB\"], \"trust_boundaries\": [\"External User Zone -> Application Frontend\", \"Internet -> VPC Public Subnet\", \"VPC Public Subnet -> VPC Private Subnet\", \"Application -> Data Stores\"], \"data_flows\": [{\"source\": \"Researcher\", \"destination\": \"User Authentication\", \"data_description\": \"Login credentials\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"MFA\"}, {\"source\": \"User Authentication\", \"destination\": \"Researcher\", \"data_description\": \"Session token\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"JWT\"}, {\"source\": \"Researcher\", \"destination\": \"Data Ingestion\", \"data_description\": \"Anonymized dataset\", \"data_classification\": \"Public\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"Session token\"}, {\"source\": \"Data Ingestion\", \"destination\": \"RawDataBucket\", \"data_description\": \"Uploaded dataset\", \"data_classification\": \"Public\", \"protocol\": \"Internal network\", \"authentication_mechanism\": \"IAM roles\"}, {\"source\": \"Researcher\", \"destination\": \"Data Analysis\", \"data_description\": \"Analysis job request\", \"data_classification\": \"Public\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"Session token\"}, {\"source\": \"Data Analysis\", \"destination\": \"RawDataBucket\", \"data_description\": \"Input data for analysis\", \"data_classification\": \"Public\", \"protocol\": \"Internal network\", \"authentication_mechanism\": \"IAM roles\"}, {\"source\": \"Data Analysis\", \"destination\": \"ProcessedDataBucket\", \"data_description\": \"Processed data\", \"data_classification\": \"Public\", \"protocol\": \"Internal network\", \"authentication_mechanism\": \"IAM roles\"}, {\"source\": \"Data Analysis\", \"destination\": \"ResultsDB\", \"data_description\": \"Analysis results\", \"data_classification\": \"Public\", \"protocol\": \"Internal network\", \"authentication_mechanism\": \"IAM roles\"}, {\"source\": \"Researcher\", \"destination\": \"Results Presentation\", \"data_description\": \"View results request\", \"data_classification\": \"Public\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"Session token\"}, {\"source\": \"Results Presentation\", \"destination\": \"ResultsDB\", \"data_description\": \"Results data\", \"data_classification\": \"Public\", \"protocol\": \"Internal network\", \"authentication_mechanism\": \"IAM roles\"}, {\"source\": \"Results Presentation\", \"destination\": \"Researcher\", \"data_description\": \"Formatted results\", \"data_classification\": \"Public\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"Session token\"}, {\"source\": \"Organizational Admin\", \"destination\": \"User Authentication\", \"data_description\": \"User management requests\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"MFA\"}, {\"source\": \"User Authentication\", \"destination\": \"Organizational Admin\", \"data_description\": \"Confirmation\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"JWT\"}, {\"source\": \"External Identity Provider (IdP)\", \"destination\": \"User Authentication\", \"data_description\": \"SAML assertions\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"SAML\"}]}\n",
      "2025-07-29 13:02:20,879 - INFO - --- Token Usage for Scaleway ---\n",
      "2025-07-29 13:02:20,880 - INFO - Input Tokens: 2187\n",
      "2025-07-29 13:02:20,881 - INFO - Output Tokens: 829\n",
      "2025-07-29 13:02:20,882 - INFO - Total Tokens: 3016\n",
      "2025-07-29 13:02:20,885 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-29 13:02:20,886 - INFO - \n",
      "--- LLM Output (DFD Components) ---\n",
      "2025-07-29 13:02:20,887 - INFO - \n",
      "--- DFD components successfully saved to './output/dfd_components.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dfd\": {\n",
      "    \"project_name\": \"HealthData Insights Platform\",\n",
      "    \"project_version\": \"1.0\",\n",
      "    \"industry_context\": \"Healthcare\",\n",
      "    \"external_entities\": [\n",
      "      \"Researcher\",\n",
      "      \"Organizational Admin\",\n",
      "      \"External Identity Provider\"\n",
      "    ],\n",
      "    \"assets\": [\n",
      "      \"UsersDB\",\n",
      "      \"RawDataBucket\",\n",
      "      \"ProcessedDataBucket\",\n",
      "      \"ResultsDB\"\n",
      "    ],\n",
      "    \"processes\": [\n",
      "      \"User Authentication\",\n",
      "      \"Data Ingestion\",\n",
      "      \"Data Analysis\",\n",
      "      \"Results Presentation\"\n",
      "    ],\n",
      "    \"trust_boundaries\": [\n",
      "      \"External User Zone -> Application Frontend\",\n",
      "      \"Internet -> VPC Public Subnet\",\n",
      "      \"VPC Public Subnet -> VPC Private Subnet\",\n",
      "      \"Application -> Data Stores\"\n",
      "    ],\n",
      "    \"data_flows\": [\n",
      "      {\n",
      "        \"source\": \"Researcher\",\n",
      "        \"destination\": \"User Authentication\",\n",
      "        \"data_description\": \"Login credentials\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"MFA\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"User Authentication\",\n",
      "        \"destination\": \"Researcher\",\n",
      "        \"data_description\": \"Session token\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"Session Token\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Researcher\",\n",
      "        \"destination\": \"Data Ingestion\",\n",
      "        \"data_description\": \"Anonymized dataset\",\n",
      "        \"data_classification\": \"Public\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Data Ingestion\",\n",
      "        \"destination\": \"RawDataBucket\",\n",
      "        \"data_description\": \"Dataset\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"Internal Network\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Researcher\",\n",
      "        \"destination\": \"Data Analysis\",\n",
      "        \"data_description\": \"Analysis job request\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"Session Token\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"RawDataBucket\",\n",
      "        \"destination\": \"Data Analysis\",\n",
      "        \"data_description\": \"Dataset\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"Internal Network\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Data Analysis\",\n",
      "        \"destination\": \"ProcessedDataBucket\",\n",
      "        \"data_description\": \"Results\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"Internal Network\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Data Analysis\",\n",
      "        \"destination\": \"ResultsDB\",\n",
      "        \"data_description\": \"Results\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"Internal Network\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Researcher\",\n",
      "        \"destination\": \"Results Presentation\",\n",
      "        \"data_description\": \"View results request\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"Session Token\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"ResultsDB\",\n",
      "        \"destination\": \"Results Presentation\",\n",
      "        \"data_description\": \"Results\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"Internal Network\",\n",
      "        \"authentication_mechanism\": \"None\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"Results Presentation\",\n",
      "        \"destination\": \"Researcher\",\n",
      "        \"data_description\": \"Formatted results\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"Session Token\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-29T13:02:20.883568\",\n",
      "    \"source_documents\": [\n",
      "      \"./input_documents/Design Document.docx\"\n",
      "    ],\n",
      "    \"assumptions\": [],\n",
      "    \"llm_provider\": \"scaleway\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client\n",
    "import PyPDF2\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "import docx  # Added for DOCX support\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"scaleway\").lower()  # Default to 'ollama', can be set to 'scaleway'\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama-3.3-70b-instruct\")\n",
    "SCW_API_URL = os.getenv(\"SCW_API_URL\", \"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\")\n",
    "SCW_SECRET_KEY = os.getenv(\"SCW_SECRET_KEY\")\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./input_documents\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "DFD_OUTPUT_PATH = os.getenv(\"DFD_OUTPUT_PATH\", os.path.join(OUTPUT_DIR, \"dfd_components.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Initialize LLM Client ---\n",
    "def initialize_llm_client():\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        if not SCW_SECRET_KEY:\n",
    "            raise ValueError(\"SCW_SECRET_KEY environment variable is required for Scaleway API.\")\n",
    "        try:\n",
    "            client = instructor.from_openai(OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY))\n",
    "            logger.info(\"--- Scaleway OpenAI client initialized successfully ---\")\n",
    "            return client, \"scaleway\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Scaleway client: {e} ---\")\n",
    "            raise\n",
    "    else:  # Default to Ollama\n",
    "        try:\n",
    "            raw_client = Client()  # Raw Ollama client for debugging\n",
    "            # Patch the Ollama client with instructor for structured output\n",
    "            instructor_client = instructor.patch(Client())\n",
    "            logger.info(\"--- Ollama client initialized successfully ---\")\n",
    "            return raw_client, instructor_client, \"ollama\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Ollama client: {e} ---\")\n",
    "            raise\n",
    "\n",
    "# --- DFD Schema for Validation ---\n",
    "class DataFlow(BaseModel):\n",
    "    source: str = Field(description=\"Source component of the data flow (e.g., 'U' for User).\")\n",
    "    destination: str = Field(description=\"Destination component of the data flow (e.g., 'CDN').\")\n",
    "    data_description: str = Field(description=\"Description of data being transferred (e.g., 'User session tokens').\")\n",
    "    data_classification: str = Field(description=\"Classification like 'Confidential', 'PII', or 'Public'.\")\n",
    "    protocol: str = Field(description=\"Protocol used (e.g., 'HTTPS', 'JDBC/ODBC over TLS').\")\n",
    "    authentication_mechanism: str = Field(description=\"Authentication method (e.g., 'JWT in Header').\")\n",
    "\n",
    "class DFDComponents(BaseModel):\n",
    "    project_name: str = Field(description=\"Name of the project (e.g., 'Web Application Security Model').\")\n",
    "    project_version: str = Field(description=\"Version of the project (e.g., '1.1').\")\n",
    "    industry_context: str = Field(description=\"Industry context (e.g., 'Finance').\")\n",
    "    external_entities: list[str] = Field(description=\"List of external entities (e.g., ['U', 'Attacker']).\")\n",
    "    assets: list[str] = Field(description=\"List of assets like data stores (e.g., ['DB_P', 'DB_B']).\")\n",
    "    processes: list[str] = Field(description=\"List of processes (e.g., ['CDN', 'LB', 'WS']).\")\n",
    "    trust_boundaries: list[str] = Field(description=\"List of trust boundaries (e.g., ['Public Zone to Edge Zone']).\")\n",
    "    data_flows: list[DataFlow] = Field(description=\"List of data flows between components.\")\n",
    "\n",
    "class DFDOutput(BaseModel):\n",
    "    dfd: DFDComponents\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample Input for Testing (if no documents are found) ---\n",
    "SAMPLE_DOCUMENT_CONTENT = \"\"\"\n",
    "System: Web Application Security Model, Version 1.1, Finance Industry\n",
    "External Entities: User (U), External Attacker\n",
    "Assets: Profile Database (DB_P), Billing Database (DB_B)\n",
    "Processes: Content Delivery Network (CDN), Load Balancer (LB), Web Server (WS), Message Queue (MQ), Worker (WRK), Admin Service (ADM), Admin Portal (ADM_P)\n",
    "Trust Boundaries: Public Zone to Edge Zone, Edge Zone to Application DMZ, Application DMZ to Internal Core, Internal Core to Data Zone, Management Zone to Application DMZ\n",
    "Data Flows:\n",
    "- From User to CDN: User session tokens and requests for static assets, Confidential, HTTPS, JWT in Header\n",
    "- From CDN to LB: Cached content and user requests, Confidential, HTTPS, mTLS\n",
    "- From WS to DB_P: User profile data including names and email addresses, PII, JDBC/ODBC over TLS, Database Credentials from Secrets Manager\n",
    "\"\"\"\n",
    "\n",
    "# --- Load and Parse Documents ---\n",
    "def load_documents(input_dir):\n",
    "    logger.info(f\"--- Loading documents from '{input_dir}' ---\")\n",
    "    documents = []\n",
    "    # Expanded glob patterns to include more file types: TXT, PDF, DOCX, MD\n",
    "    file_patterns = [\n",
    "        \"*.[tT][xX][tT]\",      # TXT files (case-insensitive)\n",
    "        \"*.[pP][dD][fF]\",      # PDF files\n",
    "        \"*.[dD][oO][cC][xX]\",  # DOCX files\n",
    "        \"*.[mM][dD]\"           # Markdown files (optional addition for more types)\n",
    "    ]\n",
    "    all_files = []\n",
    "    for pattern in file_patterns:\n",
    "        all_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            ext = os.path.splitext(file_path)[1].lower()\n",
    "            if ext == \".txt\" or ext == \".md\":  # Treat MD like TXT\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                logger.info(f\"Loaded text-based file: {file_path}\")\n",
    "            elif ext == \".pdf\":\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\".join(page.extract_text() for page in pdf_reader.pages if page.extract_text())\n",
    "                    documents.append(text)\n",
    "                logger.info(f\"Loaded PDF file: {file_path}\")\n",
    "            elif ext == \".docx\":\n",
    "                doc = docx.Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
    "                documents.append(text)\n",
    "                logger.info(f\"Loaded DOCX file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {file_path}: {e}\")\n",
    "    if not documents:\n",
    "        logger.warning(\"--- No valid documents found. Using sample document content ---\")\n",
    "        documents = [SAMPLE_DOCUMENT_CONTENT]\n",
    "    return documents\n",
    "\n",
    "# --- Prompt Engineering for Document Extraction ---\n",
    "extract_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling. Your task is to extract structured information from one or more input documents describing a system and transform it into a comprehensive and accurate JSON object representing a Data Flow Diagram (DFD).\n",
    "\n",
    "Your analysis must be meticulous. Follow these reasoning steps precisely:\n",
    "\n",
    "1.  **Identify Core Components**: First, perform a full scan of the document(s) to identify and list all high-level components. This includes:\n",
    "    * `project_name`, `project_version`, and `industry_context`.\n",
    "    * `external_entities`: Any user, actor, or system outside the primary application boundary.\n",
    "    * `processes`: The distinct computational components or services that handle data.\n",
    "    * `assets`: The data stores, such as databases, object storage buckets, or message queues.\n",
    "    * `trust_boundaries`: The defined boundaries separating zones of different trust levels.\n",
    "\n",
    "2.  **Systematically Extract ALL Data Flows**: This is the most critical step. You must identify every single flow of data mentioned or implied in the documents. Scrutinize sections like \"Use Cases,\" \"Data Flow Diagrams,\" \"Architecture,\" and \"Technology Stack\" to find them. Create a data flow entry for each of the following interaction types:\n",
    "    * **External-to-Process**: Flows from an `external_entity` to an internal `process` (e.g., user submitting credentials, uploading a file).\n",
    "    * **Process-to-External**: Flows from an internal `process` to an `external_entity` (e.g., returning results, sending a session token).\n",
    "    * **Process-to-Asset**: Flows where a `process` reads from or writes to a data store `asset` (e.g., \"Authentication Service reads from UsersDB,\" \"Analysis process writes to ResultsDB\"). These are essential and must not be omitted.\n",
    "    * **Process-to-Process**: Flows between internal `processes` (e.g., \"API Gateway routes request to Authentication Service\").\n",
    "\n",
    "3.  **Detail and Classify Each Flow**: For every data flow you identify, you must accurately populate all its attributes: `source`, `destination`, `data_description`, `data_classification`, `protocol`, and `authentication_mechanism`.\n",
    "    * **Data Classification Rules**: Apply strict classification.\n",
    "        * **Confidential**: Use for any data that, if exposed, could harm the organization or its users. This includes, but is not limited to: credentials, session tokens (JWTs), API keys, SAML assertions, Personally Identifiable Information (PII), health information (PHI), financial data, and proprietary business logic.\n",
    "        * **Public**: Use ONLY for data that is explicitly intended for public consumption and carries no security risk if intercepted (e.g., a list of available public APIs). **Authentication-related data is never public.**\n",
    "    * If information for a field (like `protocol` or `authentication_mechanism`) is not explicitly stated, infer it from the context (e.g., a web service likely uses HTTPS, a database connection likely uses JDBC/ODBC over TLS) and make a note of this in the `assumptions` key in the metadata.\n",
    "\n",
    "\n",
    "Input Documents:\n",
    "---\n",
    "{documents}\n",
    "---\n",
    "\n",
    "4.  **Final Review**: Before generating the final output, review the generated list of data flows against the use cases in the source document. Ensure that every major action described in the use cases is represented by one or more data flows in your output.\n",
    "\n",
    "\n",
    "Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_template(extract_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Starting Pre-Filter for Document Extraction ---\")\n",
    "try:\n",
    "    # Initialize LLM client\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        client, client_type = initialize_llm_client()\n",
    "    else:\n",
    "        raw_client, instructor_client, client_type = initialize_llm_client()\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(INPUT_DIR)\n",
    "    documents_combined = \"\\n--- Document Separator ---\\n\".join(documents)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = extract_prompt.format_messages(documents=documents_combined)\n",
    "\n",
    "    # Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    if client_type == \"scaleway\":\n",
    "        # Use instructor client for Scaleway\n",
    "        dfd_obj = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_client = OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY)\n",
    "        raw_response = raw_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        logger.info(f\"--- Raw Scaleway Response ---\\n{raw_response.choices[0].message.content}\")\n",
    "        # Log token usage for Scaleway\n",
    "        if hasattr(raw_response, 'usage'):\n",
    "            prompt_tokens = raw_response.usage.prompt_tokens or 'N/A'\n",
    "            completion_tokens = raw_response.usage.completion_tokens or 'N/A'\n",
    "            total_tokens = raw_response.usage.total_tokens or 'N/A'\n",
    "            logger.info(f\"--- Token Usage for Scaleway ---\")\n",
    "            logger.info(f\"Input Tokens: {prompt_tokens}\")\n",
    "            logger.info(f\"Output Tokens: {completion_tokens}\")\n",
    "            logger.info(f\"Total Tokens: {total_tokens}\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        # Use instructor client for Ollama\n",
    "        dfd_obj = instructor_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_response = raw_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "        logger.info(f\"--- Raw Ollama Response ---\\n{raw_response['message']['content']}\")\n",
    "        # Log Token Count and Performance\n",
    "        prompt_tokens = raw_response.get('prompt_eval_count', 'N/A')\n",
    "        prompt_duration_ns = raw_response.get('prompt_eval_duration', 0)\n",
    "        response_tokens = raw_response.get('eval_count', 'N/A')\n",
    "        response_duration_ns = raw_response.get('eval_duration', 0)\n",
    "        prompt_duration_s = f\"{prompt_duration_ns / 1_000_000_000:.2f}s\" if prompt_duration_ns else \"N/A\"\n",
    "        response_duration_s = f\"{response_duration_ns / 1_000_000_000:.2f}s\" if response_duration_ns else \"N/A\"\n",
    "        logger.info(f\"--- Token Usage & Performance ---\")\n",
    "        logger.info(f\"Input Tokens: {prompt_tokens} (processed in {prompt_duration_s})\")\n",
    "        logger.info(f\"Output Tokens: {response_tokens} (generated in {response_duration_s})\")\n",
    "\n",
    "    dfd_dict = dfd_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    output_dict = {\n",
    "        \"dfd\": dfd_dict,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_documents\": glob.glob(os.path.join(INPUT_DIR, \"*.[tT][xX][tT]\")) + glob.glob(os.path.join(INPUT_DIR, \"*.[pP][dD][fF]\")) + glob.glob(os.path.join(INPUT_DIR, \"*.[dD][oO][cC][xX]\")) + glob.glob(os.path.join(INPUT_DIR, \"*.[mM][dD]\")),\n",
    "            \"assumptions\": [],\n",
    "            \"llm_provider\": LLM_PROVIDER\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema\n",
    "    try:\n",
    "        validated = DFDOutput(**output_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the DFD components to a file\n",
    "    with open(DFD_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(output_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (DFD Components) ---\")\n",
    "    print(json.dumps(output_dict, indent=2))\n",
    "    logger.info(f\"\\n--- DFD components successfully saved to '{DFD_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during document extraction ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 19:49:51,081 - INFO - Initializing ollama provider with model llama3:8b\n",
      "2025-07-27 19:49:51,103 - INFO - Client initialized\n",
      "2025-07-27 19:49:51,118 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-27 19:49:51,119 - INFO - --- DFD components loaded successfully ---\n",
      "2025-07-27 19:49:51,119 - INFO - \n",
      "--- Invoking Local LLM to generate STRIDE threats ---\n",
      "2025-07-27 19:49:51,120 - INFO - --- Prompt sent to LLM ---\n",
      "\n",
      "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
      "\n",
      "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
      "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
      "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
      "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
      "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
      "\n",
      "For each threat, include:\n",
      "- 'component_name': Affected asset, process, data flow, or entity.\n",
      "- 'stride_category': One STRIDE category.\n",
      "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
      "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
      "- 'impact': Low/Medium/High.\n",
      "- 'likelihood': Low/Medium/High.\n",
      "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
      "\n",
      "DFD Components:\n",
      "---\n",
      "{\n",
      "  \"external_entities\": [\n",
      "    \"U\"\n",
      "  ],\n",
      "  \"assets\": [\n",
      "    \"DB_P\",\n",
      "    \"DB_B\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"CDN\",\n",
      "    \"LB\",\n",
      "    \"WS\",\n",
      "    \"MQ\",\n",
      "    \"WRK\",\n",
      "    \"ADM\",\n",
      "    \"ADM_P\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"U\",\n",
      "      \"destination\": \"CDN\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"CDN\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"LB\",\n",
      "      \"destination\": \"WS\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"DB_B\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"Backup Protocol over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM\",\n",
      "      \"destination\": \"ADM_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS over VPN\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM_P\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    }\n",
      "  ],\n",
      "  \"trust_boundaries\": [\n",
      "    \"Public Zone to Edge Zone\",\n",
      "    \"Edge Zone to Application DMZ\",\n",
      "    \"Application DMZ to Internal Core\",\n",
      "    \"Internal Core to Data Zone\",\n",
      "    \"Management Zone to Application DMZ\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T16:38:38.774870\",\n",
      "    \"source_document\": \"./docs/designdoc.docx\"\n",
      "  }\n",
      "}\n",
      "---\n",
      "\n",
      "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
      "\n",
      "2025-07-27 19:50:05,251 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:05,253 - INFO - --- Raw LLM Response ---\n",
      "[\n",
      "  {\n",
      "    \"component_name\": \"U\",\n",
      "    \"stride_category\": \"Spoofing\",\n",
      "    \"threat_description\": \"Attacker impersonates user U to access sensitive data\",\n",
      "    \"mitigation_suggestion\": \"Implement MFA and rate limiting for all authentication attempts\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A01:2021\", \"NIST AC-2\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"CDN\",\n",
      "    \"stride_category\": \"Tampering\",\n",
      "    \"threat_description\": \"Malicious actor injects malicious code into CDN's response\",\n",
      "    \"mitigation_suggestion\": \"Implement Web Application Firewall (WAF) and regular security updates for CDN\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A03:2021\", \"NIST SI-3\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"LB\",\n",
      "    \"stride_category\": \"Repudiation\",\n",
      "    \"threat_description\": \"Attacker alters load balancer configuration to deny service to legitimate users\",\n",
      "    \"mitigation_suggestion\": \"Implement logging and monitoring for load balancer, and have a backup plan in place\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A04:2021\", \"NIST AC-6\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"WS\",\n",
      "    \"stride_category\": \"Information Disclosure\",\n",
      "    \"threat_description\": \"Unencrypted data is transmitted from WS to DB_P, allowing eavesdropping\",\n",
      "    \"mitigation_suggestion\": \"Implement TLS 1.3 with certificate pinning for all data flows from WS to DB_P\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A06:2021\", \"NIST SP 800-53\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"MQ\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor floods MQ with messages, causing performance degradation or failure\",\n",
      "    \"mitigation_suggestion\": \"Implement message queuing and dead-letter queues to handle malformed messages\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"DB_P\",\n",
      "    \"stride_category\": \"Elevation of Privilege\",\n",
      "    \"threat_description\": \"Insufficient access controls allow an attacker to escalate privileges within DB_P\",\n",
      "    \"mitigation_suggestion\": \"Implement least privilege and role-based access control for all database users\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"NIST AC-6\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"ADM_P\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor sends a large number of HTTPS requests to ADM_P, overwhelming the system\",\n",
      "    \"mitigation_suggestion\": \"Implement rate limiting and logging for all administrative access\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"NIST AC-7\"]\n",
      "  }\n",
      "]\n",
      "2025-07-27 19:50:18,652 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:18,655 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-27 19:50:18,658 - INFO - \n",
      "--- LLM Output (Identified Threats) ---\n",
      "2025-07-27 19:50:18,659 - INFO - \n",
      "--- Identified threats successfully saved to './output/identified_threats.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"threats\": [\n",
      "    {\n",
      "      \"component_name\": \"U\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Unencrypted data transmitted from U to CDN potentially disclosed\",\n",
      "      \"mitigation_suggestion\": \"Implement end-to-end encryption; Use HTTPS protocol\",\n",
      "      \"impact\": \"Medium\",\n",
      "      \"likelihood\": \"High\",\n",
      "      \"references\": [\n",
      "        \"OWASP A01:2021\",\n",
      "        \"NIST SI-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"CDN\",\n",
      "      \"stride_category\": \"Tampering\",\n",
      "      \"threat_description\": \"Malicious actor intercepts and modifies data in transit from CDN to LB\",\n",
      "      \"mitigation_suggestion\": \"Implement integrity checking; Use digital signatures\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A03:2021\",\n",
      "        \"MITRE CA-8\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"LB\",\n",
      "      \"stride_category\": \"Elevation of Privilege\",\n",
      "      \"threat_description\": \"Unprivileged actor gains elevated privileges on LB, potentially leading to DoS\",\n",
      "      \"mitigation_suggestion\": \"Implement least privilege; Restrict unnecessary access\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"NIST AC-6\",\n",
      "        \"MITRE AU-5\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"WS\",\n",
      "      \"stride_category\": \"Denial of Service\",\n",
      "      \"threat_description\": \"Volume-based DoS attack on WS, impacting application availability and performance\",\n",
      "      \"mitigation_suggestion\": \"Implement rate limiting; Monitor for suspicious activity\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A04:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"DB_P\",\n",
      "      \"stride_category\": \"Repudiation\",\n",
      "      \"threat_description\": \"Insider attempts to discredit or deny attacks on DB_P, potentially compromising audit logs\",\n",
      "      \"mitigation_suggestion\": \"Implement tamper-evident logging; Conduct regular auditing and monitoring\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Low\",\n",
      "      \"references\": [\n",
      "        \"OWASP A05:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"ADM_P\",\n",
      "      \"stride_category\": \"Spoofing\",\n",
      "      \"threat_description\": \"Attackers impersonate ADM_P, potentially gaining unauthorized access to administrative functions\",\n",
      "      \"mitigation_suggestion\": \"Implement multi-factor authentication; Conduct regular security audits\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A07:2021\",\n",
      "        \"MITRE CA-7\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T19:50:18.655020\",\n",
      "    \"source_dfd\": \"./output/dfd_components.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama python-dotenv pydantic logging instructor\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client  # Added for raw response debugging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use environment variables for paths and settings\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama3:8b\")  # Changed default to a more reliable model for testing\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists early\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LLM with Instructor for schema enforcement\n",
    "llm = instructor.from_provider(f\"ollama/{LLM_MODEL}\", mode=instructor.Mode.JSON_SCHEMA)\n",
    "\n",
    "# Added: Raw Ollama client for debugging\n",
    "ollama_client = Client()\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str = Field(description=\"Affected asset, process, data flow, or entity.\")\n",
    "    stride_category: str = Field(description=\"One STRIDE category: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\")\n",
    "    threat_description: str = Field(description=\"Clear, specific description of the threat.\")\n",
    "    mitigation_suggestion: str = Field(description=\"Practical, actionable mitigation.\")\n",
    "    impact: str = Field(description=\"Low/Medium/High based on potential damage.\")\n",
    "    likelihood: str = Field(description=\"Low/Medium/High based on exploitability.\")\n",
    "    references: list[str] = Field(description=\"Array of standard references (e.g., ['OWASP A01:2021', 'NIST SI-2']).\")\n",
    "\n",
    "class Threats(BaseModel):\n",
    "    threats: list[Threat]\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing (if input is empty) ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [\"User\", \"Attacker\"],\n",
    "    \"processes\": [\"Web Application\", \"Authentication Service\"],\n",
    "    \"data_stores\": [\"User Database\"],\n",
    "    \"data_flows\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"to\": \"Web Application\",\n",
    "            \"data\": \"Login Credentials\",\n",
    "            \"protocol\": \"HTTP\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Web Application\",\n",
    "            \"to\": \"User Database\",\n",
    "            \"data\": \"Query User Data\",\n",
    "            \"protocol\": \"SQL\"\n",
    "        }\n",
    "    ],\n",
    "    \"trust_boundaries\": [\"Internet to DMZ\", \"DMZ to Internal Network\"]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:  # Added: Check for empty data\n",
    "        logger.warning(\"--- DFD data is empty. Using sample DFD for testing ---\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "    logger.info(\"--- DFD components loaded successfully ---\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"--- Input file not found at '{DFD_INPUT_PATH}'. Using sample DFD for testing ---\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"--- FATAL ERROR: Could not parse JSON from '{DFD_INPUT_PATH}' ---\")\n",
    "    logger.error(\"The file may be corrupted or empty. Using sample DFD for testing.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- FATAL ERROR: An unexpected error occurred while loading DFD components ---\")\n",
    "    logger.error(f\"Error details: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Prompt Engineering for Threat Generation ---\n",
    "threat_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
    "\n",
    "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
    "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
    "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
    "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
    "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
    "\n",
    "For each threat, include:\n",
    "- 'component_name': Affected asset, process, data flow, or entity.\n",
    "- 'stride_category': One STRIDE category.\n",
    "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
    "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
    "- 'impact': Low/Medium/High.\n",
    "- 'likelihood': Low/Medium/High.\n",
    "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
    "\n",
    "DFD Components:\n",
    "---\n",
    "{dfd_json}\n",
    "---\n",
    "\n",
    "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "threat_prompt = ChatPromptTemplate.from_template(threat_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Invoking Local LLM to generate STRIDE threats ---\")\n",
    "try:\n",
    "    # Convert the loaded DFD dictionary back to a JSON string for the prompt\n",
    "    dfd_json_string = json.dumps(dfd_data, indent=2)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = threat_prompt.format_messages(dfd_json=dfd_json_string)\n",
    "\n",
    "    # Added: Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    # Added: Call raw Ollama for response debugging (before Instructor)\n",
    "    raw_response = ollama_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "    logger.info(f\"--- Raw LLM Response ---\\n{raw_response['message']['content']}\")\n",
    "\n",
    "    # Invoke the LLM with Instructor for structured output\n",
    "    threats_obj = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "        response_model=Threats,\n",
    "        max_retries=5  # Increased for better handling\n",
    "    )\n",
    "\n",
    "    threats_dict = threats_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    threats_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_dfd\": DFD_INPUT_PATH\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema (Instructor already enforces, but double-check)\n",
    "    try:\n",
    "        validated = ThreatsOutput(**threats_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the threats to a new file\n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(threats_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (Identified Threats) ---\")\n",
    "    print(json.dumps(threats_dict, indent=2))\n",
    "    logger.info(f\"\\n--- Identified threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during threat generation ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 20:17:51,001 - INFO - --- Setting up RAG pipeline with universal ingestion ---\n",
      "2025-07-29 20:17:54,009 - INFO - Use pytorch device_name: mps\n",
      "2025-07-29 20:17:54,010 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-07-29 20:17:57,042 - INFO - --- No existing FAISS index found. Building a new one. ---\n",
      " 27%|â–ˆâ–ˆâ–‹       | 3/11 [00:03<00:10,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install openai pydantic logging python-dotenv langchain langchain_community langchain_huggingface faiss-cpu pypdf sentence-transformers requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "import requests  # Added for auto-download\n",
    "import fitz\n",
    "import io\n",
    "import pytesseract\n",
    "\n",
    "# RAG specific imports\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader, PyPDFLoader, TextLoader, Docx2txtLoader, CSVLoader, BSHTMLLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama-3.1-70b-instruct\") # Updated model suggestion\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_DOCS_DIR = \"rag_docs\"\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RAG_DOCS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a scanned PDF, including OCR for image-based content.\n",
    "    Returns a list of Document objects.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from scanned PDF: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    documents = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = \"\"\n",
    "        \n",
    "        # Extract native text (if any)\n",
    "        page_text = page.get_text()\n",
    "        if page_text.strip():\n",
    "            text += page_text + \"\\n\"\n",
    "        \n",
    "        # Extract images and apply OCR\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image for OCR\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(image, lang='eng')\n",
    "            if ocr_text.strip():\n",
    "                text += ocr_text + \"\\n\"\n",
    "        \n",
    "        if text.strip():\n",
    "            # Create Document object for each page\n",
    "            documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": pdf_path, \"type\": \"pdf\", \"page\": page_num + 1}\n",
    "            ))\n",
    "    \n",
    "    doc.close()\n",
    "    if not documents:\n",
    "        logger.warning(f\"No text extracted from {pdf_path}. Check OCR setup or PDF content.\")\n",
    "    return documents\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"\n",
    "    Extract text from a standalone image using OCR.\n",
    "    Returns a list of Document objects (typically one per image).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from image: {image_path}\")\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image, lang='eng')\n",
    "        if text.strip():\n",
    "            return [Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": image_path, \"type\": \"image\"}\n",
    "            )]\n",
    "        else:\n",
    "            logger.warning(f\"No text extracted from image {image_path}.\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to process image {image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def setup_rag_pipeline():\n",
    "    \"\"\"Initializes the RAG pipeline by creating or loading a FAISS vector store with universal ingestion.\"\"\"\n",
    "    logger.info(\"--- Setting up RAG pipeline with universal ingestion ---\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    if os.path.exists(FAISS_INDEX_PATH):\n",
    "        logger.info(f\"--- Loading existing FAISS index from '{FAISS_INDEX_PATH}' ---\")\n",
    "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        logger.info(\"--- No existing FAISS index found. Building a new one. ---\")\n",
    "        \n",
    "        # Expanded loaders for universal support\n",
    "        loaders = {\n",
    "            \"**/*.pdf\": PyPDFLoader,  # For text-based PDFs\n",
    "            \"**/*.md\": TextLoader,\n",
    "            \"**/*.txt\": TextLoader,\n",
    "            \"**/*.docx\": Docx2txtLoader,\n",
    "            \"**/*.csv\": CSVLoader,\n",
    "            \"**/*.html\": BSHTMLLoader\n",
    "        }\n",
    "        documents = []\n",
    "        \n",
    "        # Load standard documents\n",
    "        for glob, loader_cls in loaders.items():\n",
    "            try:\n",
    "                loader = DirectoryLoader(RAG_DOCS_DIR, glob=glob, loader_cls=loader_cls, show_progress=True, use_multithreading=True, silent_errors=True)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load files with pattern {glob} using {loader_cls.__name__}. Error: {e}\")\n",
    "\n",
    "        # Handle scanned PDFs with OCR (if PyPDFLoader didn't extract text)\n",
    "        pdf_files = [os.path.join(root, f) for root, _, files in os.walk(RAG_DOCS_DIR) for f in files if f.endswith(\".pdf\")]\n",
    "        for pdf_path in pdf_files:\n",
    "            try:\n",
    "                # Try PyPDFLoader first\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                if any(doc.page_content.strip() for doc in docs):\n",
    "                    documents.extend(docs)\n",
    "                else:\n",
    "                    # Fallback to OCR\n",
    "                    logger.info(f\"No text extracted with PyPDFLoader for {pdf_path}. Attempting OCR.\")\n",
    "                    ocr_docs = extract_text_from_scanned_pdf(pdf_path)\n",
    "                    documents.extend(ocr_docs)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not process PDF {pdf_path}. Error: {e}\")\n",
    "\n",
    "        # Handle standalone images with OCR\n",
    "        image_extensions = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")\n",
    "        image_files = [os.path.join(root, f) for root, _, files in os.walk(RAG_DOCS_DIR) for f in files if f.lower().endswith(image_extensions)]\n",
    "        for image_path in image_files:\n",
    "            ocr_docs = extract_text_from_image(image_path)\n",
    "            documents.extend(ocr_docs)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(f\"--- No supported documents found in '{RAG_DOCS_DIR}'. Attempting to auto-download key resources ---\")\n",
    "            # Auto-download OWASP Top 10 2021 PDF\n",
    "            owasp_url = \"https://owasp.org/Top10/assets/PDF/OWASP-Top-10-2021.pdf\"\n",
    "            try:\n",
    "                response = requests.get(owasp_url)\n",
    "                response.raise_for_status()\n",
    "                owasp_path = os.path.join(RAG_DOCS_DIR, \"owasp_top10_2021.pdf\")\n",
    "                with open(owasp_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                logger.info(f\"--- Downloaded OWASP Top 10 2021 PDF to '{owasp_path}' ---\")\n",
    "                # Try PyPDFLoader for OWASP PDF (it's text-based)\n",
    "                pdf_loader = PyPDFLoader(owasp_path)\n",
    "                documents.extend(pdf_loader.load())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"--- Failed to auto-download OWASP PDF: {e} ---\")\n",
    "                raise ValueError(f\"No documents available for RAG. Please add files to '{RAG_DOCS_DIR}'.\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        logger.info(f\"--- Creating FAISS index from {len(docs)} document chunks. This may take a moment... ---\")\n",
    "        db = FAISS.from_documents(docs, embeddings)\n",
    "        db.save_local(FAISS_INDEX_PATH)\n",
    "        logger.info(f\"--- FAISS index created and saved to '{FAISS_INDEX_PATH}' ---\")\n",
    "        \n",
    "    return db\n",
    "\n",
    "# --- Initialize RAG and OpenAI Client ---\n",
    "try:\n",
    "    rag_db = setup_rag_pipeline()\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\",\n",
    "        api_key=os.getenv(\"SCW_SECRET_KEY\")\n",
    "    )\n",
    "    logger.info(\"--- OpenAI client initialized successfully ---\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- Failed to initialize services: {e} ---\")\n",
    "    raise\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str\n",
    "    stride_category: str\n",
    "    threat_description: str\n",
    "    mitigation_suggestion: str\n",
    "    impact: str\n",
    "    likelihood: str\n",
    "    references: list[str]\n",
    "    risk_score: str\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [{\"name\": \"User\"}],\n",
    "    \"processes\": [{\"name\": \"Web Application\"}, {\"name\": \"Authentication Service\"}],\n",
    "    \"data_stores\": [{\"name\": \"User Database\"}],\n",
    "    \"data_flows\": [\n",
    "        {\"source\": \"User\", \"destination\": \"Web Application\", \"data_description\": \"Login Credentials\", \"protocol\": \"HTTPS\"},\n",
    "        {\"source\": \"Web Application\", \"destination\": \"User Database\", \"data_description\": \"Query User Data\", \"protocol\": \"SQL\"}\n",
    "    ],\n",
    "    \"trust_boundaries\": [{\"name\": \"Internet to DMZ\"}]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:\n",
    "        logger.warning(f\"DFD file at '{DFD_INPUT_PATH}' is empty. Using sample DFD for demonstration.\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"DFD file not found at '{DFD_INPUT_PATH}'. Using sample DFD for demonstration.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError as e:\n",
    "    logger.error(f\"FATAL: Error decoding JSON from '{DFD_INPUT_PATH}': {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"FATAL: Error loading DFD: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Enhanced Prompting Strategy ---\n",
    "\n",
    "# **FIX 1: Define STRIDE categories explicitly to ensure systematic coverage.**\n",
    "# Parametrized: Load from config or file if needed\n",
    "STRIDE_DEFINITIONS = {\n",
    "    \"S\": (\"Spoofing\", \"Illegitimately accessing systems or data by impersonating a user, process, or component.\"),\n",
    "    \"T\": (\"Tampering\", \"Unauthorized modification of data, either in transit or at rest.\"),\n",
    "    \"R\": (\"Repudiation\", \"A user or system denying that they performed an action, often due to a lack of sufficient proof (e.g., logs).\"),\n",
    "    \"I\": (\"Information Disclosure\", \"Exposing sensitive information to unauthorized individuals.\"),\n",
    "    \"D\": (\"Denial of Service\", \"Preventing legitimate users from accessing a system or service.\"),\n",
    "    \"E\": (\"Elevation of Privilege\", \"A user or process gaining rights beyond their authorized level.\")\n",
    "}\n",
    "\n",
    "# Optional: Load custom STRIDE from file\n",
    "stride_config_path = \"stride_config.json\"\n",
    "if os.path.exists(stride_config_path):\n",
    "    with open(stride_config_path, 'r') as f:\n",
    "        STRIDE_DEFINITIONS = json.load(f)\n",
    "    logger.info(\"--- Loaded custom STRIDE definitions from 'stride_config.json' ---\")\n",
    "\n",
    "# **FIX 2: Create a highly specific prompt template focused on a SINGLE STRIDE category.**\n",
    "# This prevents generic responses and forces the model to generate relevant, accurate threats.\n",
    "threat_prompt_template_specific_rag = \"\"\"\n",
    "You are a cybersecurity architect specializing in threat modeling using the STRIDE methodology.\n",
    "Your task is to generate 1-2 specific threats for a given DFD component, focusing ONLY on a single STRIDE category.\n",
    "\n",
    "**DFD Component to Analyze:**\n",
    "{component_info}\n",
    "\n",
    "**STRIDE Category to Focus On:**\n",
    "- **{stride_category} ({stride_name}):** {stride_definition}\n",
    "\n",
    "**Security Context from Knowledge Base (for accuracy):**\n",
    "'''\n",
    "{rag_context}\n",
    "'''\n",
    "\n",
    "**Instructions:**\n",
    "1.  Generate 1-2 distinct and realistic threats for the component that fall **strictly** under the '{stride_name}' category.\n",
    "2.  **Be specific.** Relate the threat directly to the component's type and details. For a database, a Spoofing threat is a spoofed connection, not user impersonation. For a data flow, a Tampering threat is a Man-in-the-Middle attack.\n",
    "3.  Use the provided Security Context to create specific descriptions, **actionable mitigations**, and accurate references (e.g., CWE, OWASP Cheat Sheets). Do not invent references.\n",
    "4.  Provide a realistic risk assessment (Impact, Likelihood, Score).\n",
    "5.  Output ONLY a valid JSON object with a single key \"threats\", containing a list of threat objects. Do not include any other text or commentary.\n",
    "\n",
    "**JSON Threat Object Schema:**\n",
    "{{\n",
    "  \"component_name\": \"string (the name of the component being analyzed)\",\n",
    "  \"stride_category\": \"{stride_category}\",\n",
    "  \"threat_description\": \"string (Specific to the component and STRIDE category)\",\n",
    "  \"mitigation_suggestion\": \"string (Actionable and specific)\",\n",
    "  \"impact\": \"Low, Medium, or High\",\n",
    "  \"likelihood\": \"Low, Medium, or High\",\n",
    "  \"references\": [\"list of strings, e.g., 'OWASP A01:2021', 'CWE-89'\"],\n",
    "  \"risk_score\": \"Critical, High, Medium, or Low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "retry_prompt_addition = \" Generate at least one threat if realistically applicable, even if minor.\"\n",
    "\n",
    "# Function to calculate risk_score\n",
    "def calculate_risk_score(impact, likelihood):\n",
    "    if impact == \"High\" and likelihood in [\"Medium\", \"High\"]:\n",
    "        return \"Critical\"\n",
    "    elif (impact == \"High\" and likelihood == \"Low\") or (impact == \"Medium\" and likelihood == \"High\"):\n",
    "        return \"High\"\n",
    "    elif (impact == \"Medium\" and likelihood in [\"Medium\", \"Low\"]) or (impact == \"Low\" and likelihood == \"High\"):\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# --- Main Invocation Logic ---\n",
    "logger.info(\"\\n--- Invoking LLM with RAG to systematically generate STRIDE threats ---\")\n",
    "all_threats = []\n",
    "try:\n",
    "    components_to_analyze = []\n",
    "    for key, value in dfd_data.items():\n",
    "        if isinstance(value, list) and value:\n",
    "            for item in value:\n",
    "                # Ensure component has a name for better identification\n",
    "                if isinstance(item, dict) and item.get(\"name\"):\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "                elif isinstance(item, dict): # Fallback for components without a 'name' field\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "\n",
    "\n",
    "    # **FIX 3: Iterate through each component AND each STRIDE category.**\n",
    "    # This loop structure ensures every category is considered for every component.\n",
    "    for component in components_to_analyze:\n",
    "        component_str = json.dumps(component)\n",
    "        component_name = component.get(\"details\", {}).get(\"name\", component_str)\n",
    "        logger.info(f\"\\n--- Analyzing component: {component_name} ---\")\n",
    "\n",
    "        retrieved_docs = rag_db.similarity_search(component_str, k=5)  # Increased to 5 for broader context\n",
    "        rag_context = \"\\n---\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        logger.info(\"--- Retrieved RAG context for component ---\")\n",
    "\n",
    "        for cat_letter, (cat_name, cat_def) in STRIDE_DEFINITIONS.items():\n",
    "            logger.info(f\"--- Generating threats for STRIDE category: {cat_name} ---\")\n",
    "            \n",
    "            prompt = threat_prompt_template_specific_rag.format(\n",
    "                component_info=component_str,\n",
    "                rag_context=rag_context,\n",
    "                stride_category=cat_letter,\n",
    "                stride_name=cat_name,\n",
    "                stride_definition=cat_def\n",
    "            )\n",
    "            \n",
    "            retry_count = 0\n",
    "            max_retries = 1  # Retry once if no threats\n",
    "            threats = []\n",
    "            while retry_count <= max_retries and not threats:\n",
    "                try:\n",
    "                    if retry_count > 0:\n",
    "                        prompt += retry_prompt_addition  # Add retry instruction\n",
    "                    \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=LLM_MODEL,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        response_format={\"type\": \"json_object\"},\n",
    "                        max_tokens=2048,\n",
    "                        temperature=0.4 # Slightly lower temp for more focused output\n",
    "                    )\n",
    "                    \n",
    "                    response_content = response.choices[0].message.content\n",
    "                    generated_data = json.loads(response_content)\n",
    "                    \n",
    "                    # Ensure the response is a dict with a 'threats' key which is a list\n",
    "                    if isinstance(generated_data, dict) and isinstance(generated_data.get(\"threats\"), list):\n",
    "                        threats = generated_data[\"threats\"]\n",
    "                        # Add component name if missing from LLM response\n",
    "                        for threat in threats:\n",
    "                            if 'component_name' not in threat or not threat['component_name']:\n",
    "                                threat['component_name'] = component_name\n",
    "                        logger.info(f\"--- Successfully generated {len(threats)} threat(s) for category {cat_name} ---\")\n",
    "                    else:\n",
    "                        logger.warning(f\"--- LLM response for {cat_name} on {component_name} had unexpected structure. ---\")\n",
    "                        logger.debug(f\"Raw Response: {response_content}\")\n",
    "\n",
    "                except (json.JSONDecodeError, AttributeError) as e:\n",
    "                    logger.warning(f\"--- Could not parse LLM response for {cat_name} on {component_name}: {e} ---\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"--- An API error occurred for {cat_name} on {component_name}: {e} ---\")\n",
    "                \n",
    "                retry_count += 1\n",
    "\n",
    "            all_threats.extend(threats)\n",
    "\n",
    "    # Deduplication: Remove exact duplicates based on description\n",
    "    unique_threats = []\n",
    "    seen_descriptions = set()\n",
    "    for threat in all_threats:\n",
    "        desc = threat.get('threat_description', '')\n",
    "        if desc not in seen_descriptions:\n",
    "            seen_descriptions.add(desc)\n",
    "            # Recalculate risk_score\n",
    "            threat['risk_score'] = calculate_risk_score(threat.get('impact', 'Low'), threat.get('likelihood', 'Low'))\n",
    "            unique_threats.append(threat)\n",
    "    all_threats = unique_threats\n",
    "    logger.info(f\"--- Deduplicated threats: {len(all_threats)} unique threats remaining ---\")\n",
    "\n",
    "    # --- Final Processing and Validation ---\n",
    "    risk_order = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1, \"Informational\": 0}\n",
    "    all_threats.sort(key=lambda t: risk_order.get(t.get('risk_score', 'Low'), 0), reverse=True)\n",
    "\n",
    "    final_output = {\n",
    "        \"threats\": all_threats,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_dfd\": os.path.basename(DFD_INPUT_PATH),\n",
    "            \"llm_model\": LLM_MODEL,\n",
    "            \"rag_index\": FAISS_INDEX_PATH\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        validated_output = ThreatsOutput(**final_output)\n",
    "        logger.info(\"--- Final JSON output validated successfully against schema ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- FINAL JSON VALIDATION FAILED: {ve} ---\")\n",
    "        \n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    logger.info(\"\\n--- LLM RAG Output (Identified Threats) ---\")\n",
    "    # print(json.dumps(final_output, indent=2))\n",
    "    logger.info(f\"\\n--- Identified {len(all_threats)} threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during the threat generation process ---\")\n",
    "    logger.error(f\"Error: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b793d59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 20:33:21,238 - INFO - --- Fetching CISA Known Exploited Vulnerabilities (KEV) catalog ---\n",
      "2025-07-28 20:33:21,763 - INFO - --- Successfully loaded 1391 entries from CISA KEV catalog ---\n",
      "2025-07-28 20:33:21,764 - INFO - --- Loading initial threats from './output/identified_threats.json' ---\n",
      "2025-07-28 20:33:21,765 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-28 20:33:21,766 - INFO - --- Loading controls from './output/controls.json' ---\n",
      "2025-07-28 20:33:21,766 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- CVE CVE-2018-14732 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14732' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'U to CDN'. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'U to CDN'. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- CVE CVE-2005-2088 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2005-2088' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,772 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,772 - INFO - --- Starting threat deduplication ---\n",
      "2025-07-28 20:33:21,773 - INFO - Use pytorch device_name: mps\n",
      "2025-07-28 20:33:21,774 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cc73e31810451080f32135e0e20064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 20:33:24,925 - INFO - --- Merged 2 similar threats for 'U to CDN' (S) ---\n",
      "2025-07-28 20:33:24,925 - INFO - --- Merged 2 similar threats for 'U to CDN' (D) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'CDN to LB' (I) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (R) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (D) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (E) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to DB_P' (I) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (S) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (I) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (D) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WS to MQ' (E) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (S) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (I) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (D) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (E) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to DB_P' (S) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to DB_P' (T) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'DB_B to DB_P' (S) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'DB_B to DB_P' (D) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (R) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (D) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (E) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM_P to LB' (D) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Merged 2 similar threats for 'ADM_P to LB' (E) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Merged 2 similar threats for 'CDN to LB' (R) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Deduplication reduced 120 threats to 95 ---\n",
      "2025-07-28 20:33:24,931 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,931 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,933 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,933 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,934 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,934 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,942 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,942 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,950 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,955 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,955 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,957 - INFO - --- Final refined JSON output validated successfully against schema. ---\n",
      "2025-07-28 20:33:24,960 - INFO - --- Refined 95 threats saved to './output/refined_threats.json' ---\n",
      "2025-07-28 20:33:24,960 - INFO - --- Summary report saved to './output/threat_summary.json' ---\n",
      "2025-07-28 20:33:24,963 - INFO - --- CSV report saved to './output/threats.csv' ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_INPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "REFINED_THREATS_OUTPUT_PATH = os.getenv(\"REFINED_THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"refined_threats.json\"))\n",
    "CONTROLS_INPUT_PATH = os.getenv(\"CONTROLS_INPUT_PATH\", os.path.join(INPUT_DIR, \"controls.json\"))\n",
    "NVD_API_URL = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n",
    "CISA_KEV_URL = \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\"\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Threat Schema ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str = Field(..., description=\"Standardized name of the component or data flow\")\n",
    "    stride_category: str = Field(..., pattern=\"^[STRIDE]$\", description=\"STRIDE category (S, T, R, I, D, E)\")\n",
    "    threat_description: str = Field(..., description=\"Detailed description of the threat\")\n",
    "    mitigation_suggestion: str = Field(..., description=\"Actionable mitigation specific to the threat\")\n",
    "    impact: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Impact level\")\n",
    "    likelihood: str = Field(..., pattern=\"^(Low|Medium|High)$\", description=\"Likelihood level\")\n",
    "    references: list[str] = Field(..., description=\"List of references (e.g., CWE, CVE, OWASP)\")\n",
    "    risk_score: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Derived risk score\")\n",
    "    residual_risk_score: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Risk score post-mitigation\")\n",
    "    exploitability: str = Field(..., pattern=\"^(Low|Medium|High)$\", description=\"Ease of exploitation\")\n",
    "    mitigation_maturity: str = Field(..., pattern=\"^(Immature|Mature|Advanced)$\", description=\"Maturity of mitigation controls\")\n",
    "    justification: str = Field(..., description=\"Rationale for impact and likelihood ratings\")\n",
    "    risk_statement: str = Field(..., description=\"Business-contextualized risk description\")\n",
    "\n",
    "class RefinedThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Caching for External APIs ---\n",
    "cisa_kev_cache = None\n",
    "\n",
    "def get_cisa_kev_catalog():\n",
    "    \"\"\"Fetches and caches the CISA KEV catalog.\"\"\"\n",
    "    global cisa_kev_cache\n",
    "    if cisa_kev_cache is None:\n",
    "        try:\n",
    "            logger.info(\"--- Fetching CISA Known Exploited Vulnerabilities (KEV) catalog ---\")\n",
    "            response = requests.get(CISA_KEV_URL, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            cisa_kev_cache = {vuln['cveID'] for vuln in response.json().get('vulnerabilities', [])}\n",
    "            logger.info(f\"--- Successfully loaded {len(cisa_kev_cache)} entries from CISA KEV catalog ---\")\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"--- Failed to fetch CISA KEV catalog: {e}. Proceeding without it. ---\")\n",
    "            cisa_kev_cache = set()\n",
    "    return cisa_kev_cache\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_dfd_components():\n",
    "    \"\"\"Load DFD components to provide full data flow details.\"\"\"\n",
    "    logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        with open(DFD_INPUT_PATH, 'r') as f:\n",
    "            dfd_data = json.load(f)\n",
    "        return dfd_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Failed to load DFD components: {e} ---\")\n",
    "        raise\n",
    "\n",
    "def load_controls():\n",
    "    \"\"\"Load client-provided controls to suppress threats.\"\"\"\n",
    "    logger.info(f\"--- Loading controls from '{CONTROLS_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        if os.path.exists(CONTROLS_INPUT_PATH):\n",
    "            with open(CONTROLS_INPUT_PATH, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"https_enabled\": False, \"tls_version\": \"1.2\", \"mtls_enabled\": False, \"secrets_manager\": False}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"--- Failed to load controls, using defaults: {e} ---\")\n",
    "        return {\"https_enabled\": False, \"tls_version\": \"1.2\", \"mtls_enabled\": False, \"secrets_manager\": False}\n",
    "\n",
    "def check_cve_relevance(cve_id):\n",
    "    \"\"\"Check if a CVE is recent (within 5 years) and not actively exploited.\"\"\"\n",
    "    kev_catalog = get_cisa_kev_catalog()\n",
    "    if cve_id in kev_catalog:\n",
    "        logger.warning(f\"--- CVE {cve_id} is in the CISA KEV catalog and should NOT be suppressed. ---\")\n",
    "        return True # It's relevant because it's known to be exploited\n",
    "\n",
    "    try:\n",
    "        year = int(cve_id.split('-')[1])\n",
    "        if year < datetime.now().year - 5:\n",
    "            logger.info(f\"--- CVE {cve_id} is older than 5 years and not in KEV catalog. Considered for suppression. ---\")\n",
    "            return False # Not relevant\n",
    "        return True # Relevant\n",
    "    except (ValueError, IndexError):\n",
    "        logger.warning(f\"--- Could not parse year from CVE ID {cve_id}. Assuming it's relevant. ---\")\n",
    "        return True\n",
    "\n",
    "def calculate_risk_score(impact, likelihood):\n",
    "    \"\"\"Calculate risk score based on impact and likelihood.\"\"\"\n",
    "    # Convert impact to a numeric value for calculation\n",
    "    impact_map = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    likelihood_map = {\"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    \n",
    "    score = impact_map.get(impact, 1) * likelihood_map.get(likelihood, 1)\n",
    "    \n",
    "    if score >= 9:\n",
    "        return \"Critical\"\n",
    "    if score >= 6:\n",
    "        return \"High\"\n",
    "    if score >= 3:\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "def assess_exploitability(threat, dfd_data):\n",
    "    \"\"\"Assess exploitability based on component exposure and protocol.\"\"\"\n",
    "    component_name = threat[\"component_name\"]\n",
    "    # Find the corresponding data flow to check trust boundaries and protocols\n",
    "    flow = next((f for f in dfd_data.get(\"data_flows\", []) if f\"{f['source']} to {f['destination']}\" == component_name), None)\n",
    "    \n",
    "    if flow and flow.get(\"source\") == \"U\": # 'U' is the external user\n",
    "        return \"High\"\n",
    "    if flow and \"TLS\" in flow.get(\"protocol\", \"\") or \"HTTPS\" in flow.get(\"protocol\", \"\"):\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "\n",
    "def assess_mitigation_maturity(mitigation):\n",
    "    \"\"\"Assess maturity of mitigation based on specificity and implementation ease.\"\"\"\n",
    "    mitigation_lower = mitigation.lower()\n",
    "    if \"end-to-end encryption\" in mitigation_lower or \"certificate pinning\" in mitigation_lower:\n",
    "        return \"Advanced\"\n",
    "    if \"mtls\" in mitigation_lower or \"waf\" in mitigation_lower or \"rate limiting\" in mitigation_lower or \"secrets management\" in mitigation_lower:\n",
    "        return \"Mature\"\n",
    "    if \"logging\" in mitigation_lower or \"auditing\" in mitigation_lower:\n",
    "        return \"Immature\"\n",
    "    return \"Mature\"\n",
    "\n",
    "def standardize_component_name(original_name, valid_flows):\n",
    "    \"\"\"Standardize component names to match DFD format.\"\"\"\n",
    "    valid_component_names = {f\"{flow['source']} to {flow['destination']}\" for flow in valid_flows}\n",
    "    \n",
    "    # Clean up common variations\n",
    "    normalized = original_name.replace(\"Data Flow from \", \"\").replace(\" data flow\", \"\").strip()\n",
    "    normalized = \" \".join(normalized.split()).replace(\" to \", \" to \")\n",
    "    \n",
    "    if normalized in valid_component_names:\n",
    "        return normalized\n",
    "    # Fallback for close matches\n",
    "    for valid_name in valid_component_names:\n",
    "        if valid_name.lower() in normalized.lower():\n",
    "            return valid_name\n",
    "    \n",
    "    logger.warning(f\"--- Component name '{original_name}' not found in DFD; retaining original. ---\")\n",
    "    return original_name\n",
    "\n",
    "def generate_justification(threat, flow_details):\n",
    "    \"\"\"Generate tailored justification for impact and likelihood based on data classification.\"\"\"\n",
    "    impact = threat['impact']\n",
    "    likelihood = threat['likelihood']\n",
    "    data_classification = flow_details.get(\"data_classification\", \"Unclassified\") if flow_details else \"Unclassified\"\n",
    "\n",
    "    # Justification for Impact\n",
    "    impact_reason = f\"Impact rated {impact} because \"\n",
    "    if data_classification != \"Unclassified\":\n",
    "        impact_reason += f\"the data flow handles '{data_classification}' data, \"\n",
    "        if data_classification in [\"PII\", \"Confidential\", \"PHI\", \"PCI\"]:\n",
    "            impact_reason += \"and a breach could lead to regulatory fines and significant reputational damage.\"\n",
    "        else:\n",
    "            impact_reason += \"and a breach could cause moderate business disruption.\"\n",
    "    elif \"DB_P\" in threat[\"component_name\"]: # Fallback if no classification\n",
    "         impact_reason += \"of potential exposure of sensitive data in the primary database, leading to severe reputational damage.\"\n",
    "    else: # Generic fallback\n",
    "        impact_reason += {\n",
    "            \"Critical\": \"of potential for severe business disruption or data breach.\",\n",
    "            \"High\": \"of potential for significant business disruption or data exposure.\",\n",
    "            \"Medium\": \"of moderate disruption or partial data exposure.\",\n",
    "            \"Low\": \"of minimal operational impact.\"\n",
    "        }.get(impact, \"of minimal operational impact.\")\n",
    "\n",
    "    # Justification for Likelihood\n",
    "    likelihood_reason = f\"Likelihood rated {likelihood} because \"\n",
    "    if flow_details and flow_details.get(\"source\") == 'U':\n",
    "        likelihood_reason += \"the component is internet-facing, increasing the attack surface.\"\n",
    "    else:\n",
    "        likelihood_reason += \"the component is internal, reducing direct exposure.\"\n",
    "\n",
    "    return f\"{impact_reason} {likelihood_reason}\"\n",
    "\n",
    "\n",
    "def generate_risk_statement(threat, flow_details, industry=\"Generic\"):\n",
    "    \"\"\"Generate a business-contextualized risk statement using data classification.\"\"\"\n",
    "    impact_map = {\n",
    "        \"Critical\": \"a critical event, potentially causing severe financial loss (e.g., >$1M), major regulatory fines, and long-term reputational damage\",\n",
    "        \"High\": \"significant financial loss (e.g., >$500K), regulatory fines, or reputational damage\",\n",
    "        \"Medium\": \"moderate financial loss (e.g., $50K-$500K) or operational disruption\",\n",
    "        \"Low\": \"minimal financial or operational impact\"\n",
    "    }\n",
    "    component = threat[\"component_name\"]\n",
    "    data_classification = flow_details.get(\"data_classification\", \"data\") if flow_details else \"data\"\n",
    "\n",
    "    risk = f\"Risk of {threat['threat_description'].lower()} on the '{component}' flow, which handles **{data_classification}**, could lead to {impact_map[threat['impact']]}.\"\n",
    "    \n",
    "    if industry == \"Finance\" and data_classification == \"PCI\":\n",
    "        risk += \" This may violate PCI-DSS compliance.\"\n",
    "    elif industry == \"Healthcare\" and data_classification == \"PHI\":\n",
    "        risk += \" This may violate HIPAA regulations.\"\n",
    "    \n",
    "    # Comment on residual risk based on mitigation maturity\n",
    "    if threat['residual_risk_score'] < threat['risk_score']:\n",
    "         risk += f\" The proposed mitigation, '{threat['mitigation_suggestion']}', is expected to reduce the risk to '{threat['residual_risk_score']}'.\"\n",
    "         \n",
    "    return risk\n",
    "\n",
    "def suppress_threats(threats, controls, dfd_data):\n",
    "    \"\"\"Suppress or downgrade threats based on implemented controls and CVE relevance.\"\"\"\n",
    "    active_threats = []\n",
    "    for threat in threats:\n",
    "        suppress = False\n",
    "        component = threat[\"component_name\"]\n",
    "        flow = next((f for f in dfd_data.get(\"data_flows\", []) if f\"{f['source']} to {f['destination']}\" == component), None)\n",
    "        protocol = flow.get(\"protocol\", \"Unknown\") if flow else \"Unknown\"\n",
    "\n",
    "        # Suppress based on controls\n",
    "        if controls.get(\"mtls_enabled\") and \"spoof\" in threat[\"threat_description\"].lower():\n",
    "            logger.info(f\"--- Suppressing '{component}' ({threat['stride_category']}) due to mTLS control. ---\")\n",
    "            suppress = True\n",
    "        if controls.get(\"secrets_manager\") and \"cleartext\" in threat[\"threat_description\"].lower():\n",
    "            logger.info(f\"--- Suppressing '{component}' ({threat['stride_category']}) due to secrets management. ---\")\n",
    "            suppress = True\n",
    "        \n",
    "        # Suppress based on irrelevant CVEs\n",
    "        if not suppress:\n",
    "            relevant_references = []\n",
    "            for ref in threat.get(\"references\", []):\n",
    "                if ref.startswith(\"CVE-\") and not check_cve_relevance(ref):\n",
    "                    logger.info(f\"--- Removing outdated/irrelevant CVE '{ref}' from threat '{component}'. ---\")\n",
    "                else:\n",
    "                    relevant_references.append(ref)\n",
    "            \n",
    "            # If all references were irrelevant CVEs, suppress the threat\n",
    "            if threat.get(\"references\") and not relevant_references:\n",
    "                 logger.info(f\"--- Suppressing threat for '{component}' as its only CVE references were irrelevant. ---\")\n",
    "                 suppress = True\n",
    "            else:\n",
    "                threat[\"references\"] = relevant_references\n",
    "\n",
    "        if not suppress:\n",
    "            active_threats.append(threat)\n",
    "            \n",
    "    return active_threats\n",
    "\n",
    "def deduplicate_threats(threats, similarity_threshold=0.80):\n",
    "    \"\"\"Deduplicate threats using clustering on description and mitigation similarity.\"\"\"\n",
    "    if not threats:\n",
    "        return []\n",
    "    logger.info(\"--- Starting threat deduplication ---\")\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # Embed a combination of description and mitigation for semantic meaning\n",
    "    combined_texts = [f\"{threat['threat_description']} {threat['mitigation_suggestion']}\" for threat in threats]\n",
    "    embeddings = model.encode(combined_texts, convert_to_tensor=True).cpu().numpy()\n",
    "    \n",
    "    # Use DBSCAN for density-based clustering\n",
    "    clustering = DBSCAN(eps=1 - similarity_threshold, min_samples=1, metric=\"cosine\").fit(embeddings)\n",
    "    labels = clustering.labels_\n",
    "    \n",
    "    # Group threats by cluster, component, and STRIDE for accurate merging\n",
    "    groups = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        key = (label, threats[idx][\"component_name\"], threats[idx][\"stride_category\"])\n",
    "        if key not in groups:\n",
    "            groups[key] = []\n",
    "        groups[key].append(idx)\n",
    "    \n",
    "    # Merge threats within each cluster\n",
    "    deduplicated_threats = []\n",
    "    for key, indices in groups.items():\n",
    "        if len(indices) == 1:\n",
    "            deduplicated_threats.append(threats[indices[0]])\n",
    "        else:\n",
    "            cluster_threats = [threats[i] for i in indices]\n",
    "            # Choose the most detailed description and mitigation from the cluster\n",
    "            primary_threat = max(cluster_threats, key=lambda t: len(t.get('threat_description', '')))\n",
    "            primary_threat['mitigation_suggestion'] = max(cluster_threats, key=lambda t: len(t.get('mitigation_suggestion', ''))).get('mitigation_suggestion')\n",
    "\n",
    "            # Combine all unique references\n",
    "            combined_references = set()\n",
    "            for t in cluster_threats:\n",
    "                combined_references.update(t.get(\"references\", []))\n",
    "            primary_threat[\"references\"] = sorted(list(combined_references))\n",
    "            \n",
    "            deduplicated_threats.append(primary_threat)\n",
    "            logger.info(f\"--- Merged {len(indices)} similar threats for '{primary_threat['component_name']}' ({primary_threat['stride_category']}) ---\")\n",
    "    \n",
    "    logger.info(f\"--- Deduplication reduced {len(threats)} threats to {len(deduplicated_threats)} ---\")\n",
    "    return deduplicated_threats\n",
    "\n",
    "# --- Main Refinement Logic ---\n",
    "def refine_threats():\n",
    "    \"\"\"Refine threats by deduplicating, standardizing, and enhancing with business risk context.\"\"\"\n",
    "    logger.info(f\"--- Loading initial threats from '{THREATS_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        with open(THREATS_INPUT_PATH, 'r') as f:\n",
    "            threat_data = json.load(f)\n",
    "        threats = threat_data.get(\"threats\", [])\n",
    "        if not threats:\n",
    "            raise ValueError(\"Input file contains no threats.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Failed to load threats: {e} ---\")\n",
    "        raise\n",
    "\n",
    "    dfd_data = load_dfd_components()\n",
    "    controls = load_controls()\n",
    "    industry = os.getenv(\"CLIENT_INDUSTRY\", \"Generic\")\n",
    "    dfd_flows = dfd_data.get(\"data_flows\", [])\n",
    "    original_threat_count = len(threats)\n",
    "\n",
    "    # Step 1: Standardize component names to match DFD\n",
    "    for threat in threats:\n",
    "        threat[\"component_name\"] = standardize_component_name(threat[\"component_name\"], dfd_flows)\n",
    "\n",
    "    # Step 2: Suppress threats based on controls and CVE relevance\n",
    "    threats = suppress_threats(threats, controls, dfd_data)\n",
    "\n",
    "    # Step 3: Deduplicate remaining threats\n",
    "    threats = deduplicate_threats(threats)\n",
    "\n",
    "    # Step 4: Enrich each threat with calculated metadata\n",
    "    refined_threats = []\n",
    "    for threat in threats:\n",
    "        flow_details = next((f for f in dfd_flows if f\"{f['source']} to {f['destination']}\" == threat[\"component_name\"]), None)\n",
    "        if flow_details and flow_details.get(\"data_classification\") is None:\n",
    "            logger.warning(f\"--- Data flow '{threat['component_name']}' is missing 'data_classification'. Impact assessment will be generic. ---\")\n",
    "\n",
    "        # Set impact based on data classification if not already high\n",
    "        if flow_details and flow_details.get(\"data_classification\") in [\"PII\", \"PHI\", \"PCI\", \"Confidential\"]:\n",
    "            threat[\"impact\"] = \"Critical\" if threat[\"impact\"] == \"High\" else \"High\"\n",
    "\n",
    "        # Calculate scores and assessments\n",
    "        threat[\"risk_score\"] = calculate_risk_score(threat[\"impact\"], threat[\"likelihood\"])\n",
    "        mitigated_likelihood = \"Low\" if \"logging\" not in threat[\"mitigation_suggestion\"].lower() else threat[\"likelihood\"]\n",
    "        threat[\"residual_risk_score\"] = calculate_risk_score(threat[\"impact\"], mitigated_likelihood)\n",
    "        threat[\"exploitability\"] = assess_exploitability(threat, dfd_data)\n",
    "        threat[\"mitigation_maturity\"] = assess_mitigation_maturity(threat[\"mitigation_suggestion\"])\n",
    "        \n",
    "        # Generate human-readable statements\n",
    "        threat[\"justification\"] = generate_justification(threat, flow_details)\n",
    "        threat[\"risk_statement\"] = generate_risk_statement(threat, flow_details, industry)\n",
    "        \n",
    "        refined_threats.append(threat)\n",
    "\n",
    "    # Step 5: Sort by risk score for prioritization\n",
    "    risk_order = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    refined_threats.sort(key=lambda t: risk_order.get(t.get(\"risk_score\"), 0), reverse=True)\n",
    "\n",
    "    # Step 6: Assemble and validate the final output\n",
    "    final_output = {\n",
    "        \"threats\": refined_threats,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_dfd\": os.path.basename(DFD_INPUT_PATH),\n",
    "            \"source_threats\": os.path.basename(THREATS_INPUT_PATH),\n",
    "            \"refined_threat_count\": len(refined_threats),\n",
    "            \"original_threat_count\": original_threat_count,\n",
    "            \"industry_context\": industry\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        validated_output = RefinedThreatsOutput(**final_output)\n",
    "        logger.info(\"--- Final refined JSON output validated successfully against schema. ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- Final JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "\n",
    "    # Step 7: Save all outputs\n",
    "    with open(REFINED_THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(validated_output.model_dump(), f, indent=2)\n",
    "    logger.info(f\"--- Refined {len(refined_threats)} threats saved to '{REFINED_THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "    summary = {\n",
    "        \"total_threats\": len(refined_threats),\n",
    "        \"critical_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Critical\"),\n",
    "        \"high_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"High\"),\n",
    "        \"medium_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Medium\"),\n",
    "        \"low_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Low\"),\n",
    "        \"prioritization_recommendation\": (\n",
    "            \"Remediation should be prioritized based on risk score. Address all 'Critical' and 'High' risk threats within the next development cycle. \"\n",
    "            \"Focus on implementing robust, mature controls like mTLS and centralized logging to address systemic weaknesses.\"\n",
    "        )\n",
    "    }\n",
    "    summary_path = os.path.join(INPUT_DIR, \"threat_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    logger.info(f\"--- Summary report saved to '{summary_path}' ---\")\n",
    "\n",
    "    pd.DataFrame(final_output[\"threats\"]).to_csv(os.path.join(INPUT_DIR, \"threats.csv\"), index=False)\n",
    "    logger.info(f\"--- CSV report saved to '{os.path.join(INPUT_DIR, 'threats.csv')}' ---\")\n",
    "\n",
    "# --- Execute ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_cisa_kev_catalog() # Pre-fetch KEV catalog on startup\n",
    "        refine_threats()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Threat refinement process failed with an unrecoverable error: {e} ---\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e4d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71b796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
