{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572111a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 13:13:41,925 - INFO - \n",
      "--- Starting Pre-Filter for Document Extraction ---\n",
      "2025-07-30 13:13:41,960 - ERROR - --- Failed to initialize Ollama client: 'function' object has no attribute 'completions' ---\n",
      "2025-07-30 13:13:41,961 - ERROR - \n",
      "--- An error occurred during document extraction ---\n",
      "2025-07-30 13:13:41,961 - ERROR - Error: 'function' object has no attribute 'completions'\n",
      "2025-07-30 13:13:41,961 - ERROR - This could be due to the LLM not returning a well-formed JSON object or an issue with the input documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client\n",
    "import PyPDF2\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "import docx  # Added for DOCX support\n",
    "import base64  # Added for image base64 encoding\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"ollama\").lower()  # Default to 'ollama', can be set to 'scaleway'\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama3:70b-instruct\")\n",
    "VISION_MODEL = os.getenv(\"VISION_MODEL\", \"llava:34b\" if LLM_PROVIDER == \"ollama\" else LLM_MODEL)\n",
    "SCW_API_URL = os.getenv(\"SCW_API_URL\", \"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\")\n",
    "SCW_SECRET_KEY = os.getenv(\"SCW_SECRET_KEY\")\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./input_documents\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "DFD_OUTPUT_PATH = os.getenv(\"DFD_OUTPUT_PATH\", os.path.join(OUTPUT_DIR, \"dfd_components.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Initialize LLM Client ---\n",
    "def initialize_llm_client():\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        if not SCW_SECRET_KEY:\n",
    "            raise ValueError(\"SCW_SECRET_KEY environment variable is required for Scaleway API.\")\n",
    "        try:\n",
    "            client = instructor.from_openai(OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY))\n",
    "            logger.info(\"--- Scaleway OpenAI client initialized successfully ---\")\n",
    "            return client, \"scaleway\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Scaleway client: {e} ---\")\n",
    "            raise\n",
    "    else:  # Default to Ollama\n",
    "        try:\n",
    "            raw_client = Client()  # Raw Ollama client for debugging\n",
    "            # Patch the Ollama client with instructor for structured output\n",
    "            instructor_client = instructor.patch(Client())\n",
    "            logger.info(\"--- Ollama client initialized successfully ---\")\n",
    "            return raw_client, instructor_client, \"ollama\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Ollama client: {e} ---\")\n",
    "            raise\n",
    "\n",
    "# --- DFD Schema for Validation ---\n",
    "class DataFlow(BaseModel):\n",
    "    source: str = Field(description=\"Source component of the data flow (e.g., 'U' for User).\")\n",
    "    destination: str = Field(description=\"Destination component of the data flow (e.g., 'CDN').\")\n",
    "    data_description: str = Field(description=\"Description of data being transferred (e.g., 'User session tokens').\")\n",
    "    data_classification: str = Field(description=\"Classification like 'Confidential', 'PII', or 'Public'.\")\n",
    "    protocol: str = Field(description=\"Protocol used (e.g., 'HTTPS', 'JDBC/ODBC over TLS').\")\n",
    "    authentication_mechanism: str = Field(description=\"Authentication method (e.g., 'JWT in Header').\")\n",
    "\n",
    "class DFDComponents(BaseModel):\n",
    "    project_name: str = Field(description=\"Name of the project (e.g., 'Web Application Security Model').\")\n",
    "    project_version: str = Field(description=\"Version of the project (e.g., '1.1').\")\n",
    "    industry_context: str = Field(description=\"Industry context (e.g., 'Finance').\")\n",
    "    external_entities: list[str] = Field(description=\"List of external entities (e.g., ['U', 'Attacker']).\")\n",
    "    assets: list[str] = Field(description=\"List of assets like data stores (e.g., ['DB_P', 'DB_B']).\")\n",
    "    processes: list[str] = Field(description=\"List of processes (e.g., ['CDN', 'LB', 'WS']).\")\n",
    "    trust_boundaries: list[str] = Field(description=\"List of trust boundaries (e.g., ['Public Zone to Edge Zone']).\")\n",
    "    data_flows: list[DataFlow] = Field(description=\"List of data flows between components.\")\n",
    "\n",
    "class DFDOutput(BaseModel):\n",
    "    dfd: DFDComponents\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample Input for Testing (if no documents are found) ---\n",
    "SAMPLE_DOCUMENT_CONTENT = \"\"\"\n",
    "System: Web Application Security Model, Version 1.1, Finance Industry\n",
    "External Entities: User (U), External Attacker\n",
    "Assets: Profile Database (DB_P), Billing Database (DB_B)\n",
    "Processes: Content Delivery Network (CDN), Load Balancer (LB), Web Server (WS), Message Queue (MQ), Worker (WRK), Admin Service (ADM), Admin Portal (ADM_P)\n",
    "Trust Boundaries: Public Zone to Edge Zone, Edge Zone to Application DMZ, Application DMZ to Internal Core, Internal Core to Data Zone, Management Zone to Application DMZ\n",
    "Data Flows:\n",
    "- From User to CDN: User session tokens and requests for static assets, Confidential, HTTPS, JWT in Header\n",
    "- From CDN to LB: Cached content and user requests, Confidential, HTTPS, mTLS\n",
    "- From WS to DB_P: User profile data including names and email addresses, PII, JDBC/ODBC over TLS, Database Credentials from Secrets Manager\n",
    "\"\"\"\n",
    "\n",
    "# --- Load and Parse Documents ---\n",
    "def load_documents(input_dir):\n",
    "    logger.info(f\"--- Loading documents from '{input_dir}' ---\")\n",
    "    documents = []\n",
    "    # Expanded glob patterns to include more file types: TXT, PDF, DOCX, MD, PNG, JPG/JPEG\n",
    "    file_patterns = [\n",
    "        \"*.[tT][xX][tT]\",      # TXT files (case-insensitive)\n",
    "        \"*.[pP][dD][fF]\",      # PDF files\n",
    "        \"*.[dD][oO][cC][xX]\",  # DOCX files\n",
    "        \"*.[mM][dD]\",          # Markdown files\n",
    "        \"*.[pP][nN][gG]\",      # PNG files\n",
    "        \"*.[jJ][pP][gG]\",      # JPG files\n",
    "        \"*.[jJ][pP][eE][gG]\"   # JPEG files\n",
    "    ]\n",
    "    all_files = []\n",
    "    for pattern in file_patterns:\n",
    "        all_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            ext = os.path.splitext(file_path)[1].lower()\n",
    "            if ext == \".txt\" or ext == \".md\":  # Treat MD like TXT\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                logger.info(f\"Loaded text-based file: {file_path}\")\n",
    "            elif ext == \".pdf\":\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\".join(page.extract_text() for page in pdf_reader.pages if page.extract_text())\n",
    "                    documents.append(text)\n",
    "                logger.info(f\"Loaded PDF file: {file_path}\")\n",
    "            elif ext == \".docx\":\n",
    "                doc = docx.Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
    "                documents.append(text)\n",
    "                logger.info(f\"Loaded DOCX file: {file_path}\")\n",
    "            elif ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "                vision_prompt = \"\"\"You are an expert in analyzing diagrams, especially Data Flow Diagrams (DFD). \n",
    "Describe this diagram in full detail. Identify all external entities, processes, data stores (assets), trust boundaries, and every data flow with source, destination, data description, classification, protocol, authentication mechanism if possible.\n",
    "Be as comprehensive as possible to allow extracting structured DFD components.\"\"\"\n",
    "                if LLM_PROVIDER == \"ollama\":\n",
    "                    raw_response = raw_client.chat(\n",
    "                        model=VISION_MODEL,\n",
    "                        messages=[{\"role\": \"user\", \"content\": vision_prompt, \"images\": [file_path]}]\n",
    "                    )\n",
    "                    description = raw_response['message']['content']\n",
    "                elif LLM_PROVIDER == \"scaleway\":\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        base64_image = base64.b64encode(f.read()).decode('utf-8')\n",
    "                    image_type = \"png\" if ext == \".png\" else \"jpeg\"\n",
    "                    content = [\n",
    "                        {\"type\": \"text\", \"text\": vision_prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{image_type};base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                    raw_response = raw_client.chat.completions.create(\n",
    "                        model=VISION_MODEL,\n",
    "                        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "                    )\n",
    "                    description = raw_response.choices[0].message.content\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported LLM provider for image processing: {LLM_PROVIDER}\")\n",
    "                documents.append(description)\n",
    "                logger.info(f\"Loaded and described image file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {file_path}: {e}\")\n",
    "    if not documents:\n",
    "        logger.warning(\"--- No valid documents found. Using sample document content ---\")\n",
    "        documents = [SAMPLE_DOCUMENT_CONTENT]\n",
    "    return documents\n",
    "\n",
    "# --- Prompt Engineering for Document Extraction ---\n",
    "extract_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling. Your task is to extract structured information from one or more input documents describing a system and transform it into a comprehensive and accurate JSON object representing a Data Flow Diagram (DFD).\n",
    "\n",
    "Your analysis must be meticulous. Follow these reasoning steps precisely:\n",
    "\n",
    "1.  **Identify Core Components**: First, perform a full scan of the document(s) to identify and list all high-level components. This includes:\n",
    "    * `project_name`, `project_version`, and `industry_context`.\n",
    "    * `external_entities`: Any user, actor, or system outside the primary application boundary.\n",
    "    * `processes`: The distinct computational components or services that handle data.\n",
    "    * `assets`: The data stores, such as databases, object storage buckets, or message queues.\n",
    "    * `trust_boundaries`: The defined boundaries separating zones of different trust levels.\n",
    "\n",
    "2.  **Systematically Extract ALL Data Flows**: This is the most critical step. You must identify every single flow of data mentioned or implied in the documents. Scrutinize sections like \"Use Cases,\" \"Data Flow Diagrams,\" \"Architecture,\" and \"Technology Stack\" to find them. Create a data flow entry for each of the following interaction types:\n",
    "    * **External-to-Process**: Flows from an `external_entity` to an internal `process` (e.g., user submitting credentials, uploading a file).\n",
    "    * **Process-to-External**: Flows from an internal `process` to an `external_entity` (e.g., returning results, sending a session token).\n",
    "    * **Process-to-Asset**: Flows where a `process` reads from or writes to a data store `asset` (e.g., \"Authentication Service reads from UsersDB,\" \"Analysis process writes to ResultsDB\"). These are essential and must not be omitted.\n",
    "    * **Process-to-Process**: Flows between internal `processes` (e.g., \"API Gateway routes request to Authentication Service\").\n",
    "\n",
    "3.  **Detail and Classify Each Flow**: For every data flow you identify, you must accurately populate all its attributes: `source`, `destination`, `data_description`, `data_classification`, `protocol`, and `authentication_mechanism`.\n",
    "    * **Data Classification Rules**: Apply strict classification.\n",
    "        * **Confidential**: Use for any data that, if exposed, could harm the organization or its users. This includes, but is not limited to: credentials, session tokens (JWTs), API keys, SAML assertions, Personally Identifiable Information (PII), health information (PHI), financial data, and proprietary business logic.\n",
    "        * **Public**: Use ONLY for data that is explicitly intended for public consumption and carries no security risk if intercepted (e.g., a list of available public APIs). **Authentication-related data is never public.**\n",
    "    * If information for a field (like `protocol` or `authentication_mechanism`) is not explicitly stated, infer it from the context (e.g., a web service likely uses HTTPS, a database connection likely uses JDBC/ODBC over TLS) and make a note of this in the `assumptions` key in the metadata.\n",
    "\n",
    "\n",
    "Input Documents:\n",
    "---\n",
    "{documents}\n",
    "---\n",
    "\n",
    "4.  **Final Review**: Before generating the final output, review the generated list of data flows against the use cases in the source document. Ensure that every major action described in the use cases is represented by one or more data flows in your output.\n",
    "\n",
    "\n",
    "Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_template(extract_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Starting Pre-Filter for Document Extraction ---\")\n",
    "try:\n",
    "    # Initialize LLM client\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        client, client_type = initialize_llm_client()\n",
    "        raw_client = OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY)\n",
    "    else:\n",
    "        raw_client, instructor_client, client_type = initialize_llm_client()\n",
    "        client = instructor_client\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(INPUT_DIR)\n",
    "    documents_combined = \"\\n--- Document Separator ---\\n\".join(documents)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = extract_prompt.format_messages(documents=documents_combined)\n",
    "\n",
    "    # Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    if client_type == \"scaleway\":\n",
    "        # Use instructor client for Scaleway\n",
    "        dfd_obj = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_response = raw_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        logger.info(f\"--- Raw Scaleway Response ---\\n{raw_response.choices[0].message.content}\")\n",
    "        # Log token usage for Scaleway\n",
    "        if hasattr(raw_response, 'usage'):\n",
    "            prompt_tokens = raw_response.usage.prompt_tokens or 'N/A'\n",
    "            completion_tokens = raw_response.usage.completion_tokens or 'N/A'\n",
    "            total_tokens = raw_response.usage.total_tokens or 'N/A'\n",
    "            logger.info(f\"--- Token Usage for Scaleway ---\")\n",
    "            logger.info(f\"Input Tokens: {prompt_tokens}\")\n",
    "            logger.info(f\"Output Tokens: {completion_tokens}\")\n",
    "            logger.info(f\"Total Tokens: {total_tokens}\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        # Use instructor client for Ollama\n",
    "        dfd_obj = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_response = raw_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "        logger.info(f\"--- Raw Ollama Response ---\\n{raw_response['message']['content']}\")\n",
    "        # Log Token Count and Performance\n",
    "        prompt_tokens = raw_response.get('prompt_eval_count', 'N/A')\n",
    "        prompt_duration_ns = raw_response.get('prompt_eval_duration', 0)\n",
    "        response_tokens = raw_response.get('eval_count', 'N/A')\n",
    "        response_duration_ns = raw_response.get('eval_duration', 0)\n",
    "        prompt_duration_s = f\"{prompt_duration_ns / 1_000_000_000:.2f}s\" if prompt_duration_ns else \"N/A\"\n",
    "        response_duration_s = f\"{response_duration_ns / 1_000_000_000:.2f}s\" if response_duration_ns else \"N/A\"\n",
    "        logger.info(f\"--- Token Usage & Performance ---\")\n",
    "        logger.info(f\"Input Tokens: {prompt_tokens} (processed in {prompt_duration_s})\")\n",
    "        logger.info(f\"Output Tokens: {response_tokens} (generated in {response_duration_s})\")\n",
    "\n",
    "    dfd_dict = dfd_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    output_dict = {\n",
    "        \"dfd\": dfd_dict,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_documents\": (\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[tT][xX][tT]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[pP][dD][fF]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[dD][oO][cC][xX]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[mM][dD]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[pP][nN][gG]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[jJ][pP][gG]\")) +\n",
    "                glob.glob(os.path.join(INPUT_DIR, \"*.[jJ][pP][eE][gG]\"))\n",
    "            ),\n",
    "            \"assumptions\": [],\n",
    "            \"llm_provider\": LLM_PROVIDER\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema\n",
    "    try:\n",
    "        validated = DFDOutput(**output_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the DFD components to a file\n",
    "    with open(DFD_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(output_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (DFD Components) ---\")\n",
    "    print(json.dumps(output_dict, indent=2))\n",
    "    logger.info(f\"\\n--- DFD components successfully saved to '{DFD_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during document extraction ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 19:49:51,081 - INFO - Initializing ollama provider with model llama3:8b\n",
      "2025-07-27 19:49:51,103 - INFO - Client initialized\n",
      "2025-07-27 19:49:51,118 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-27 19:49:51,119 - INFO - --- DFD components loaded successfully ---\n",
      "2025-07-27 19:49:51,119 - INFO - \n",
      "--- Invoking Local LLM to generate STRIDE threats ---\n",
      "2025-07-27 19:49:51,120 - INFO - --- Prompt sent to LLM ---\n",
      "\n",
      "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
      "\n",
      "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
      "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
      "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
      "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
      "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
      "\n",
      "For each threat, include:\n",
      "- 'component_name': Affected asset, process, data flow, or entity.\n",
      "- 'stride_category': One STRIDE category.\n",
      "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
      "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
      "- 'impact': Low/Medium/High.\n",
      "- 'likelihood': Low/Medium/High.\n",
      "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
      "\n",
      "DFD Components:\n",
      "---\n",
      "{\n",
      "  \"external_entities\": [\n",
      "    \"U\"\n",
      "  ],\n",
      "  \"assets\": [\n",
      "    \"DB_P\",\n",
      "    \"DB_B\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"CDN\",\n",
      "    \"LB\",\n",
      "    \"WS\",\n",
      "    \"MQ\",\n",
      "    \"WRK\",\n",
      "    \"ADM\",\n",
      "    \"ADM_P\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"U\",\n",
      "      \"destination\": \"CDN\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"CDN\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"LB\",\n",
      "      \"destination\": \"WS\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"DB_B\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"Backup Protocol over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM\",\n",
      "      \"destination\": \"ADM_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS over VPN\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM_P\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    }\n",
      "  ],\n",
      "  \"trust_boundaries\": [\n",
      "    \"Public Zone to Edge Zone\",\n",
      "    \"Edge Zone to Application DMZ\",\n",
      "    \"Application DMZ to Internal Core\",\n",
      "    \"Internal Core to Data Zone\",\n",
      "    \"Management Zone to Application DMZ\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T16:38:38.774870\",\n",
      "    \"source_document\": \"./docs/designdoc.docx\"\n",
      "  }\n",
      "}\n",
      "---\n",
      "\n",
      "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
      "\n",
      "2025-07-27 19:50:05,251 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:05,253 - INFO - --- Raw LLM Response ---\n",
      "[\n",
      "  {\n",
      "    \"component_name\": \"U\",\n",
      "    \"stride_category\": \"Spoofing\",\n",
      "    \"threat_description\": \"Attacker impersonates user U to access sensitive data\",\n",
      "    \"mitigation_suggestion\": \"Implement MFA and rate limiting for all authentication attempts\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A01:2021\", \"NIST AC-2\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"CDN\",\n",
      "    \"stride_category\": \"Tampering\",\n",
      "    \"threat_description\": \"Malicious actor injects malicious code into CDN's response\",\n",
      "    \"mitigation_suggestion\": \"Implement Web Application Firewall (WAF) and regular security updates for CDN\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A03:2021\", \"NIST SI-3\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"LB\",\n",
      "    \"stride_category\": \"Repudiation\",\n",
      "    \"threat_description\": \"Attacker alters load balancer configuration to deny service to legitimate users\",\n",
      "    \"mitigation_suggestion\": \"Implement logging and monitoring for load balancer, and have a backup plan in place\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A04:2021\", \"NIST AC-6\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"WS\",\n",
      "    \"stride_category\": \"Information Disclosure\",\n",
      "    \"threat_description\": \"Unencrypted data is transmitted from WS to DB_P, allowing eavesdropping\",\n",
      "    \"mitigation_suggestion\": \"Implement TLS 1.3 with certificate pinning for all data flows from WS to DB_P\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A06:2021\", \"NIST SP 800-53\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"MQ\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor floods MQ with messages, causing performance degradation or failure\",\n",
      "    \"mitigation_suggestion\": \"Implement message queuing and dead-letter queues to handle malformed messages\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"DB_P\",\n",
      "    \"stride_category\": \"Elevation of Privilege\",\n",
      "    \"threat_description\": \"Insufficient access controls allow an attacker to escalate privileges within DB_P\",\n",
      "    \"mitigation_suggestion\": \"Implement least privilege and role-based access control for all database users\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"NIST AC-6\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"ADM_P\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor sends a large number of HTTPS requests to ADM_P, overwhelming the system\",\n",
      "    \"mitigation_suggestion\": \"Implement rate limiting and logging for all administrative access\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"NIST AC-7\"]\n",
      "  }\n",
      "]\n",
      "2025-07-27 19:50:18,652 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:18,655 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-27 19:50:18,658 - INFO - \n",
      "--- LLM Output (Identified Threats) ---\n",
      "2025-07-27 19:50:18,659 - INFO - \n",
      "--- Identified threats successfully saved to './output/identified_threats.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"threats\": [\n",
      "    {\n",
      "      \"component_name\": \"U\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Unencrypted data transmitted from U to CDN potentially disclosed\",\n",
      "      \"mitigation_suggestion\": \"Implement end-to-end encryption; Use HTTPS protocol\",\n",
      "      \"impact\": \"Medium\",\n",
      "      \"likelihood\": \"High\",\n",
      "      \"references\": [\n",
      "        \"OWASP A01:2021\",\n",
      "        \"NIST SI-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"CDN\",\n",
      "      \"stride_category\": \"Tampering\",\n",
      "      \"threat_description\": \"Malicious actor intercepts and modifies data in transit from CDN to LB\",\n",
      "      \"mitigation_suggestion\": \"Implement integrity checking; Use digital signatures\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A03:2021\",\n",
      "        \"MITRE CA-8\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"LB\",\n",
      "      \"stride_category\": \"Elevation of Privilege\",\n",
      "      \"threat_description\": \"Unprivileged actor gains elevated privileges on LB, potentially leading to DoS\",\n",
      "      \"mitigation_suggestion\": \"Implement least privilege; Restrict unnecessary access\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"NIST AC-6\",\n",
      "        \"MITRE AU-5\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"WS\",\n",
      "      \"stride_category\": \"Denial of Service\",\n",
      "      \"threat_description\": \"Volume-based DoS attack on WS, impacting application availability and performance\",\n",
      "      \"mitigation_suggestion\": \"Implement rate limiting; Monitor for suspicious activity\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A04:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"DB_P\",\n",
      "      \"stride_category\": \"Repudiation\",\n",
      "      \"threat_description\": \"Insider attempts to discredit or deny attacks on DB_P, potentially compromising audit logs\",\n",
      "      \"mitigation_suggestion\": \"Implement tamper-evident logging; Conduct regular auditing and monitoring\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Low\",\n",
      "      \"references\": [\n",
      "        \"OWASP A05:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"ADM_P\",\n",
      "      \"stride_category\": \"Spoofing\",\n",
      "      \"threat_description\": \"Attackers impersonate ADM_P, potentially gaining unauthorized access to administrative functions\",\n",
      "      \"mitigation_suggestion\": \"Implement multi-factor authentication; Conduct regular security audits\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A07:2021\",\n",
      "        \"MITRE CA-7\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T19:50:18.655020\",\n",
      "    \"source_dfd\": \"./output/dfd_components.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama python-dotenv pydantic logging instructor\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client  # Added for raw response debugging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use environment variables for paths and settings\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama3:8b\")  # Changed default to a more reliable model for testing\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists early\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LLM with Instructor for schema enforcement\n",
    "llm = instructor.from_provider(f\"ollama/{LLM_MODEL}\", mode=instructor.Mode.JSON_SCHEMA)\n",
    "\n",
    "# Added: Raw Ollama client for debugging\n",
    "ollama_client = Client()\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str = Field(description=\"Affected asset, process, data flow, or entity.\")\n",
    "    stride_category: str = Field(description=\"One STRIDE category: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\")\n",
    "    threat_description: str = Field(description=\"Clear, specific description of the threat.\")\n",
    "    mitigation_suggestion: str = Field(description=\"Practical, actionable mitigation.\")\n",
    "    impact: str = Field(description=\"Low/Medium/High based on potential damage.\")\n",
    "    likelihood: str = Field(description=\"Low/Medium/High based on exploitability.\")\n",
    "    references: list[str] = Field(description=\"Array of standard references (e.g., ['OWASP A01:2021', 'NIST SI-2']).\")\n",
    "\n",
    "class Threats(BaseModel):\n",
    "    threats: list[Threat]\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing (if input is empty) ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [\"User\", \"Attacker\"],\n",
    "    \"processes\": [\"Web Application\", \"Authentication Service\"],\n",
    "    \"data_stores\": [\"User Database\"],\n",
    "    \"data_flows\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"to\": \"Web Application\",\n",
    "            \"data\": \"Login Credentials\",\n",
    "            \"protocol\": \"HTTP\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Web Application\",\n",
    "            \"to\": \"User Database\",\n",
    "            \"data\": \"Query User Data\",\n",
    "            \"protocol\": \"SQL\"\n",
    "        }\n",
    "    ],\n",
    "    \"trust_boundaries\": [\"Internet to DMZ\", \"DMZ to Internal Network\"]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:  # Added: Check for empty data\n",
    "        logger.warning(\"--- DFD data is empty. Using sample DFD for testing ---\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "    logger.info(\"--- DFD components loaded successfully ---\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"--- Input file not found at '{DFD_INPUT_PATH}'. Using sample DFD for testing ---\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"--- FATAL ERROR: Could not parse JSON from '{DFD_INPUT_PATH}' ---\")\n",
    "    logger.error(\"The file may be corrupted or empty. Using sample DFD for testing.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- FATAL ERROR: An unexpected error occurred while loading DFD components ---\")\n",
    "    logger.error(f\"Error details: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Prompt Engineering for Threat Generation ---\n",
    "threat_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
    "\n",
    "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
    "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
    "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
    "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
    "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
    "\n",
    "For each threat, include:\n",
    "- 'component_name': Affected asset, process, data flow, or entity.\n",
    "- 'stride_category': One STRIDE category.\n",
    "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
    "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
    "- 'impact': Low/Medium/High.\n",
    "- 'likelihood': Low/Medium/High.\n",
    "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
    "\n",
    "DFD Components:\n",
    "---\n",
    "{dfd_json}\n",
    "---\n",
    "\n",
    "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "threat_prompt = ChatPromptTemplate.from_template(threat_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Invoking Local LLM to generate STRIDE threats ---\")\n",
    "try:\n",
    "    # Convert the loaded DFD dictionary back to a JSON string for the prompt\n",
    "    dfd_json_string = json.dumps(dfd_data, indent=2)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = threat_prompt.format_messages(dfd_json=dfd_json_string)\n",
    "\n",
    "    # Added: Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    # Added: Call raw Ollama for response debugging (before Instructor)\n",
    "    raw_response = ollama_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "    logger.info(f\"--- Raw LLM Response ---\\n{raw_response['message']['content']}\")\n",
    "\n",
    "    # Invoke the LLM with Instructor for structured output\n",
    "    threats_obj = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "        response_model=Threats,\n",
    "        max_retries=5  # Increased for better handling\n",
    "    )\n",
    "\n",
    "    threats_dict = threats_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    threats_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_dfd\": DFD_INPUT_PATH\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema (Instructor already enforces, but double-check)\n",
    "    try:\n",
    "        validated = ThreatsOutput(**threats_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the threats to a new file\n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(threats_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (Identified Threats) ---\")\n",
    "    print(json.dumps(threats_dict, indent=2))\n",
    "    logger.info(f\"\\n--- Identified threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during threat generation ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 20:17:51,001 - INFO - --- Setting up RAG pipeline with universal ingestion ---\n",
      "2025-07-29 20:17:54,009 - INFO - Use pytorch device_name: mps\n",
      "2025-07-29 20:17:54,010 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-07-29 20:17:57,042 - INFO - --- No existing FAISS index found. Building a new one. ---\n",
      "100%|██████████| 11/11 [10:12:47<00:00, 6449.76s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 199\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# --- Initialize RAG and OpenAI Client ---\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     rag_db = \u001b[43msetup_rag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     client = OpenAI(\n\u001b[32m    201\u001b[39m         base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    202\u001b[39m         api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mSCW_SECRET_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m     )\n\u001b[32m    204\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m--- OpenAI client initialized successfully ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36msetup_rag_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    139\u001b[39m     loader = DirectoryLoader(RAG_DOCS_DIR, glob=glob, loader_cls=loader_cls, show_progress=\u001b[38;5;28;01mTrue\u001b[39;00m, use_multithreading=\u001b[38;5;28;01mTrue\u001b[39;00m, silent_errors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     loaded_docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     documents.extend(loaded_docs)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SynologyDrive/Projects/Threatalicious/venv/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:117\u001b[39m, in \u001b[36mDirectoryLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SynologyDrive/Projects/Threatalicious/venv/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:190\u001b[39m, in \u001b[36mDirectoryLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[32m    182\u001b[39m     futures.append(\n\u001b[32m    183\u001b[39m         executor.submit(\n\u001b[32m    184\u001b[39m             \u001b[38;5;28mself\u001b[39m._lazy_load_file_to_non_generator(\u001b[38;5;28mself\u001b[39m._lazy_load_file),\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m         )\n\u001b[32m    189\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install openai pydantic logging python-dotenv langchain langchain_community langchain_huggingface faiss-cpu pypdf sentence-transformers requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "import requests  # Added for auto-download\n",
    "import fitz\n",
    "import io\n",
    "import pytesseract\n",
    "\n",
    "# RAG specific imports\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader, PyPDFLoader, TextLoader, Docx2txtLoader, CSVLoader, BSHTMLLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama-3.1-70b-instruct\") # Updated model suggestion\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_DOCS_DIR = \"rag_docs\"\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RAG_DOCS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a scanned PDF, including OCR for image-based content.\n",
    "    Returns a list of Document objects.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from scanned PDF: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    documents = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = \"\"\n",
    "        \n",
    "        # Extract native text (if any)\n",
    "        page_text = page.get_text()\n",
    "        if page_text.strip():\n",
    "            text += page_text + \"\\n\"\n",
    "        \n",
    "        # Extract images and apply OCR\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image for OCR\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(image, lang='eng')\n",
    "            if ocr_text.strip():\n",
    "                text += ocr_text + \"\\n\"\n",
    "        \n",
    "        if text.strip():\n",
    "            # Create Document object for each page\n",
    "            documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": pdf_path, \"type\": \"pdf\", \"page\": page_num + 1}\n",
    "            ))\n",
    "    \n",
    "    doc.close()\n",
    "    if not documents:\n",
    "        logger.warning(f\"No text extracted from {pdf_path}. Check OCR setup or PDF content.\")\n",
    "    return documents\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"\n",
    "    Extract text from a standalone image using OCR.\n",
    "    Returns a list of Document objects (typically one per image).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from image: {image_path}\")\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image, lang='eng')\n",
    "        if text.strip():\n",
    "            return [Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": image_path, \"type\": \"image\"}\n",
    "            )]\n",
    "        else:\n",
    "            logger.warning(f\"No text extracted from image {image_path}.\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to process image {image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def setup_rag_pipeline():\n",
    "    \"\"\"Initializes the RAG pipeline by creating or loading a FAISS vector store with universal ingestion.\"\"\"\n",
    "    logger.info(\"--- Setting up RAG pipeline with universal ingestion ---\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    if os.path.exists(FAISS_INDEX_PATH):\n",
    "        logger.info(f\"--- Loading existing FAISS index from '{FAISS_INDEX_PATH}' ---\")\n",
    "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        logger.info(\"--- No existing FAISS index found. Building a new one. ---\")\n",
    "        \n",
    "        # Expanded loaders for universal support\n",
    "        loaders = {\n",
    "            \"**/*.pdf\": PyPDFLoader,  # For text-based PDFs\n",
    "            \"**/*.md\": TextLoader,\n",
    "            \"**/*.txt\": TextLoader,\n",
    "            \"**/*.docx\": Docx2txtLoader,\n",
    "            \"**/*.csv\": CSVLoader,\n",
    "            \"**/*.html\": BSHTMLLoader\n",
    "        }\n",
    "        documents = []\n",
    "        \n",
    "        # Load standard documents\n",
    "        for glob, loader_cls in loaders.items():\n",
    "            try:\n",
    "                loader = DirectoryLoader(RAG_DOCS_DIR, glob=glob, loader_cls=loader_cls, show_progress=True, use_multithreading=True, silent_errors=True)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load files with pattern {glob} using {loader_cls.__name__}. Error: {e}\")\n",
    "\n",
    "        # Handle scanned PDFs with OCR (if PyPDFLoader didn't extract text)\n",
    "        pdf_files = [os.path.join(root, f) for root, _, files in os.walk(RAG_DOCS_DIR) for f in files if f.endswith(\".pdf\")]\n",
    "        for pdf_path in pdf_files:\n",
    "            try:\n",
    "                # Try PyPDFLoader first\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                if any(doc.page_content.strip() for doc in docs):\n",
    "                    documents.extend(docs)\n",
    "                else:\n",
    "                    # Fallback to OCR\n",
    "                    logger.info(f\"No text extracted with PyPDFLoader for {pdf_path}. Attempting OCR.\")\n",
    "                    ocr_docs = extract_text_from_scanned_pdf(pdf_path)\n",
    "                    documents.extend(ocr_docs)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not process PDF {pdf_path}. Error: {e}\")\n",
    "\n",
    "        # Handle standalone images with OCR\n",
    "        image_extensions = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")\n",
    "        image_files = [os.path.join(root, f) for root, _, files in os.walk(RAG_DOCS_DIR) for f in files if f.lower().endswith(image_extensions)]\n",
    "        for image_path in image_files:\n",
    "            ocr_docs = extract_text_from_image(image_path)\n",
    "            documents.extend(ocr_docs)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(f\"--- No supported documents found in '{RAG_DOCS_DIR}'. Attempting to auto-download key resources ---\")\n",
    "            # Auto-download OWASP Top 10 2021 PDF\n",
    "            owasp_url = \"https://owasp.org/Top10/assets/PDF/OWASP-Top-10-2021.pdf\"\n",
    "            try:\n",
    "                response = requests.get(owasp_url)\n",
    "                response.raise_for_status()\n",
    "                owasp_path = os.path.join(RAG_DOCS_DIR, \"owasp_top10_2021.pdf\")\n",
    "                with open(owasp_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                logger.info(f\"--- Downloaded OWASP Top 10 2021 PDF to '{owasp_path}' ---\")\n",
    "                # Try PyPDFLoader for OWASP PDF (it's text-based)\n",
    "                pdf_loader = PyPDFLoader(owasp_path)\n",
    "                documents.extend(pdf_loader.load())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"--- Failed to auto-download OWASP PDF: {e} ---\")\n",
    "                raise ValueError(f\"No documents available for RAG. Please add files to '{RAG_DOCS_DIR}'.\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        logger.info(f\"--- Creating FAISS index from {len(docs)} document chunks. This may take a moment... ---\")\n",
    "        db = FAISS.from_documents(docs, embeddings)\n",
    "        db.save_local(FAISS_INDEX_PATH)\n",
    "        logger.info(f\"--- FAISS index created and saved to '{FAISS_INDEX_PATH}' ---\")\n",
    "        \n",
    "    return db\n",
    "\n",
    "# --- Initialize RAG and OpenAI Client ---\n",
    "try:\n",
    "    rag_db = setup_rag_pipeline()\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\",\n",
    "        api_key=os.getenv(\"SCW_SECRET_KEY\")\n",
    "    )\n",
    "    logger.info(\"--- OpenAI client initialized successfully ---\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- Failed to initialize services: {e} ---\")\n",
    "    raise\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str\n",
    "    stride_category: str\n",
    "    threat_description: str\n",
    "    mitigation_suggestion: str\n",
    "    impact: str\n",
    "    likelihood: str\n",
    "    references: list[str]\n",
    "    risk_score: str\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [{\"name\": \"User\"}],\n",
    "    \"processes\": [{\"name\": \"Web Application\"}, {\"name\": \"Authentication Service\"}],\n",
    "    \"data_stores\": [{\"name\": \"User Database\"}],\n",
    "    \"data_flows\": [\n",
    "        {\"source\": \"User\", \"destination\": \"Web Application\", \"data_description\": \"Login Credentials\", \"protocol\": \"HTTPS\"},\n",
    "        {\"source\": \"Web Application\", \"destination\": \"User Database\", \"data_description\": \"Query User Data\", \"protocol\": \"SQL\"}\n",
    "    ],\n",
    "    \"trust_boundaries\": [{\"name\": \"Internet to DMZ\"}]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:\n",
    "        logger.warning(f\"DFD file at '{DFD_INPUT_PATH}' is empty. Using sample DFD for demonstration.\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"DFD file not found at '{DFD_INPUT_PATH}'. Using sample DFD for demonstration.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError as e:\n",
    "    logger.error(f\"FATAL: Error decoding JSON from '{DFD_INPUT_PATH}': {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"FATAL: Error loading DFD: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Enhanced Prompting Strategy ---\n",
    "\n",
    "# **FIX 1: Define STRIDE categories explicitly to ensure systematic coverage.**\n",
    "# Parametrized: Load from config or file if needed\n",
    "STRIDE_DEFINITIONS = {\n",
    "    \"S\": (\"Spoofing\", \"Illegitimately accessing systems or data by impersonating a user, process, or component.\"),\n",
    "    \"T\": (\"Tampering\", \"Unauthorized modification of data, either in transit or at rest.\"),\n",
    "    \"R\": (\"Repudiation\", \"A user or system denying that they performed an action, often due to a lack of sufficient proof (e.g., logs).\"),\n",
    "    \"I\": (\"Information Disclosure\", \"Exposing sensitive information to unauthorized individuals.\"),\n",
    "    \"D\": (\"Denial of Service\", \"Preventing legitimate users from accessing a system or service.\"),\n",
    "    \"E\": (\"Elevation of Privilege\", \"A user or process gaining rights beyond their authorized level.\")\n",
    "}\n",
    "\n",
    "# Optional: Load custom STRIDE from file\n",
    "stride_config_path = \"stride_config.json\"\n",
    "if os.path.exists(stride_config_path):\n",
    "    with open(stride_config_path, 'r') as f:\n",
    "        STRIDE_DEFINITIONS = json.load(f)\n",
    "    logger.info(\"--- Loaded custom STRIDE definitions from 'stride_config.json' ---\")\n",
    "\n",
    "# **FIX 2: Create a highly specific prompt template focused on a SINGLE STRIDE category.**\n",
    "# This prevents generic responses and forces the model to generate relevant, accurate threats.\n",
    "threat_prompt_template_specific_rag = \"\"\"\n",
    "You are a cybersecurity architect specializing in threat modeling using the STRIDE methodology.\n",
    "Your task is to generate 1-2 specific threats for a given DFD component, focusing ONLY on a single STRIDE category.\n",
    "\n",
    "**DFD Component to Analyze:**\n",
    "{component_info}\n",
    "\n",
    "**STRIDE Category to Focus On:**\n",
    "- **{stride_category} ({stride_name}):** {stride_definition}\n",
    "\n",
    "**Security Context from Knowledge Base (for accuracy):**\n",
    "'''\n",
    "{rag_context}\n",
    "'''\n",
    "\n",
    "**Instructions:**\n",
    "1.  Generate 1-2 distinct and realistic threats for the component that fall **strictly** under the '{stride_name}' category.\n",
    "2.  **Be specific.** Relate the threat directly to the component's type and details. For a database, a Spoofing threat is a spoofed connection, not user impersonation. For a data flow, a Tampering threat is a Man-in-the-Middle attack.\n",
    "3.  Use the provided Security Context to create specific descriptions, **actionable mitigations**, and accurate references (e.g., CWE, OWASP Cheat Sheets). Do not invent references.\n",
    "4.  Provide a realistic risk assessment (Impact, Likelihood, Score).\n",
    "5.  Output ONLY a valid JSON object with a single key \"threats\", containing a list of threat objects. Do not include any other text or commentary.\n",
    "\n",
    "**JSON Threat Object Schema:**\n",
    "{{\n",
    "  \"component_name\": \"string (the name of the component being analyzed)\",\n",
    "  \"stride_category\": \"{stride_category}\",\n",
    "  \"threat_description\": \"string (Specific to the component and STRIDE category)\",\n",
    "  \"mitigation_suggestion\": \"string (Actionable and specific)\",\n",
    "  \"impact\": \"Low, Medium, or High\",\n",
    "  \"likelihood\": \"Low, Medium, or High\",\n",
    "  \"references\": [\"list of strings, e.g., 'OWASP A01:2021', 'CWE-89'\"],\n",
    "  \"risk_score\": \"Critical, High, Medium, or Low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "retry_prompt_addition = \" Generate at least one threat if realistically applicable, even if minor.\"\n",
    "\n",
    "# Function to calculate risk_score\n",
    "def calculate_risk_score(impact, likelihood):\n",
    "    if impact == \"High\" and likelihood in [\"Medium\", \"High\"]:\n",
    "        return \"Critical\"\n",
    "    elif (impact == \"High\" and likelihood == \"Low\") or (impact == \"Medium\" and likelihood == \"High\"):\n",
    "        return \"High\"\n",
    "    elif (impact == \"Medium\" and likelihood in [\"Medium\", \"Low\"]) or (impact == \"Low\" and likelihood == \"High\"):\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# --- Main Invocation Logic ---\n",
    "logger.info(\"\\n--- Invoking LLM with RAG to systematically generate STRIDE threats ---\")\n",
    "all_threats = []\n",
    "try:\n",
    "    components_to_analyze = []\n",
    "    for key, value in dfd_data.items():\n",
    "        if isinstance(value, list) and value:\n",
    "            for item in value:\n",
    "                # Ensure component has a name for better identification\n",
    "                if isinstance(item, dict) and item.get(\"name\"):\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "                elif isinstance(item, dict): # Fallback for components without a 'name' field\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "\n",
    "\n",
    "    # **FIX 3: Iterate through each component AND each STRIDE category.**\n",
    "    # This loop structure ensures every category is considered for every component.\n",
    "    for component in components_to_analyze:\n",
    "        component_str = json.dumps(component)\n",
    "        component_name = component.get(\"details\", {}).get(\"name\", component_str)\n",
    "        logger.info(f\"\\n--- Analyzing component: {component_name} ---\")\n",
    "\n",
    "        retrieved_docs = rag_db.similarity_search(component_str, k=5)  # Increased to 5 for broader context\n",
    "        rag_context = \"\\n---\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        logger.info(\"--- Retrieved RAG context for component ---\")\n",
    "\n",
    "        for cat_letter, (cat_name, cat_def) in STRIDE_DEFINITIONS.items():\n",
    "            logger.info(f\"--- Generating threats for STRIDE category: {cat_name} ---\")\n",
    "            \n",
    "            prompt = threat_prompt_template_specific_rag.format(\n",
    "                component_info=component_str,\n",
    "                rag_context=rag_context,\n",
    "                stride_category=cat_letter,\n",
    "                stride_name=cat_name,\n",
    "                stride_definition=cat_def\n",
    "            )\n",
    "            \n",
    "            retry_count = 0\n",
    "            max_retries = 1  # Retry once if no threats\n",
    "            threats = []\n",
    "            while retry_count <= max_retries and not threats:\n",
    "                try:\n",
    "                    if retry_count > 0:\n",
    "                        prompt += retry_prompt_addition  # Add retry instruction\n",
    "                    \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=LLM_MODEL,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        response_format={\"type\": \"json_object\"},\n",
    "                        max_tokens=2048,\n",
    "                        temperature=0.4 # Slightly lower temp for more focused output\n",
    "                    )\n",
    "                    \n",
    "                    response_content = response.choices[0].message.content\n",
    "                    generated_data = json.loads(response_content)\n",
    "                    \n",
    "                    # Ensure the response is a dict with a 'threats' key which is a list\n",
    "                    if isinstance(generated_data, dict) and isinstance(generated_data.get(\"threats\"), list):\n",
    "                        threats = generated_data[\"threats\"]\n",
    "                        # Add component name if missing from LLM response\n",
    "                        for threat in threats:\n",
    "                            if 'component_name' not in threat or not threat['component_name']:\n",
    "                                threat['component_name'] = component_name\n",
    "                        logger.info(f\"--- Successfully generated {len(threats)} threat(s) for category {cat_name} ---\")\n",
    "                    else:\n",
    "                        logger.warning(f\"--- LLM response for {cat_name} on {component_name} had unexpected structure. ---\")\n",
    "                        logger.debug(f\"Raw Response: {response_content}\")\n",
    "\n",
    "                except (json.JSONDecodeError, AttributeError) as e:\n",
    "                    logger.warning(f\"--- Could not parse LLM response for {cat_name} on {component_name}: {e} ---\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"--- An API error occurred for {cat_name} on {component_name}: {e} ---\")\n",
    "                \n",
    "                retry_count += 1\n",
    "\n",
    "            all_threats.extend(threats)\n",
    "\n",
    "    # Deduplication: Remove exact duplicates based on description\n",
    "    unique_threats = []\n",
    "    seen_descriptions = set()\n",
    "    for threat in all_threats:\n",
    "        desc = threat.get('threat_description', '')\n",
    "        if desc not in seen_descriptions:\n",
    "            seen_descriptions.add(desc)\n",
    "            # Recalculate risk_score\n",
    "            threat['risk_score'] = calculate_risk_score(threat.get('impact', 'Low'), threat.get('likelihood', 'Low'))\n",
    "            unique_threats.append(threat)\n",
    "    all_threats = unique_threats\n",
    "    logger.info(f\"--- Deduplicated threats: {len(all_threats)} unique threats remaining ---\")\n",
    "\n",
    "    # --- Final Processing and Validation ---\n",
    "    risk_order = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1, \"Informational\": 0}\n",
    "    all_threats.sort(key=lambda t: risk_order.get(t.get('risk_score', 'Low'), 0), reverse=True)\n",
    "\n",
    "    final_output = {\n",
    "        \"threats\": all_threats,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_dfd\": os.path.basename(DFD_INPUT_PATH),\n",
    "            \"llm_model\": LLM_MODEL,\n",
    "            \"rag_index\": FAISS_INDEX_PATH\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        validated_output = ThreatsOutput(**final_output)\n",
    "        logger.info(\"--- Final JSON output validated successfully against schema ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- FINAL JSON VALIDATION FAILED: {ve} ---\")\n",
    "        \n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    logger.info(\"\\n--- LLM RAG Output (Identified Threats) ---\")\n",
    "    # print(json.dumps(final_output, indent=2))\n",
    "    logger.info(f\"\\n--- Identified {len(all_threats)} threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during the threat generation process ---\")\n",
    "    logger.error(f\"Error: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793d59d",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (1542674646.py, line 660)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 660\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.logger.info(\"Output validation successful\")\u001b[39m\n                                                    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71b796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
