{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572111a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 11:46:56,370 - INFO - \n",
      "--- Starting Pre-Filter for Document Extraction ---\n",
      "2025-07-29 11:46:56,391 - INFO - --- Scaleway OpenAI client initialized successfully ---\n",
      "2025-07-29 11:46:56,391 - INFO - --- Loading documents from './input_documents' ---\n",
      "2025-07-29 11:46:56,392 - WARNING - --- No valid documents found. Using sample document content ---\n",
      "2025-07-29 11:46:56,392 - INFO - --- Prompt sent to LLM ---\n",
      "\n",
      "You are a senior cybersecurity analyst specializing in threat modeling. Your task is to extract structured information from multiple input documents describing a system architecture and transform it into a standardized JSON format for a Data Flow Diagram (DFD). The documents may include architecture diagrams, design specs, or text descriptions in varied formats.\n",
      "\n",
      "Using Chain-of-Thought reasoning:\n",
      "1. Identify and extract key elements: project metadata (name, version, industry), external entities, assets (e.g., databases), processes, trust boundaries, and data flows.\n",
      "2. Normalize component names (e.g., use 'DB_P' for 'Profile Database' if abbreviated elsewhere).\n",
      "3. For data flows, capture source, destination, data description, classification (e.g., 'Confidential', 'PII'), protocol, and authentication mechanism.\n",
      "4. Resolve conflicts across documents by prioritizing the most detailed description.\n",
      "5. If information is ambiguous, flag it in the metadata with an 'assumptions' key.\n",
      "\n",
      "Output a JSON object with:\n",
      "- 'project_name': Project name (default: 'Unknown Project' if not specified).\n",
      "- 'project_version': Version (default: '1.0').\n",
      "- 'industry_context': Industry (default: 'Unknown').\n",
      "- 'external_entities': List of external entities (e.g., ['U', 'Attacker']).\n",
      "- 'assets': List of assets like databases (e.g., ['DB_P', 'DB_B']).\n",
      "- 'processes': List of processes (e.g., ['CDN', 'LB', 'WS']).\n",
      "- 'trust_boundaries': List of trust boundaries (e.g., ['Public Zone to Edge Zone']).\n",
      "- 'data_flows': List of data flow objects with source, destination, data_description, data_classification, protocol, and authentication_mechanism.\n",
      "\n",
      "Input Documents:\n",
      "---\n",
      "\n",
      "System: Web Application Security Model, Version 1.1, Finance Industry\n",
      "External Entities: User (U), External Attacker\n",
      "Assets: Profile Database (DB_P), Billing Database (DB_B)\n",
      "Processes: Content Delivery Network (CDN), Load Balancer (LB), Web Server (WS), Message Queue (MQ), Worker (WRK), Admin Service (ADM), Admin Portal (ADM_P)\n",
      "Trust Boundaries: Public Zone to Edge Zone, Edge Zone to Application DMZ, Application DMZ to Internal Core, Internal Core to Data Zone, Management Zone to Application DMZ\n",
      "Data Flows:\n",
      "- From User to CDN: User session tokens and requests for static assets, Confidential, HTTPS, JWT in Header\n",
      "- From CDN to LB: Cached content and user requests, Confidential, HTTPS, mTLS\n",
      "- From WS to DB_P: User profile data including names and email addresses, PII, JDBC/ODBC over TLS, Database Credentials from Secrets Manager\n",
      "\n",
      "---\n",
      "\n",
      "Output ONLY the JSON, with no additional commentary or formatting.\n",
      "\n",
      "2025-07-29 11:47:09,167 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-29 11:47:21,447 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-29 11:47:21,450 - INFO - --- Raw Scaleway Response ---\n",
      "{\"project_name\": \"Web Application Security Model\", \"project_version\": \"1.1\", \"industry_context\": \"Finance\", \"external_entities\": [\"U\", \"Attacker\"], \"assets\": [\"DB_P\", \"DB_B\"], \"processes\": [\"CDN\", \"LB\", \"WS\", \"MQ\", \"WRK\", \"ADM\", \"ADM_P\"], \"trust_boundaries\": [\"Public Zone to Edge Zone\", \"Edge Zone to Application DMZ\", \"Application DMZ to Internal Core\", \"Internal Core to Data Zone\", \"Management Zone to Application DMZ\"], \"data_flows\": [{\"source\": \"U\", \"destination\": \"CDN\", \"data_description\": \"User session tokens and requests for static assets\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"JWT in Header\"}, {\"source\": \"CDN\", \"destination\": \"LB\", \"data_description\": \"Cached content and user requests\", \"data_classification\": \"Confidential\", \"protocol\": \"HTTPS\", \"authentication_mechanism\": \"mTLS\"}, {\"source\": \"WS\", \"destination\": \"DB_P\", \"data_description\": \"User profile data including names and email addresses\", \"data_classification\": \"PII\", \"protocol\": \"JDBC/ODBC over TLS\", \"authentication_mechanism\": \"Database Credentials from Secrets Manager\"}]}\n",
      "2025-07-29 11:47:21,451 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-29 11:47:21,455 - INFO - \n",
      "--- LLM Output (DFD Components) ---\n",
      "2025-07-29 11:47:21,456 - INFO - \n",
      "--- DFD components successfully saved to './output/dfd_components.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dfd\": {\n",
      "    \"project_name\": \"Web Application Security Model\",\n",
      "    \"project_version\": \"1.1\",\n",
      "    \"industry_context\": \"Finance\",\n",
      "    \"external_entities\": [\n",
      "      \"U\",\n",
      "      \"Attacker\"\n",
      "    ],\n",
      "    \"assets\": [\n",
      "      \"DB_P\",\n",
      "      \"DB_B\"\n",
      "    ],\n",
      "    \"processes\": [\n",
      "      \"CDN\",\n",
      "      \"LB\",\n",
      "      \"WS\",\n",
      "      \"MQ\",\n",
      "      \"WRK\",\n",
      "      \"ADM\",\n",
      "      \"ADM_P\"\n",
      "    ],\n",
      "    \"trust_boundaries\": [\n",
      "      \"Public Zone to Edge Zone\",\n",
      "      \"Edge Zone to Application DMZ\",\n",
      "      \"Application DMZ to Internal Core\",\n",
      "      \"Internal Core to Data Zone\",\n",
      "      \"Management Zone to Application DMZ\"\n",
      "    ],\n",
      "    \"data_flows\": [\n",
      "      {\n",
      "        \"source\": \"U\",\n",
      "        \"destination\": \"CDN\",\n",
      "        \"data_description\": \"User session tokens and requests for static assets\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"JWT in Header\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"CDN\",\n",
      "        \"destination\": \"LB\",\n",
      "        \"data_description\": \"Cached content and user requests\",\n",
      "        \"data_classification\": \"Confidential\",\n",
      "        \"protocol\": \"HTTPS\",\n",
      "        \"authentication_mechanism\": \"mTLS\"\n",
      "      },\n",
      "      {\n",
      "        \"source\": \"WS\",\n",
      "        \"destination\": \"DB_P\",\n",
      "        \"data_description\": \"User profile data including names and email addresses\",\n",
      "        \"data_classification\": \"PII\",\n",
      "        \"protocol\": \"JDBC/ODBC over TLS\",\n",
      "        \"authentication_mechanism\": \"Database Credentials from Secrets Manager\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-29T11:47:21.451112\",\n",
      "    \"source_documents\": [],\n",
      "    \"assumptions\": [],\n",
      "    \"llm_provider\": \"scaleway\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client\n",
    "import PyPDF2\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"scaleway\").lower()  # Default to 'ollama', can be set to 'scaleway'\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama-3.3-70b-instruct\")\n",
    "SCW_API_URL = os.getenv(\"SCW_API_URL\", \"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\")\n",
    "SCW_SECRET_KEY = os.getenv(\"SCW_SECRET_KEY\")\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./input_documents\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "DFD_OUTPUT_PATH = os.getenv(\"DFD_OUTPUT_PATH\", os.path.join(OUTPUT_DIR, \"dfd_components.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Initialize LLM Client ---\n",
    "def initialize_llm_client():\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        if not SCW_SECRET_KEY:\n",
    "            raise ValueError(\"SCW_SECRET_KEY environment variable is required for Scaleway API.\")\n",
    "        try:\n",
    "            client = instructor.from_openai(OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY))\n",
    "            logger.info(\"--- Scaleway OpenAI client initialized successfully ---\")\n",
    "            return client, \"scaleway\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Scaleway client: {e} ---\")\n",
    "            raise\n",
    "    else:  # Default to Ollama\n",
    "        try:\n",
    "            raw_client = Client()  # Raw Ollama client for debugging\n",
    "            # Patch the Ollama client with instructor for structured output\n",
    "            instructor_client = instructor.patch(Client())\n",
    "            logger.info(\"--- Ollama client initialized successfully ---\")\n",
    "            return raw_client, instructor_client, \"ollama\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"--- Failed to initialize Ollama client: {e} ---\")\n",
    "            raise\n",
    "\n",
    "# --- DFD Schema for Validation ---\n",
    "class DataFlow(BaseModel):\n",
    "    source: str = Field(description=\"Source component of the data flow (e.g., 'U' for User).\")\n",
    "    destination: str = Field(description=\"Destination component of the data flow (e.g., 'CDN').\")\n",
    "    data_description: str = Field(description=\"Description of data being transferred (e.g., 'User session tokens').\")\n",
    "    data_classification: str = Field(description=\"Classification like 'Confidential', 'PII', or 'Public'.\")\n",
    "    protocol: str = Field(description=\"Protocol used (e.g., 'HTTPS', 'JDBC/ODBC over TLS').\")\n",
    "    authentication_mechanism: str = Field(description=\"Authentication method (e.g., 'JWT in Header').\")\n",
    "\n",
    "class DFDComponents(BaseModel):\n",
    "    project_name: str = Field(description=\"Name of the project (e.g., 'Web Application Security Model').\")\n",
    "    project_version: str = Field(description=\"Version of the project (e.g., '1.1').\")\n",
    "    industry_context: str = Field(description=\"Industry context (e.g., 'Finance').\")\n",
    "    external_entities: list[str] = Field(description=\"List of external entities (e.g., ['U', 'Attacker']).\")\n",
    "    assets: list[str] = Field(description=\"List of assets like data stores (e.g., ['DB_P', 'DB_B']).\")\n",
    "    processes: list[str] = Field(description=\"List of processes (e.g., ['CDN', 'LB', 'WS']).\")\n",
    "    trust_boundaries: list[str] = Field(description=\"List of trust boundaries (e.g., ['Public Zone to Edge Zone']).\")\n",
    "    data_flows: list[DataFlow] = Field(description=\"List of data flows between components.\")\n",
    "\n",
    "class DFDOutput(BaseModel):\n",
    "    dfd: DFDComponents\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample Input for Testing (if no documents are found) ---\n",
    "SAMPLE_DOCUMENT_CONTENT = \"\"\"\n",
    "System: Web Application Security Model, Version 1.1, Finance Industry\n",
    "External Entities: User (U), External Attacker\n",
    "Assets: Profile Database (DB_P), Billing Database (DB_B)\n",
    "Processes: Content Delivery Network (CDN), Load Balancer (LB), Web Server (WS), Message Queue (MQ), Worker (WRK), Admin Service (ADM), Admin Portal (ADM_P)\n",
    "Trust Boundaries: Public Zone to Edge Zone, Edge Zone to Application DMZ, Application DMZ to Internal Core, Internal Core to Data Zone, Management Zone to Application DMZ\n",
    "Data Flows:\n",
    "- From User to CDN: User session tokens and requests for static assets, Confidential, HTTPS, JWT in Header\n",
    "- From CDN to LB: Cached content and user requests, Confidential, HTTPS, mTLS\n",
    "- From WS to DB_P: User profile data including names and email addresses, PII, JDBC/ODBC over TLS, Database Credentials from Secrets Manager\n",
    "\"\"\"\n",
    "\n",
    "# --- Load and Parse Documents ---\n",
    "def load_documents(input_dir):\n",
    "    logger.info(f\"--- Loading documents from '{input_dir}' ---\")\n",
    "    documents = []\n",
    "    for file_path in glob.glob(os.path.join(input_dir, \"*.[tT][xX][tT]\")) + glob.glob(os.path.join(input_dir, \"*.[pP][dD][fF]\")):\n",
    "        try:\n",
    "            if file_path.lower().endswith(\".txt\"):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                logger.info(f\"Loaded text file: {file_path}\")\n",
    "            elif file_path.lower().endswith(\".pdf\"):\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\".join(page.extract_text() for page in pdf_reader.pages if page.extract_text())\n",
    "                    documents.append(text)\n",
    "                logger.info(f\"Loaded PDF file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {file_path}: {e}\")\n",
    "    if not documents:\n",
    "        logger.warning(\"--- No valid documents found. Using sample document content ---\")\n",
    "        documents = [SAMPLE_DOCUMENT_CONTENT]\n",
    "    return documents\n",
    "\n",
    "# --- Prompt Engineering for Document Extraction ---\n",
    "extract_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling. Your task is to extract structured information from multiple input documents describing a system architecture and transform it into a standardized JSON format for a Data Flow Diagram (DFD). The documents may include architecture diagrams, design specs, or text descriptions in varied formats.\n",
    "\n",
    "Using Chain-of-Thought reasoning:\n",
    "1. Identify and extract key elements: project metadata (name, version, industry), external entities, assets (e.g., databases), processes, trust boundaries, and data flows.\n",
    "2. Normalize component names (e.g., use 'DB_P' for 'Profile Database' if abbreviated elsewhere).\n",
    "3. For data flows, capture source, destination, data description, classification (e.g., 'Confidential', 'PII'), protocol, and authentication mechanism.\n",
    "4. Resolve conflicts across documents by prioritizing the most detailed description.\n",
    "5. If information is ambiguous, flag it in the metadata with an 'assumptions' key.\n",
    "\n",
    "Output a JSON object with:\n",
    "- 'project_name': Project name (default: 'Unknown Project' if not specified).\n",
    "- 'project_version': Version (default: '1.0').\n",
    "- 'industry_context': Industry (default: 'Unknown').\n",
    "- 'external_entities': List of external entities (e.g., ['U', 'Attacker']).\n",
    "- 'assets': List of assets like databases (e.g., ['DB_P', 'DB_B']).\n",
    "- 'processes': List of processes (e.g., ['CDN', 'LB', 'WS']).\n",
    "- 'trust_boundaries': List of trust boundaries (e.g., ['Public Zone to Edge Zone']).\n",
    "- 'data_flows': List of data flow objects with source, destination, data_description, data_classification, protocol, and authentication_mechanism.\n",
    "\n",
    "Input Documents:\n",
    "---\n",
    "{documents}\n",
    "---\n",
    "\n",
    "Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_template(extract_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Starting Pre-Filter for Document Extraction ---\")\n",
    "try:\n",
    "    # Initialize LLM client\n",
    "    if LLM_PROVIDER == \"scaleway\":\n",
    "        client, client_type = initialize_llm_client()\n",
    "    else:\n",
    "        raw_client, instructor_client, client_type = initialize_llm_client()\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(INPUT_DIR)\n",
    "    documents_combined = \"\\n--- Document Separator ---\\n\".join(documents)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = extract_prompt.format_messages(documents=documents_combined)\n",
    "\n",
    "    # Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    if client_type == \"scaleway\":\n",
    "        # Use instructor client for Scaleway\n",
    "        dfd_obj = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_client = OpenAI(base_url=SCW_API_URL, api_key=SCW_SECRET_KEY)\n",
    "        raw_response = raw_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        logger.info(f\"--- Raw Scaleway Response ---\\n{raw_response.choices[0].message.content}\")\n",
    "        # Log token usage for Scaleway\n",
    "        if hasattr(raw_response, 'usage'):\n",
    "            prompt_tokens = raw_response.usage.prompt_tokens or 'N/A'\n",
    "            completion_tokens = raw_response.usage.completion_tokens or 'N/A'\n",
    "            total_tokens = raw_response.usage.total_tokens or 'N/A'\n",
    "            logger.info(f\"--- Token Usage for Scaleway ---\")\n",
    "            logger.info(f\"Input Tokens: {prompt_tokens}\")\n",
    "            logger.info(f\"Output Tokens: {completion_tokens}\")\n",
    "            logger.info(f\"Total Tokens: {total_tokens}\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        # Use instructor client for Ollama\n",
    "        dfd_obj = instructor_client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "            response_model=DFDComponents,\n",
    "            max_retries=5\n",
    "        )\n",
    "        # Log raw response for debugging\n",
    "        raw_response = raw_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "        logger.info(f\"--- Raw Ollama Response ---\\n{raw_response['message']['content']}\")\n",
    "        # Log Token Count and Performance\n",
    "        prompt_tokens = raw_response.get('prompt_eval_count', 'N/A')\n",
    "        prompt_duration_ns = raw_response.get('prompt_eval_duration', 0)\n",
    "        response_tokens = raw_response.get('eval_count', 'N/A')\n",
    "        response_duration_ns = raw_response.get('eval_duration', 0)\n",
    "        prompt_duration_s = f\"{prompt_duration_ns / 1_000_000_000:.2f}s\" if prompt_duration_ns else \"N/A\"\n",
    "        response_duration_s = f\"{response_duration_ns / 1_000_000_000:.2f}s\" if response_duration_ns else \"N/A\"\n",
    "        logger.info(f\"--- Token Usage & Performance ---\")\n",
    "        logger.info(f\"Input Tokens: {prompt_tokens} (processed in {prompt_duration_s})\")\n",
    "        logger.info(f\"Output Tokens: {response_tokens} (generated in {response_duration_s})\")\n",
    "\n",
    "    dfd_dict = dfd_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    output_dict = {\n",
    "        \"dfd\": dfd_dict,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_documents\": glob.glob(os.path.join(INPUT_DIR, \"*.[tT][xX][tT]\")) + glob.glob(os.path.join(INPUT_DIR, \"*.[pP][dD][fF]\")),\n",
    "            \"assumptions\": [],\n",
    "            \"llm_provider\": LLM_PROVIDER\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema\n",
    "    try:\n",
    "        validated = DFDOutput(**output_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the DFD components to a file\n",
    "    with open(DFD_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(output_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (DFD Components) ---\")\n",
    "    print(json.dumps(output_dict, indent=2))\n",
    "    logger.info(f\"\\n--- DFD components successfully saved to '{DFD_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during document extraction ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 19:49:51,081 - INFO - Initializing ollama provider with model llama3:8b\n",
      "2025-07-27 19:49:51,103 - INFO - Client initialized\n",
      "2025-07-27 19:49:51,118 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-27 19:49:51,119 - INFO - --- DFD components loaded successfully ---\n",
      "2025-07-27 19:49:51,119 - INFO - \n",
      "--- Invoking Local LLM to generate STRIDE threats ---\n",
      "2025-07-27 19:49:51,120 - INFO - --- Prompt sent to LLM ---\n",
      "\n",
      "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
      "\n",
      "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
      "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
      "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
      "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
      "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
      "\n",
      "For each threat, include:\n",
      "- 'component_name': Affected asset, process, data flow, or entity.\n",
      "- 'stride_category': One STRIDE category.\n",
      "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
      "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
      "- 'impact': Low/Medium/High.\n",
      "- 'likelihood': Low/Medium/High.\n",
      "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
      "\n",
      "DFD Components:\n",
      "---\n",
      "{\n",
      "  \"external_entities\": [\n",
      "    \"U\"\n",
      "  ],\n",
      "  \"assets\": [\n",
      "    \"DB_P\",\n",
      "    \"DB_B\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"CDN\",\n",
      "    \"LB\",\n",
      "    \"WS\",\n",
      "    \"MQ\",\n",
      "    \"WRK\",\n",
      "    \"ADM\",\n",
      "    \"ADM_P\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"U\",\n",
      "      \"destination\": \"CDN\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"CDN\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"LB\",\n",
      "      \"destination\": \"WS\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WS\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"MQ\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"WRK\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"DB_B\",\n",
      "      \"destination\": \"DB_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"Backup Protocol over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM\",\n",
      "      \"destination\": \"ADM_P\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS over VPN\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ADM_P\",\n",
      "      \"destination\": \"LB\",\n",
      "      \"data_description\": \"\",\n",
      "      \"protocol\": \"HTTPS\"\n",
      "    }\n",
      "  ],\n",
      "  \"trust_boundaries\": [\n",
      "    \"Public Zone to Edge Zone\",\n",
      "    \"Edge Zone to Application DMZ\",\n",
      "    \"Application DMZ to Internal Core\",\n",
      "    \"Internal Core to Data Zone\",\n",
      "    \"Management Zone to Application DMZ\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T16:38:38.774870\",\n",
      "    \"source_document\": \"./docs/designdoc.docx\"\n",
      "  }\n",
      "}\n",
      "---\n",
      "\n",
      "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
      "\n",
      "2025-07-27 19:50:05,251 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:05,253 - INFO - --- Raw LLM Response ---\n",
      "[\n",
      "  {\n",
      "    \"component_name\": \"U\",\n",
      "    \"stride_category\": \"Spoofing\",\n",
      "    \"threat_description\": \"Attacker impersonates user U to access sensitive data\",\n",
      "    \"mitigation_suggestion\": \"Implement MFA and rate limiting for all authentication attempts\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A01:2021\", \"NIST AC-2\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"CDN\",\n",
      "    \"stride_category\": \"Tampering\",\n",
      "    \"threat_description\": \"Malicious actor injects malicious code into CDN's response\",\n",
      "    \"mitigation_suggestion\": \"Implement Web Application Firewall (WAF) and regular security updates for CDN\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A03:2021\", \"NIST SI-3\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"LB\",\n",
      "    \"stride_category\": \"Repudiation\",\n",
      "    \"threat_description\": \"Attacker alters load balancer configuration to deny service to legitimate users\",\n",
      "    \"mitigation_suggestion\": \"Implement logging and monitoring for load balancer, and have a backup plan in place\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A04:2021\", \"NIST AC-6\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"WS\",\n",
      "    \"stride_category\": \"Information Disclosure\",\n",
      "    \"threat_description\": \"Unencrypted data is transmitted from WS to DB_P, allowing eavesdropping\",\n",
      "    \"mitigation_suggestion\": \"Implement TLS 1.3 with certificate pinning for all data flows from WS to DB_P\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A06:2021\", \"NIST SP 800-53\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"MQ\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor floods MQ with messages, causing performance degradation or failure\",\n",
      "    \"mitigation_suggestion\": \"Implement message queuing and dead-letter queues to handle malformed messages\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"DB_P\",\n",
      "    \"stride_category\": \"Elevation of Privilege\",\n",
      "    \"threat_description\": \"Insufficient access controls allow an attacker to escalate privileges within DB_P\",\n",
      "    \"mitigation_suggestion\": \"Implement least privilege and role-based access control for all database users\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"NIST AC-6\", \"MITRE ATT&CK\"]\n",
      "  },\n",
      "  {\n",
      "    \"component_name\": \"ADM_P\",\n",
      "    \"stride_category\": \"Denial of Service\",\n",
      "    \"threat_description\": \"Malicious actor sends a large number of HTTPS requests to ADM_P, overwhelming the system\",\n",
      "    \"mitigation_suggestion\": \"Implement rate limiting and logging for all administrative access\",\n",
      "    \"impact\": \"High\",\n",
      "    \"likelihood\": \"Medium\",\n",
      "    \"references\": [\"OWASP A09:2021\", \"NIST AC-7\"]\n",
      "  }\n",
      "]\n",
      "2025-07-27 19:50:18,652 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 19:50:18,655 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-27 19:50:18,658 - INFO - \n",
      "--- LLM Output (Identified Threats) ---\n",
      "2025-07-27 19:50:18,659 - INFO - \n",
      "--- Identified threats successfully saved to './output/identified_threats.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"threats\": [\n",
      "    {\n",
      "      \"component_name\": \"U\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Unencrypted data transmitted from U to CDN potentially disclosed\",\n",
      "      \"mitigation_suggestion\": \"Implement end-to-end encryption; Use HTTPS protocol\",\n",
      "      \"impact\": \"Medium\",\n",
      "      \"likelihood\": \"High\",\n",
      "      \"references\": [\n",
      "        \"OWASP A01:2021\",\n",
      "        \"NIST SI-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"CDN\",\n",
      "      \"stride_category\": \"Tampering\",\n",
      "      \"threat_description\": \"Malicious actor intercepts and modifies data in transit from CDN to LB\",\n",
      "      \"mitigation_suggestion\": \"Implement integrity checking; Use digital signatures\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A03:2021\",\n",
      "        \"MITRE CA-8\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"LB\",\n",
      "      \"stride_category\": \"Elevation of Privilege\",\n",
      "      \"threat_description\": \"Unprivileged actor gains elevated privileges on LB, potentially leading to DoS\",\n",
      "      \"mitigation_suggestion\": \"Implement least privilege; Restrict unnecessary access\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"NIST AC-6\",\n",
      "        \"MITRE AU-5\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"WS\",\n",
      "      \"stride_category\": \"Denial of Service\",\n",
      "      \"threat_description\": \"Volume-based DoS attack on WS, impacting application availability and performance\",\n",
      "      \"mitigation_suggestion\": \"Implement rate limiting; Monitor for suspicious activity\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A04:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"DB_P\",\n",
      "      \"stride_category\": \"Repudiation\",\n",
      "      \"threat_description\": \"Insider attempts to discredit or deny attacks on DB_P, potentially compromising audit logs\",\n",
      "      \"mitigation_suggestion\": \"Implement tamper-evident logging; Conduct regular auditing and monitoring\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Low\",\n",
      "      \"references\": [\n",
      "        \"OWASP A05:2021\",\n",
      "        \"NIST AC-2\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"ADM_P\",\n",
      "      \"stride_category\": \"Spoofing\",\n",
      "      \"threat_description\": \"Attackers impersonate ADM_P, potentially gaining unauthorized access to administrative functions\",\n",
      "      \"mitigation_suggestion\": \"Implement multi-factor authentication; Conduct regular security audits\",\n",
      "      \"impact\": \"High\",\n",
      "      \"likelihood\": \"Medium\",\n",
      "      \"references\": [\n",
      "        \"OWASP A07:2021\",\n",
      "        \"MITRE CA-7\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T19:50:18.655020\",\n",
      "    \"source_dfd\": \"./output/dfd_components.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama python-dotenv pydantic logging instructor\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import instructor\n",
    "from ollama import Client  # Added for raw response debugging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use environment variables for paths and settings\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama3:8b\")  # Changed default to a more reliable model for testing\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure output directory exists early\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LLM with Instructor for schema enforcement\n",
    "llm = instructor.from_provider(f\"ollama/{LLM_MODEL}\", mode=instructor.Mode.JSON_SCHEMA)\n",
    "\n",
    "# Added: Raw Ollama client for debugging\n",
    "ollama_client = Client()\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str = Field(description=\"Affected asset, process, data flow, or entity.\")\n",
    "    stride_category: str = Field(description=\"One STRIDE category: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\")\n",
    "    threat_description: str = Field(description=\"Clear, specific description of the threat.\")\n",
    "    mitigation_suggestion: str = Field(description=\"Practical, actionable mitigation.\")\n",
    "    impact: str = Field(description=\"Low/Medium/High based on potential damage.\")\n",
    "    likelihood: str = Field(description=\"Low/Medium/High based on exploitability.\")\n",
    "    references: list[str] = Field(description=\"Array of standard references (e.g., ['OWASP A01:2021', 'NIST SI-2']).\")\n",
    "\n",
    "class Threats(BaseModel):\n",
    "    threats: list[Threat]\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing (if input is empty) ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [\"User\", \"Attacker\"],\n",
    "    \"processes\": [\"Web Application\", \"Authentication Service\"],\n",
    "    \"data_stores\": [\"User Database\"],\n",
    "    \"data_flows\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"to\": \"Web Application\",\n",
    "            \"data\": \"Login Credentials\",\n",
    "            \"protocol\": \"HTTP\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Web Application\",\n",
    "            \"to\": \"User Database\",\n",
    "            \"data\": \"Query User Data\",\n",
    "            \"protocol\": \"SQL\"\n",
    "        }\n",
    "    ],\n",
    "    \"trust_boundaries\": [\"Internet to DMZ\", \"DMZ to Internal Network\"]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:  # Added: Check for empty data\n",
    "        logger.warning(\"--- DFD data is empty. Using sample DFD for testing ---\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "    logger.info(\"--- DFD components loaded successfully ---\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"--- Input file not found at '{DFD_INPUT_PATH}'. Using sample DFD for testing ---\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"--- FATAL ERROR: Could not parse JSON from '{DFD_INPUT_PATH}' ---\")\n",
    "    logger.error(\"The file may be corrupted or empty. Using sample DFD for testing.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- FATAL ERROR: An unexpected error occurred while loading DFD components ---\")\n",
    "    logger.error(f\"Error details: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Prompt Engineering for Threat Generation ---\n",
    "threat_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
    "\n",
    "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
    "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
    "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
    "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
    "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
    "\n",
    "For each threat, include:\n",
    "- 'component_name': Affected asset, process, data flow, or entity.\n",
    "- 'stride_category': One STRIDE category.\n",
    "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
    "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
    "- 'impact': Low/Medium/High.\n",
    "- 'likelihood': Low/Medium/High.\n",
    "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
    "\n",
    "DFD Components:\n",
    "---\n",
    "{dfd_json}\n",
    "---\n",
    "\n",
    "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "threat_prompt = ChatPromptTemplate.from_template(threat_prompt_template)\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Invoking Local LLM to generate STRIDE threats ---\")\n",
    "try:\n",
    "    # Convert the loaded DFD dictionary back to a JSON string for the prompt\n",
    "    dfd_json_string = json.dumps(dfd_data, indent=2)\n",
    "\n",
    "    # Generate messages from the prompt template\n",
    "    messages = threat_prompt.format_messages(dfd_json=dfd_json_string)\n",
    "\n",
    "    # Added: Log the prompt for debugging\n",
    "    logger.info(f\"--- Prompt sent to LLM ---\\n{messages[0].content}\")\n",
    "\n",
    "    # Added: Call raw Ollama for response debugging (before Instructor)\n",
    "    raw_response = ollama_client.chat(model=LLM_MODEL, messages=[{\"role\": \"user\", \"content\": messages[0].content}])\n",
    "    logger.info(f\"--- Raw LLM Response ---\\n{raw_response['message']['content']}\")\n",
    "\n",
    "    # Invoke the LLM with Instructor for structured output\n",
    "    threats_obj = llm.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": messages[0].content}],\n",
    "        response_model=Threats,\n",
    "        max_retries=5  # Increased for better handling\n",
    "    )\n",
    "\n",
    "    threats_dict = threats_obj.model_dump()\n",
    "    \n",
    "    # Add metadata\n",
    "    threats_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_dfd\": DFD_INPUT_PATH\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema (Instructor already enforces, but double-check)\n",
    "    try:\n",
    "        validated = ThreatsOutput(**threats_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Save the threats to a new file\n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(threats_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (Identified Threats) ---\")\n",
    "    print(json.dumps(threats_dict, indent=2))\n",
    "    logger.info(f\"\\n--- Identified threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during threat generation ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 18:09:52,560 - INFO - --- Setting up RAG pipeline ---\n",
      "2025-07-28 18:09:52,561 - INFO - Use pytorch device_name: mps\n",
      "2025-07-28 18:09:52,562 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-07-28 18:09:55,354 - INFO - --- Loading existing FAISS index from 'faiss_index' ---\n",
      "2025-07-28 18:09:55,443 - INFO - --- OpenAI client initialized successfully ---\n",
      "2025-07-28 18:09:55,444 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-28 18:09:55,445 - INFO - \n",
      "--- Invoking LLM with RAG to systematically generate STRIDE threats ---\n",
      "2025-07-28 18:09:55,445 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"U\", \"destination\": \"CDN\", \"data_description\": \"\", \"protocol\": \"HTTPS\"}} ---\n",
      "2025-07-28 18:09:55,463 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:09:55,463 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:10:22,703 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:10:22,705 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:10:22,706 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:10:34,052 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:10:34,055 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:10:34,056 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:10:45,942 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:10:45,944 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:10:45,945 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:10:58,726 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:10:58,729 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:10:58,729 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:11:26,670 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:11:26,673 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:11:26,675 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:11:38,047 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:11:38,051 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:11:38,052 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"CDN\", \"destination\": \"LB\", \"data_description\": \"\", \"protocol\": \"HTTPS\"}} ---\n",
      "2025-07-28 18:11:38,245 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:11:38,245 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:11:50,831 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:11:50,834 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:11:50,836 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:12:08,442 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:12:08,455 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:12:08,457 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:12:22,466 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:12:22,470 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:12:22,470 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:12:36,711 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:12:36,713 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:12:36,714 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:12:50,524 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:12:50,525 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:12:50,526 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:13:04,983 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:13:04,985 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:13:04,985 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"LB\", \"destination\": \"WS\", \"data_description\": \"\", \"protocol\": \"HTTPS\"}} ---\n",
      "2025-07-28 18:13:05,156 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:13:05,156 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:13:18,674 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:13:18,675 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:13:18,675 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:13:31,506 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:13:31,508 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:13:31,509 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:13:47,381 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:13:47,383 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:13:47,384 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:14:05,747 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:14:05,750 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:14:05,751 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:14:19,623 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:14:19,627 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:14:19,628 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:14:32,691 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:14:32,695 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:14:32,696 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"WS\", \"destination\": \"DB_P\", \"data_description\": \"\", \"protocol\": \"JDBC/ODBC over TLS\"}} ---\n",
      "2025-07-28 18:14:32,895 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:14:32,896 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:14:46,338 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:14:46,342 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:14:46,343 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:14:57,417 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:14:57,421 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:14:57,422 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:15:09,453 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:15:09,456 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:15:09,457 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:15:22,721 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:15:22,725 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:15:22,726 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:15:33,773 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:15:33,776 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:15:33,777 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:15:45,345 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:15:45,348 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:15:45,349 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"WS\", \"destination\": \"MQ\", \"data_description\": \"\", \"protocol\": \"AMQP over TLS\"}} ---\n",
      "2025-07-28 18:15:45,540 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:15:45,541 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:15:59,429 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:15:59,432 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:15:59,433 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:16:12,490 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:16:12,493 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:16:12,494 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:16:25,441 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:16:25,449 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:16:25,450 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:16:39,586 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:16:39,589 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:16:39,590 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:16:52,063 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:16:52,065 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:16:52,066 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:17:05,244 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:17:05,247 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:17:05,247 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"WRK\", \"destination\": \"MQ\", \"data_description\": \"\", \"protocol\": \"AMQP over TLS\"}} ---\n",
      "2025-07-28 18:17:05,438 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:17:05,438 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:17:19,097 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:17:19,101 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:17:19,102 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:17:30,922 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:17:30,926 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:17:30,927 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:17:43,720 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:17:43,724 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:17:43,725 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:17:56,405 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:17:56,408 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:17:56,409 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:18:09,325 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:18:09,329 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:18:09,330 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:18:21,720 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:18:21,724 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:18:21,725 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"WRK\", \"destination\": \"DB_P\", \"data_description\": \"\", \"protocol\": \"JDBC/ODBC over TLS\"}} ---\n",
      "2025-07-28 18:18:21,928 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:18:21,928 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:18:33,703 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:18:33,706 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:18:33,707 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:18:45,174 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:18:45,178 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:18:45,183 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:18:58,179 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:18:58,182 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:18:58,183 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:19:09,045 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:19:09,048 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:19:09,048 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:19:21,096 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:19:21,097 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:19:21,097 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:19:34,042 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:19:34,044 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:19:34,048 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"DB_B\", \"destination\": \"DB_P\", \"data_description\": \"\", \"protocol\": \"Backup Protocol over TLS\"}} ---\n",
      "2025-07-28 18:19:34,234 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:19:34,234 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:19:45,639 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:19:45,641 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:19:45,641 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:19:57,734 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:19:57,737 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:19:57,737 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:20:09,493 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:20:09,495 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:20:09,496 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:20:22,045 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:20:22,049 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:20:22,049 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:20:30,518 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:20:30,521 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:20:30,523 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:20:42,049 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:20:42,051 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:20:42,052 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"ADM\", \"destination\": \"ADM_P\", \"data_description\": \"\", \"protocol\": \"HTTPS over VPN\"}} ---\n",
      "2025-07-28 18:20:42,268 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:20:42,269 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:20:53,948 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:20:53,949 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:20:53,950 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:21:05,140 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:21:05,144 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:21:05,146 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:21:18,673 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:21:18,675 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:21:18,675 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:21:30,450 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:21:30,453 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:21:30,454 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:21:42,498 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:21:42,501 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:21:42,502 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:21:54,659 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:21:54,662 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:21:54,664 - INFO - \n",
      "--- Analyzing component: {\"type\": \"data_flows\", \"details\": {\"source\": \"ADM_P\", \"destination\": \"LB\", \"data_description\": \"\", \"protocol\": \"HTTPS\"}} ---\n",
      "2025-07-28 18:21:54,846 - INFO - --- Retrieved RAG context for component ---\n",
      "2025-07-28 18:21:54,847 - INFO - --- Generating threats for STRIDE category: Spoofing ---\n",
      "2025-07-28 18:22:07,417 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:22:07,421 - INFO - --- Successfully generated 2 threat(s) for category Spoofing ---\n",
      "2025-07-28 18:22:07,422 - INFO - --- Generating threats for STRIDE category: Tampering ---\n",
      "2025-07-28 18:22:20,627 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:22:20,631 - INFO - --- Successfully generated 2 threat(s) for category Tampering ---\n",
      "2025-07-28 18:22:20,632 - INFO - --- Generating threats for STRIDE category: Repudiation ---\n",
      "2025-07-28 18:22:32,562 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:22:32,565 - INFO - --- Successfully generated 2 threat(s) for category Repudiation ---\n",
      "2025-07-28 18:22:32,569 - INFO - --- Generating threats for STRIDE category: Information Disclosure ---\n",
      "2025-07-28 18:22:44,179 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:22:44,183 - INFO - --- Successfully generated 2 threat(s) for category Information Disclosure ---\n",
      "2025-07-28 18:22:44,184 - INFO - --- Generating threats for STRIDE category: Denial of Service ---\n",
      "2025-07-28 18:22:55,752 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:22:55,756 - INFO - --- Successfully generated 2 threat(s) for category Denial of Service ---\n",
      "2025-07-28 18:22:55,758 - INFO - --- Generating threats for STRIDE category: Elevation of Privilege ---\n",
      "2025-07-28 18:23:07,936 - INFO - HTTP Request: POST https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-28 18:23:07,939 - INFO - --- Successfully generated 2 threat(s) for category Elevation of Privilege ---\n",
      "2025-07-28 18:23:07,940 - INFO - --- Deduplicated threats: 120 unique threats remaining ---\n",
      "2025-07-28 18:23:07,944 - INFO - --- Final JSON output validated successfully against schema ---\n",
      "2025-07-28 18:23:07,949 - INFO - \n",
      "--- LLM RAG Output (Identified Threats) ---\n",
      "2025-07-28 18:23:07,950 - INFO - \n",
      "--- Identified 120 threats successfully saved to './output/identified_threats.json' ---\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install openai pydantic logging python-dotenv langchain langchain_community langchain_huggingface faiss-cpu pypdf sentence-transformers requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "import requests  # Added for auto-download\n",
    "\n",
    "# RAG specific imports\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"llama-3.1-70b-instruct\") # Updated model suggestion\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_DOCS_DIR = \"rag_docs\"\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RAG_DOCS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def setup_rag_pipeline():\n",
    "    \"\"\"Initializes the RAG pipeline by creating or loading a FAISS vector store.\"\"\"\n",
    "    logger.info(\"--- Setting up RAG pipeline ---\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    if os.path.exists(FAISS_INDEX_PATH):\n",
    "        logger.info(f\"--- Loading existing FAISS index from '{FAISS_INDEX_PATH}' ---\")\n",
    "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        logger.info(\"--- No existing FAISS index found. Building a new one. ---\")\n",
    "        \n",
    "        loaders = {\n",
    "            \"**/*.pdf\": PyPDFLoader,\n",
    "            \"**/*.md\": TextLoader,\n",
    "            \"**/*.txt\": TextLoader\n",
    "        }\n",
    "        documents = []\n",
    "        for glob, loader_cls in loaders.items():\n",
    "            try:\n",
    "                loader = DirectoryLoader(RAG_DOCS_DIR, glob=glob, loader_cls=loader_cls, show_progress=True, use_multithreading=True, silent_errors=True)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load files with pattern {glob} using {loader_cls.__name__}. Error: {e}\")\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(f\"--- No supported documents found in '{RAG_DOCS_DIR}'. Attempting to auto-download key resources ---\")\n",
    "            # Auto-download OWASP Top 10 2021 PDF (since 2025 not released as of July 2025)\n",
    "            owasp_url = \"https://owasp.org/Top10/assets/PDF/OWASP-Top-10-2021.pdf\"\n",
    "            try:\n",
    "                response = requests.get(owasp_url)\n",
    "                response.raise_for_status()\n",
    "                owasp_path = os.path.join(RAG_DOCS_DIR, \"owasp_top10_2021.pdf\")\n",
    "                with open(owasp_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                logger.info(f\"--- Downloaded OWASP Top 10 2021 PDF to '{owasp_path}' ---\")\n",
    "                # Reload PDF loader\n",
    "                pdf_loader = DirectoryLoader(RAG_DOCS_DIR, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "                documents.extend(pdf_loader.load())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"--- Failed to auto-download OWASP PDF: {e} ---\")\n",
    "                raise ValueError(f\"No documents available for RAG. Please add files to '{RAG_DOCS_DIR}'.\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        logger.info(f\"--- Creating FAISS index from {len(docs)} document chunks. This may take a moment... ---\")\n",
    "        db = FAISS.from_documents(docs, embeddings)\n",
    "        db.save_local(FAISS_INDEX_PATH)\n",
    "        logger.info(f\"--- FAISS index created and saved to '{FAISS_INDEX_PATH}' ---\")\n",
    "        \n",
    "    return db\n",
    "\n",
    "# --- Initialize RAG and OpenAI Client ---\n",
    "try:\n",
    "    rag_db = setup_rag_pipeline()\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.scaleway.ai/4a8fd76b-8606-46e6-afe6-617ce8eeb948/v1\",\n",
    "        api_key=os.getenv(\"SCW_SECRET_KEY\")\n",
    "    )\n",
    "    logger.info(\"--- OpenAI client initialized successfully ---\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- Failed to initialize services: {e} ---\")\n",
    "    raise\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str\n",
    "    stride_category: str\n",
    "    threat_description: str\n",
    "    mitigation_suggestion: str\n",
    "    impact: str\n",
    "    likelihood: str\n",
    "    references: list[str]\n",
    "    risk_score: str\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Sample DFD for Testing ---\n",
    "SAMPLE_DFD = {\n",
    "    \"external_entities\": [{\"name\": \"User\"}],\n",
    "    \"processes\": [{\"name\": \"Web Application\"}, {\"name\": \"Authentication Service\"}],\n",
    "    \"data_stores\": [{\"name\": \"User Database\"}],\n",
    "    \"data_flows\": [\n",
    "        {\"source\": \"User\", \"destination\": \"Web Application\", \"data_description\": \"Login Credentials\", \"protocol\": \"HTTPS\"},\n",
    "        {\"source\": \"Web Application\", \"destination\": \"User Database\", \"data_description\": \"Query User Data\", \"protocol\": \"SQL\"}\n",
    "    ],\n",
    "    \"trust_boundaries\": [{\"name\": \"Internet to DMZ\"}]\n",
    "}\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    if not dfd_data:\n",
    "        logger.warning(f\"DFD file at '{DFD_INPUT_PATH}' is empty. Using sample DFD for demonstration.\")\n",
    "        dfd_data = SAMPLE_DFD\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"DFD file not found at '{DFD_INPUT_PATH}'. Using sample DFD for demonstration.\")\n",
    "    dfd_data = SAMPLE_DFD\n",
    "except json.JSONDecodeError as e:\n",
    "    logger.error(f\"FATAL: Error decoding JSON from '{DFD_INPUT_PATH}': {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"FATAL: Error loading DFD: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Enhanced Prompting Strategy ---\n",
    "\n",
    "# **FIX 1: Define STRIDE categories explicitly to ensure systematic coverage.**\n",
    "# Parametrized: Load from config or file if needed\n",
    "STRIDE_DEFINITIONS = {\n",
    "    \"S\": (\"Spoofing\", \"Illegitimately accessing systems or data by impersonating a user, process, or component.\"),\n",
    "    \"T\": (\"Tampering\", \"Unauthorized modification of data, either in transit or at rest.\"),\n",
    "    \"R\": (\"Repudiation\", \"A user or system denying that they performed an action, often due to a lack of sufficient proof (e.g., logs).\"),\n",
    "    \"I\": (\"Information Disclosure\", \"Exposing sensitive information to unauthorized individuals.\"),\n",
    "    \"D\": (\"Denial of Service\", \"Preventing legitimate users from accessing a system or service.\"),\n",
    "    \"E\": (\"Elevation of Privilege\", \"A user or process gaining rights beyond their authorized level.\")\n",
    "}\n",
    "\n",
    "# Optional: Load custom STRIDE from file\n",
    "stride_config_path = \"stride_config.json\"\n",
    "if os.path.exists(stride_config_path):\n",
    "    with open(stride_config_path, 'r') as f:\n",
    "        STRIDE_DEFINITIONS = json.load(f)\n",
    "    logger.info(\"--- Loaded custom STRIDE definitions from 'stride_config.json' ---\")\n",
    "\n",
    "# **FIX 2: Create a highly specific prompt template focused on a SINGLE STRIDE category.**\n",
    "# This prevents generic responses and forces the model to generate relevant, accurate threats.\n",
    "threat_prompt_template_specific_rag = \"\"\"\n",
    "You are a cybersecurity architect specializing in threat modeling using the STRIDE methodology.\n",
    "Your task is to generate 1-2 specific threats for a given DFD component, focusing ONLY on a single STRIDE category.\n",
    "\n",
    "**DFD Component to Analyze:**\n",
    "{component_info}\n",
    "\n",
    "**STRIDE Category to Focus On:**\n",
    "- **{stride_category} ({stride_name}):** {stride_definition}\n",
    "\n",
    "**Security Context from Knowledge Base (for accuracy):**\n",
    "'''\n",
    "{rag_context}\n",
    "'''\n",
    "\n",
    "**Instructions:**\n",
    "1.  Generate 1-2 distinct and realistic threats for the component that fall **strictly** under the '{stride_name}' category.\n",
    "2.  **Be specific.** Relate the threat directly to the component's type and details. For a database, a Spoofing threat is a spoofed connection, not user impersonation. For a data flow, a Tampering threat is a Man-in-the-Middle attack.\n",
    "3.  Use the provided Security Context to create specific descriptions, **actionable mitigations**, and accurate references (e.g., CWE, OWASP Cheat Sheets). Do not invent references.\n",
    "4.  Provide a realistic risk assessment (Impact, Likelihood, Score).\n",
    "5.  Output ONLY a valid JSON object with a single key \"threats\", containing a list of threat objects. Do not include any other text or commentary.\n",
    "\n",
    "**JSON Threat Object Schema:**\n",
    "{{\n",
    "  \"component_name\": \"string (the name of the component being analyzed)\",\n",
    "  \"stride_category\": \"{stride_category}\",\n",
    "  \"threat_description\": \"string (Specific to the component and STRIDE category)\",\n",
    "  \"mitigation_suggestion\": \"string (Actionable and specific)\",\n",
    "  \"impact\": \"Low, Medium, or High\",\n",
    "  \"likelihood\": \"Low, Medium, or High\",\n",
    "  \"references\": [\"list of strings, e.g., 'OWASP A01:2021', 'CWE-89'\"],\n",
    "  \"risk_score\": \"Critical, High, Medium, or Low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "retry_prompt_addition = \" Generate at least one threat if realistically applicable, even if minor.\"\n",
    "\n",
    "# Function to calculate risk_score\n",
    "def calculate_risk_score(impact, likelihood):\n",
    "    if impact == \"High\" and likelihood in [\"Medium\", \"High\"]:\n",
    "        return \"Critical\"\n",
    "    elif (impact == \"High\" and likelihood == \"Low\") or (impact == \"Medium\" and likelihood == \"High\"):\n",
    "        return \"High\"\n",
    "    elif (impact == \"Medium\" and likelihood in [\"Medium\", \"Low\"]) or (impact == \"Low\" and likelihood == \"High\"):\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# --- Main Invocation Logic ---\n",
    "logger.info(\"\\n--- Invoking LLM with RAG to systematically generate STRIDE threats ---\")\n",
    "all_threats = []\n",
    "try:\n",
    "    components_to_analyze = []\n",
    "    for key, value in dfd_data.items():\n",
    "        if isinstance(value, list) and value:\n",
    "            for item in value:\n",
    "                # Ensure component has a name for better identification\n",
    "                if isinstance(item, dict) and item.get(\"name\"):\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "                elif isinstance(item, dict): # Fallback for components without a 'name' field\n",
    "                    components_to_analyze.append({\"type\": key, \"details\": item})\n",
    "\n",
    "\n",
    "    # **FIX 3: Iterate through each component AND each STRIDE category.**\n",
    "    # This loop structure ensures every category is considered for every component.\n",
    "    for component in components_to_analyze:\n",
    "        component_str = json.dumps(component)\n",
    "        component_name = component.get(\"details\", {}).get(\"name\", component_str)\n",
    "        logger.info(f\"\\n--- Analyzing component: {component_name} ---\")\n",
    "\n",
    "        retrieved_docs = rag_db.similarity_search(component_str, k=5)  # Increased to 5 for broader context\n",
    "        rag_context = \"\\n---\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        logger.info(\"--- Retrieved RAG context for component ---\")\n",
    "\n",
    "        for cat_letter, (cat_name, cat_def) in STRIDE_DEFINITIONS.items():\n",
    "            logger.info(f\"--- Generating threats for STRIDE category: {cat_name} ---\")\n",
    "            \n",
    "            prompt = threat_prompt_template_specific_rag.format(\n",
    "                component_info=component_str,\n",
    "                rag_context=rag_context,\n",
    "                stride_category=cat_letter,\n",
    "                stride_name=cat_name,\n",
    "                stride_definition=cat_def\n",
    "            )\n",
    "            \n",
    "            retry_count = 0\n",
    "            max_retries = 1  # Retry once if no threats\n",
    "            threats = []\n",
    "            while retry_count <= max_retries and not threats:\n",
    "                try:\n",
    "                    if retry_count > 0:\n",
    "                        prompt += retry_prompt_addition  # Add retry instruction\n",
    "                    \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=LLM_MODEL,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        response_format={\"type\": \"json_object\"},\n",
    "                        max_tokens=2048,\n",
    "                        temperature=0.4 # Slightly lower temp for more focused output\n",
    "                    )\n",
    "                    \n",
    "                    response_content = response.choices[0].message.content\n",
    "                    generated_data = json.loads(response_content)\n",
    "                    \n",
    "                    # Ensure the response is a dict with a 'threats' key which is a list\n",
    "                    if isinstance(generated_data, dict) and isinstance(generated_data.get(\"threats\"), list):\n",
    "                        threats = generated_data[\"threats\"]\n",
    "                        # Add component name if missing from LLM response\n",
    "                        for threat in threats:\n",
    "                            if 'component_name' not in threat or not threat['component_name']:\n",
    "                                threat['component_name'] = component_name\n",
    "                        logger.info(f\"--- Successfully generated {len(threats)} threat(s) for category {cat_name} ---\")\n",
    "                    else:\n",
    "                        logger.warning(f\"--- LLM response for {cat_name} on {component_name} had unexpected structure. ---\")\n",
    "                        logger.debug(f\"Raw Response: {response_content}\")\n",
    "\n",
    "                except (json.JSONDecodeError, AttributeError) as e:\n",
    "                    logger.warning(f\"--- Could not parse LLM response for {cat_name} on {component_name}: {e} ---\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"--- An API error occurred for {cat_name} on {component_name}: {e} ---\")\n",
    "                \n",
    "                retry_count += 1\n",
    "\n",
    "            all_threats.extend(threats)\n",
    "\n",
    "    # Deduplication: Remove exact duplicates based on description\n",
    "    unique_threats = []\n",
    "    seen_descriptions = set()\n",
    "    for threat in all_threats:\n",
    "        desc = threat.get('threat_description', '')\n",
    "        if desc not in seen_descriptions:\n",
    "            seen_descriptions.add(desc)\n",
    "            # Recalculate risk_score\n",
    "            threat['risk_score'] = calculate_risk_score(threat.get('impact', 'Low'), threat.get('likelihood', 'Low'))\n",
    "            unique_threats.append(threat)\n",
    "    all_threats = unique_threats\n",
    "    logger.info(f\"--- Deduplicated threats: {len(all_threats)} unique threats remaining ---\")\n",
    "\n",
    "    # --- Final Processing and Validation ---\n",
    "    risk_order = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1, \"Informational\": 0}\n",
    "    all_threats.sort(key=lambda t: risk_order.get(t.get('risk_score', 'Low'), 0), reverse=True)\n",
    "\n",
    "    final_output = {\n",
    "        \"threats\": all_threats,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_dfd\": os.path.basename(DFD_INPUT_PATH),\n",
    "            \"llm_model\": LLM_MODEL,\n",
    "            \"rag_index\": FAISS_INDEX_PATH\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        validated_output = ThreatsOutput(**final_output)\n",
    "        logger.info(\"--- Final JSON output validated successfully against schema ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- FINAL JSON VALIDATION FAILED: {ve} ---\")\n",
    "        \n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    logger.info(\"\\n--- LLM RAG Output (Identified Threats) ---\")\n",
    "    # print(json.dumps(final_output, indent=2))\n",
    "    logger.info(f\"\\n--- Identified {len(all_threats)} threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during the threat generation process ---\")\n",
    "    logger.error(f\"Error: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b793d59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 20:33:21,238 - INFO - --- Fetching CISA Known Exploited Vulnerabilities (KEV) catalog ---\n",
      "2025-07-28 20:33:21,763 - INFO - --- Successfully loaded 1391 entries from CISA KEV catalog ---\n",
      "2025-07-28 20:33:21,764 - INFO - --- Loading initial threats from './output/identified_threats.json' ---\n",
      "2025-07-28 20:33:21,765 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-28 20:33:21,766 - INFO - --- Loading controls from './output/controls.json' ---\n",
      "2025-07-28 20:33:21,766 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,767 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,768 - INFO - --- CVE CVE-2018-14732 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14732' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- CVE CVE-2018-14731 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,769 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2018-14731' from threat 'LB to WS'. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'U to CDN'. ---\n",
      "2025-07-28 20:33:21,770 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'U to CDN'. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- CVE CVE-2005-2088 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2005-2088' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,771 - INFO - --- CVE CVE-2006-6276 is older than 5 years and not in KEV catalog. Considered for suppression. ---\n",
      "2025-07-28 20:33:21,772 - INFO - --- Removing outdated/irrelevant CVE 'CVE-2006-6276' from threat 'CDN to LB'. ---\n",
      "2025-07-28 20:33:21,772 - INFO - --- Starting threat deduplication ---\n",
      "2025-07-28 20:33:21,773 - INFO - Use pytorch device_name: mps\n",
      "2025-07-28 20:33:21,774 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cc73e31810451080f32135e0e20064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 20:33:24,925 - INFO - --- Merged 2 similar threats for 'U to CDN' (S) ---\n",
      "2025-07-28 20:33:24,925 - INFO - --- Merged 2 similar threats for 'U to CDN' (D) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'CDN to LB' (I) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (R) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (D) ---\n",
      "2025-07-28 20:33:24,926 - INFO - --- Merged 2 similar threats for 'LB to WS' (E) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to DB_P' (I) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (S) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (I) ---\n",
      "2025-07-28 20:33:24,927 - INFO - --- Merged 2 similar threats for 'WS to MQ' (D) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WS to MQ' (E) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (S) ---\n",
      "2025-07-28 20:33:24,928 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (I) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (D) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to MQ' (E) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to DB_P' (S) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'WRK to DB_P' (T) ---\n",
      "2025-07-28 20:33:24,929 - INFO - --- Merged 2 similar threats for 'DB_B to DB_P' (S) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'DB_B to DB_P' (D) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (R) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (D) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM to ADM_P' (E) ---\n",
      "2025-07-28 20:33:24,930 - INFO - --- Merged 2 similar threats for 'ADM_P to LB' (D) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Merged 2 similar threats for 'ADM_P to LB' (E) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Merged 2 similar threats for 'CDN to LB' (R) ---\n",
      "2025-07-28 20:33:24,931 - INFO - --- Deduplication reduced 120 threats to 95 ---\n",
      "2025-07-28 20:33:24,931 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,931 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,932 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,933 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,933 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,934 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,934 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,935 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,936 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,937 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,938 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,939 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,940 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,941 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,942 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,942 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,943 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,944 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,945 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,946 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,947 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,948 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,949 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,950 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,951 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'U to CDN' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,952 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'CDN to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'LB to WS' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,953 - WARNING - --- Data flow 'WS to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'WRK to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,954 - WARNING - --- Data flow 'DB_B to DB_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,955 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,955 - WARNING - --- Data flow 'ADM to ADM_P' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'ADM_P to LB' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,956 - WARNING - --- Data flow 'WRK to MQ' is missing 'data_classification'. Impact assessment will be generic. ---\n",
      "2025-07-28 20:33:24,957 - INFO - --- Final refined JSON output validated successfully against schema. ---\n",
      "2025-07-28 20:33:24,960 - INFO - --- Refined 95 threats saved to './output/refined_threats.json' ---\n",
      "2025-07-28 20:33:24,960 - INFO - --- Summary report saved to './output/threat_summary.json' ---\n",
      "2025-07-28 20:33:24,963 - INFO - --- CSV report saved to './output/threats.csv' ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import logging\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_INPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "REFINED_THREATS_OUTPUT_PATH = os.getenv(\"REFINED_THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"refined_threats.json\"))\n",
    "CONTROLS_INPUT_PATH = os.getenv(\"CONTROLS_INPUT_PATH\", os.path.join(INPUT_DIR, \"controls.json\"))\n",
    "NVD_API_URL = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n",
    "CISA_KEV_URL = \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\"\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Threat Schema ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str = Field(..., description=\"Standardized name of the component or data flow\")\n",
    "    stride_category: str = Field(..., pattern=\"^[STRIDE]$\", description=\"STRIDE category (S, T, R, I, D, E)\")\n",
    "    threat_description: str = Field(..., description=\"Detailed description of the threat\")\n",
    "    mitigation_suggestion: str = Field(..., description=\"Actionable mitigation specific to the threat\")\n",
    "    impact: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Impact level\")\n",
    "    likelihood: str = Field(..., pattern=\"^(Low|Medium|High)$\", description=\"Likelihood level\")\n",
    "    references: list[str] = Field(..., description=\"List of references (e.g., CWE, CVE, OWASP)\")\n",
    "    risk_score: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Derived risk score\")\n",
    "    residual_risk_score: str = Field(..., pattern=\"^(Critical|High|Medium|Low)$\", description=\"Risk score post-mitigation\")\n",
    "    exploitability: str = Field(..., pattern=\"^(Low|Medium|High)$\", description=\"Ease of exploitation\")\n",
    "    mitigation_maturity: str = Field(..., pattern=\"^(Immature|Mature|Advanced)$\", description=\"Maturity of mitigation controls\")\n",
    "    justification: str = Field(..., description=\"Rationale for impact and likelihood ratings\")\n",
    "    risk_statement: str = Field(..., description=\"Business-contextualized risk description\")\n",
    "\n",
    "class RefinedThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Caching for External APIs ---\n",
    "cisa_kev_cache = None\n",
    "\n",
    "def get_cisa_kev_catalog():\n",
    "    \"\"\"Fetches and caches the CISA KEV catalog.\"\"\"\n",
    "    global cisa_kev_cache\n",
    "    if cisa_kev_cache is None:\n",
    "        try:\n",
    "            logger.info(\"--- Fetching CISA Known Exploited Vulnerabilities (KEV) catalog ---\")\n",
    "            response = requests.get(CISA_KEV_URL, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            cisa_kev_cache = {vuln['cveID'] for vuln in response.json().get('vulnerabilities', [])}\n",
    "            logger.info(f\"--- Successfully loaded {len(cisa_kev_cache)} entries from CISA KEV catalog ---\")\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"--- Failed to fetch CISA KEV catalog: {e}. Proceeding without it. ---\")\n",
    "            cisa_kev_cache = set()\n",
    "    return cisa_kev_cache\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_dfd_components():\n",
    "    \"\"\"Load DFD components to provide full data flow details.\"\"\"\n",
    "    logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        with open(DFD_INPUT_PATH, 'r') as f:\n",
    "            dfd_data = json.load(f)\n",
    "        return dfd_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Failed to load DFD components: {e} ---\")\n",
    "        raise\n",
    "\n",
    "def load_controls():\n",
    "    \"\"\"Load client-provided controls to suppress threats.\"\"\"\n",
    "    logger.info(f\"--- Loading controls from '{CONTROLS_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        if os.path.exists(CONTROLS_INPUT_PATH):\n",
    "            with open(CONTROLS_INPUT_PATH, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"https_enabled\": False, \"tls_version\": \"1.2\", \"mtls_enabled\": False, \"secrets_manager\": False}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"--- Failed to load controls, using defaults: {e} ---\")\n",
    "        return {\"https_enabled\": False, \"tls_version\": \"1.2\", \"mtls_enabled\": False, \"secrets_manager\": False}\n",
    "\n",
    "def check_cve_relevance(cve_id):\n",
    "    \"\"\"Check if a CVE is recent (within 5 years) and not actively exploited.\"\"\"\n",
    "    kev_catalog = get_cisa_kev_catalog()\n",
    "    if cve_id in kev_catalog:\n",
    "        logger.warning(f\"--- CVE {cve_id} is in the CISA KEV catalog and should NOT be suppressed. ---\")\n",
    "        return True # It's relevant because it's known to be exploited\n",
    "\n",
    "    try:\n",
    "        year = int(cve_id.split('-')[1])\n",
    "        if year < datetime.now().year - 5:\n",
    "            logger.info(f\"--- CVE {cve_id} is older than 5 years and not in KEV catalog. Considered for suppression. ---\")\n",
    "            return False # Not relevant\n",
    "        return True # Relevant\n",
    "    except (ValueError, IndexError):\n",
    "        logger.warning(f\"--- Could not parse year from CVE ID {cve_id}. Assuming it's relevant. ---\")\n",
    "        return True\n",
    "\n",
    "def calculate_risk_score(impact, likelihood):\n",
    "    \"\"\"Calculate risk score based on impact and likelihood.\"\"\"\n",
    "    # Convert impact to a numeric value for calculation\n",
    "    impact_map = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    likelihood_map = {\"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    \n",
    "    score = impact_map.get(impact, 1) * likelihood_map.get(likelihood, 1)\n",
    "    \n",
    "    if score >= 9:\n",
    "        return \"Critical\"\n",
    "    if score >= 6:\n",
    "        return \"High\"\n",
    "    if score >= 3:\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "def assess_exploitability(threat, dfd_data):\n",
    "    \"\"\"Assess exploitability based on component exposure and protocol.\"\"\"\n",
    "    component_name = threat[\"component_name\"]\n",
    "    # Find the corresponding data flow to check trust boundaries and protocols\n",
    "    flow = next((f for f in dfd_data.get(\"data_flows\", []) if f\"{f['source']} to {f['destination']}\" == component_name), None)\n",
    "    \n",
    "    if flow and flow.get(\"source\") == \"U\": # 'U' is the external user\n",
    "        return \"High\"\n",
    "    if flow and \"TLS\" in flow.get(\"protocol\", \"\") or \"HTTPS\" in flow.get(\"protocol\", \"\"):\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "\n",
    "def assess_mitigation_maturity(mitigation):\n",
    "    \"\"\"Assess maturity of mitigation based on specificity and implementation ease.\"\"\"\n",
    "    mitigation_lower = mitigation.lower()\n",
    "    if \"end-to-end encryption\" in mitigation_lower or \"certificate pinning\" in mitigation_lower:\n",
    "        return \"Advanced\"\n",
    "    if \"mtls\" in mitigation_lower or \"waf\" in mitigation_lower or \"rate limiting\" in mitigation_lower or \"secrets management\" in mitigation_lower:\n",
    "        return \"Mature\"\n",
    "    if \"logging\" in mitigation_lower or \"auditing\" in mitigation_lower:\n",
    "        return \"Immature\"\n",
    "    return \"Mature\"\n",
    "\n",
    "def standardize_component_name(original_name, valid_flows):\n",
    "    \"\"\"Standardize component names to match DFD format.\"\"\"\n",
    "    valid_component_names = {f\"{flow['source']} to {flow['destination']}\" for flow in valid_flows}\n",
    "    \n",
    "    # Clean up common variations\n",
    "    normalized = original_name.replace(\"Data Flow from \", \"\").replace(\" data flow\", \"\").strip()\n",
    "    normalized = \" \".join(normalized.split()).replace(\" to \", \" to \")\n",
    "    \n",
    "    if normalized in valid_component_names:\n",
    "        return normalized\n",
    "    # Fallback for close matches\n",
    "    for valid_name in valid_component_names:\n",
    "        if valid_name.lower() in normalized.lower():\n",
    "            return valid_name\n",
    "    \n",
    "    logger.warning(f\"--- Component name '{original_name}' not found in DFD; retaining original. ---\")\n",
    "    return original_name\n",
    "\n",
    "def generate_justification(threat, flow_details):\n",
    "    \"\"\"Generate tailored justification for impact and likelihood based on data classification.\"\"\"\n",
    "    impact = threat['impact']\n",
    "    likelihood = threat['likelihood']\n",
    "    data_classification = flow_details.get(\"data_classification\", \"Unclassified\") if flow_details else \"Unclassified\"\n",
    "\n",
    "    # Justification for Impact\n",
    "    impact_reason = f\"Impact rated {impact} because \"\n",
    "    if data_classification != \"Unclassified\":\n",
    "        impact_reason += f\"the data flow handles '{data_classification}' data, \"\n",
    "        if data_classification in [\"PII\", \"Confidential\", \"PHI\", \"PCI\"]:\n",
    "            impact_reason += \"and a breach could lead to regulatory fines and significant reputational damage.\"\n",
    "        else:\n",
    "            impact_reason += \"and a breach could cause moderate business disruption.\"\n",
    "    elif \"DB_P\" in threat[\"component_name\"]: # Fallback if no classification\n",
    "         impact_reason += \"of potential exposure of sensitive data in the primary database, leading to severe reputational damage.\"\n",
    "    else: # Generic fallback\n",
    "        impact_reason += {\n",
    "            \"Critical\": \"of potential for severe business disruption or data breach.\",\n",
    "            \"High\": \"of potential for significant business disruption or data exposure.\",\n",
    "            \"Medium\": \"of moderate disruption or partial data exposure.\",\n",
    "            \"Low\": \"of minimal operational impact.\"\n",
    "        }.get(impact, \"of minimal operational impact.\")\n",
    "\n",
    "    # Justification for Likelihood\n",
    "    likelihood_reason = f\"Likelihood rated {likelihood} because \"\n",
    "    if flow_details and flow_details.get(\"source\") == 'U':\n",
    "        likelihood_reason += \"the component is internet-facing, increasing the attack surface.\"\n",
    "    else:\n",
    "        likelihood_reason += \"the component is internal, reducing direct exposure.\"\n",
    "\n",
    "    return f\"{impact_reason} {likelihood_reason}\"\n",
    "\n",
    "\n",
    "def generate_risk_statement(threat, flow_details, industry=\"Generic\"):\n",
    "    \"\"\"Generate a business-contextualized risk statement using data classification.\"\"\"\n",
    "    impact_map = {\n",
    "        \"Critical\": \"a critical event, potentially causing severe financial loss (e.g., >$1M), major regulatory fines, and long-term reputational damage\",\n",
    "        \"High\": \"significant financial loss (e.g., >$500K), regulatory fines, or reputational damage\",\n",
    "        \"Medium\": \"moderate financial loss (e.g., $50K-$500K) or operational disruption\",\n",
    "        \"Low\": \"minimal financial or operational impact\"\n",
    "    }\n",
    "    component = threat[\"component_name\"]\n",
    "    data_classification = flow_details.get(\"data_classification\", \"data\") if flow_details else \"data\"\n",
    "\n",
    "    risk = f\"Risk of {threat['threat_description'].lower()} on the '{component}' flow, which handles **{data_classification}**, could lead to {impact_map[threat['impact']]}.\"\n",
    "    \n",
    "    if industry == \"Finance\" and data_classification == \"PCI\":\n",
    "        risk += \" This may violate PCI-DSS compliance.\"\n",
    "    elif industry == \"Healthcare\" and data_classification == \"PHI\":\n",
    "        risk += \" This may violate HIPAA regulations.\"\n",
    "    \n",
    "    # Comment on residual risk based on mitigation maturity\n",
    "    if threat['residual_risk_score'] < threat['risk_score']:\n",
    "         risk += f\" The proposed mitigation, '{threat['mitigation_suggestion']}', is expected to reduce the risk to '{threat['residual_risk_score']}'.\"\n",
    "         \n",
    "    return risk\n",
    "\n",
    "def suppress_threats(threats, controls, dfd_data):\n",
    "    \"\"\"Suppress or downgrade threats based on implemented controls and CVE relevance.\"\"\"\n",
    "    active_threats = []\n",
    "    for threat in threats:\n",
    "        suppress = False\n",
    "        component = threat[\"component_name\"]\n",
    "        flow = next((f for f in dfd_data.get(\"data_flows\", []) if f\"{f['source']} to {f['destination']}\" == component), None)\n",
    "        protocol = flow.get(\"protocol\", \"Unknown\") if flow else \"Unknown\"\n",
    "\n",
    "        # Suppress based on controls\n",
    "        if controls.get(\"mtls_enabled\") and \"spoof\" in threat[\"threat_description\"].lower():\n",
    "            logger.info(f\"--- Suppressing '{component}' ({threat['stride_category']}) due to mTLS control. ---\")\n",
    "            suppress = True\n",
    "        if controls.get(\"secrets_manager\") and \"cleartext\" in threat[\"threat_description\"].lower():\n",
    "            logger.info(f\"--- Suppressing '{component}' ({threat['stride_category']}) due to secrets management. ---\")\n",
    "            suppress = True\n",
    "        \n",
    "        # Suppress based on irrelevant CVEs\n",
    "        if not suppress:\n",
    "            relevant_references = []\n",
    "            for ref in threat.get(\"references\", []):\n",
    "                if ref.startswith(\"CVE-\") and not check_cve_relevance(ref):\n",
    "                    logger.info(f\"--- Removing outdated/irrelevant CVE '{ref}' from threat '{component}'. ---\")\n",
    "                else:\n",
    "                    relevant_references.append(ref)\n",
    "            \n",
    "            # If all references were irrelevant CVEs, suppress the threat\n",
    "            if threat.get(\"references\") and not relevant_references:\n",
    "                 logger.info(f\"--- Suppressing threat for '{component}' as its only CVE references were irrelevant. ---\")\n",
    "                 suppress = True\n",
    "            else:\n",
    "                threat[\"references\"] = relevant_references\n",
    "\n",
    "        if not suppress:\n",
    "            active_threats.append(threat)\n",
    "            \n",
    "    return active_threats\n",
    "\n",
    "def deduplicate_threats(threats, similarity_threshold=0.80):\n",
    "    \"\"\"Deduplicate threats using clustering on description and mitigation similarity.\"\"\"\n",
    "    if not threats:\n",
    "        return []\n",
    "    logger.info(\"--- Starting threat deduplication ---\")\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # Embed a combination of description and mitigation for semantic meaning\n",
    "    combined_texts = [f\"{threat['threat_description']} {threat['mitigation_suggestion']}\" for threat in threats]\n",
    "    embeddings = model.encode(combined_texts, convert_to_tensor=True).cpu().numpy()\n",
    "    \n",
    "    # Use DBSCAN for density-based clustering\n",
    "    clustering = DBSCAN(eps=1 - similarity_threshold, min_samples=1, metric=\"cosine\").fit(embeddings)\n",
    "    labels = clustering.labels_\n",
    "    \n",
    "    # Group threats by cluster, component, and STRIDE for accurate merging\n",
    "    groups = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        key = (label, threats[idx][\"component_name\"], threats[idx][\"stride_category\"])\n",
    "        if key not in groups:\n",
    "            groups[key] = []\n",
    "        groups[key].append(idx)\n",
    "    \n",
    "    # Merge threats within each cluster\n",
    "    deduplicated_threats = []\n",
    "    for key, indices in groups.items():\n",
    "        if len(indices) == 1:\n",
    "            deduplicated_threats.append(threats[indices[0]])\n",
    "        else:\n",
    "            cluster_threats = [threats[i] for i in indices]\n",
    "            # Choose the most detailed description and mitigation from the cluster\n",
    "            primary_threat = max(cluster_threats, key=lambda t: len(t.get('threat_description', '')))\n",
    "            primary_threat['mitigation_suggestion'] = max(cluster_threats, key=lambda t: len(t.get('mitigation_suggestion', ''))).get('mitigation_suggestion')\n",
    "\n",
    "            # Combine all unique references\n",
    "            combined_references = set()\n",
    "            for t in cluster_threats:\n",
    "                combined_references.update(t.get(\"references\", []))\n",
    "            primary_threat[\"references\"] = sorted(list(combined_references))\n",
    "            \n",
    "            deduplicated_threats.append(primary_threat)\n",
    "            logger.info(f\"--- Merged {len(indices)} similar threats for '{primary_threat['component_name']}' ({primary_threat['stride_category']}) ---\")\n",
    "    \n",
    "    logger.info(f\"--- Deduplication reduced {len(threats)} threats to {len(deduplicated_threats)} ---\")\n",
    "    return deduplicated_threats\n",
    "\n",
    "# --- Main Refinement Logic ---\n",
    "def refine_threats():\n",
    "    \"\"\"Refine threats by deduplicating, standardizing, and enhancing with business risk context.\"\"\"\n",
    "    logger.info(f\"--- Loading initial threats from '{THREATS_INPUT_PATH}' ---\")\n",
    "    try:\n",
    "        with open(THREATS_INPUT_PATH, 'r') as f:\n",
    "            threat_data = json.load(f)\n",
    "        threats = threat_data.get(\"threats\", [])\n",
    "        if not threats:\n",
    "            raise ValueError(\"Input file contains no threats.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Failed to load threats: {e} ---\")\n",
    "        raise\n",
    "\n",
    "    dfd_data = load_dfd_components()\n",
    "    controls = load_controls()\n",
    "    industry = os.getenv(\"CLIENT_INDUSTRY\", \"Generic\")\n",
    "    dfd_flows = dfd_data.get(\"data_flows\", [])\n",
    "    original_threat_count = len(threats)\n",
    "\n",
    "    # Step 1: Standardize component names to match DFD\n",
    "    for threat in threats:\n",
    "        threat[\"component_name\"] = standardize_component_name(threat[\"component_name\"], dfd_flows)\n",
    "\n",
    "    # Step 2: Suppress threats based on controls and CVE relevance\n",
    "    threats = suppress_threats(threats, controls, dfd_data)\n",
    "\n",
    "    # Step 3: Deduplicate remaining threats\n",
    "    threats = deduplicate_threats(threats)\n",
    "\n",
    "    # Step 4: Enrich each threat with calculated metadata\n",
    "    refined_threats = []\n",
    "    for threat in threats:\n",
    "        flow_details = next((f for f in dfd_flows if f\"{f['source']} to {f['destination']}\" == threat[\"component_name\"]), None)\n",
    "        if flow_details and flow_details.get(\"data_classification\") is None:\n",
    "            logger.warning(f\"--- Data flow '{threat['component_name']}' is missing 'data_classification'. Impact assessment will be generic. ---\")\n",
    "\n",
    "        # Set impact based on data classification if not already high\n",
    "        if flow_details and flow_details.get(\"data_classification\") in [\"PII\", \"PHI\", \"PCI\", \"Confidential\"]:\n",
    "            threat[\"impact\"] = \"Critical\" if threat[\"impact\"] == \"High\" else \"High\"\n",
    "\n",
    "        # Calculate scores and assessments\n",
    "        threat[\"risk_score\"] = calculate_risk_score(threat[\"impact\"], threat[\"likelihood\"])\n",
    "        mitigated_likelihood = \"Low\" if \"logging\" not in threat[\"mitigation_suggestion\"].lower() else threat[\"likelihood\"]\n",
    "        threat[\"residual_risk_score\"] = calculate_risk_score(threat[\"impact\"], mitigated_likelihood)\n",
    "        threat[\"exploitability\"] = assess_exploitability(threat, dfd_data)\n",
    "        threat[\"mitigation_maturity\"] = assess_mitigation_maturity(threat[\"mitigation_suggestion\"])\n",
    "        \n",
    "        # Generate human-readable statements\n",
    "        threat[\"justification\"] = generate_justification(threat, flow_details)\n",
    "        threat[\"risk_statement\"] = generate_risk_statement(threat, flow_details, industry)\n",
    "        \n",
    "        refined_threats.append(threat)\n",
    "\n",
    "    # Step 5: Sort by risk score for prioritization\n",
    "    risk_order = {\"Critical\": 4, \"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    refined_threats.sort(key=lambda t: risk_order.get(t.get(\"risk_score\"), 0), reverse=True)\n",
    "\n",
    "    # Step 6: Assemble and validate the final output\n",
    "    final_output = {\n",
    "        \"threats\": refined_threats,\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_dfd\": os.path.basename(DFD_INPUT_PATH),\n",
    "            \"source_threats\": os.path.basename(THREATS_INPUT_PATH),\n",
    "            \"refined_threat_count\": len(refined_threats),\n",
    "            \"original_threat_count\": original_threat_count,\n",
    "            \"industry_context\": industry\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        validated_output = RefinedThreatsOutput(**final_output)\n",
    "        logger.info(\"--- Final refined JSON output validated successfully against schema. ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- Final JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "\n",
    "    # Step 7: Save all outputs\n",
    "    with open(REFINED_THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(validated_output.model_dump(), f, indent=2)\n",
    "    logger.info(f\"--- Refined {len(refined_threats)} threats saved to '{REFINED_THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "    summary = {\n",
    "        \"total_threats\": len(refined_threats),\n",
    "        \"critical_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Critical\"),\n",
    "        \"high_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"High\"),\n",
    "        \"medium_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Medium\"),\n",
    "        \"low_count\": sum(1 for t in refined_threats if t[\"risk_score\"] == \"Low\"),\n",
    "        \"prioritization_recommendation\": (\n",
    "            \"Remediation should be prioritized based on risk score. Address all 'Critical' and 'High' risk threats within the next development cycle. \"\n",
    "            \"Focus on implementing robust, mature controls like mTLS and centralized logging to address systemic weaknesses.\"\n",
    "        )\n",
    "    }\n",
    "    summary_path = os.path.join(INPUT_DIR, \"threat_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    logger.info(f\"--- Summary report saved to '{summary_path}' ---\")\n",
    "\n",
    "    pd.DataFrame(final_output[\"threats\"]).to_csv(os.path.join(INPUT_DIR, \"threats.csv\"), index=False)\n",
    "    logger.info(f\"--- CSV report saved to '{os.path.join(INPUT_DIR, 'threats.csv')}' ---\")\n",
    "\n",
    "# --- Execute ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_cisa_kev_catalog() # Pre-fetch KEV catalog on startup\n",
    "        refine_threats()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Threat refinement process failed with an unrecoverable error: {e} ---\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e4d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71b796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
