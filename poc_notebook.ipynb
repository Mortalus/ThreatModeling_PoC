{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 07:03:02,577 - INFO - --- Verifying NLTK data packages ---\n",
      "2025-07-27 07:03:02,578 - INFO - [✓] NLTK 'punkt' data is available.\n",
      "2025-07-27 07:03:02,578 - INFO - --- NLTK setup complete ---\n",
      "2025-07-27 07:03:02,578 - INFO - --- Loading document and extracting images from: ./docs/designdoc.docx ---\n",
      "2025-07-27 07:03:02,598 - INFO - --- Successfully processed document ---\n",
      "2025-07-27 07:03:02,599 - INFO - \n",
      "--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\n",
      "2025-07-27 07:03:10,780 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 07:03:29,077 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-27 07:03:29,078 - INFO - \n",
      "--- LLM Output (Parsed JSON) ---\n",
      "2025-07-27 07:03:29,079 - INFO - \n",
      "--- DFD components successfully saved to './output/dfd_components.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"assets\": [\n",
      "    \"Primary Database\",\n",
      "    \"Backup System\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"User\",\n",
      "    \"CDN / WAF\",\n",
      "    \"Load Balancer\",\n",
      "    \"Web Server Farm\",\n",
      "    \"Message Queue Server\",\n",
      "    \"Worker Service\",\n",
      "    \"Admin Portal\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"User\",\n",
      "      \"destination\": \"CDN / WAF\",\n",
      "      \"data_description\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"CDN / WAF\",\n",
      "      \"destination\": \"Load Balancer\",\n",
      "      \"data_description\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Load Balancer\",\n",
      "      \"destination\": \"Web Server Farm\",\n",
      "      \"data_description\": \"HTTPS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Web Server Farm\",\n",
      "      \"destination\": \"Primary Database\",\n",
      "      \"data_description\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Web Server Farm\",\n",
      "      \"destination\": \"Message Queue Server\",\n",
      "      \"data_description\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Worker Service\",\n",
      "      \"destination\": \"Message Queue Server\",\n",
      "      \"data_description\": \"AMQP over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Worker Service\",\n",
      "      \"destination\": \"Primary Database\",\n",
      "      \"data_description\": \"JDBC/ODBC over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Backup System\",\n",
      "      \"destination\": \"Primary Database\",\n",
      "      \"data_description\": \"Backup Protocol over TLS\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Admin\",\n",
      "      \"destination\": \"Admin Portal\",\n",
      "      \"data_description\": \"HTTPS over VPN\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Admin Portal\",\n",
      "      \"destination\": \"Load Balancer\",\n",
      "      \"data_description\": \"HTTPS\"\n",
      "    }\n",
      "  ],\n",
      "  \"trust_boundaries\": [\n",
      "    \"Public Zone\",\n",
      "    \"Edge Zone\",\n",
      "    \"Application DMZ\",\n",
      "    \"Internal Core\",\n",
      "    \"Data Zone\",\n",
      "    \"Management Zone\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T07:03:29.077606\",\n",
      "    \"source_document\": \"./docs/designdoc.docx\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama \"unstructured[docx]\" pillow nltk python-dotenv pydantic logging\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ssl\n",
    "import nltk\n",
    "from urllib.error import URLError\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from datetime import datetime\n",
    "# --- MODIFICATION: Import partition_docx ---\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use environment variables for paths and settings\n",
    "DOCUMENT_PATH = os.getenv(\"DOCUMENT_PATH\", \"./docs/designdoc.docx\")\n",
    "IMAGE_OUTPUT_DIR = os.getenv(\"IMAGE_OUTPUT_DIR\", \"./output/images\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "NLTK_DATA_PATH = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "llm = ChatOllama(model=\"WhiteRabbitNeo/WhiteRabbitNeo-V3-7B\")\n",
    "\n",
    "# --- NLTK Data Management ---\n",
    "# This function checks for necessary NLTK data and downloads it if missing.\n",
    "# Avoid SSL unverified context; assume secure environment or pre-downloaded data.\n",
    "def ensure_nltk_data():\n",
    "    \"\"\"\n",
    "    Checks for and downloads required NLTK data securely.\n",
    "    \"\"\"\n",
    "    required_package = 'punkt'\n",
    "\n",
    "    if NLTK_DATA_PATH not in nltk.data.path:\n",
    "        nltk.data.path.append(NLTK_DATA_PATH)\n",
    "\n",
    "    logger.info(\"--- Verifying NLTK data packages ---\")\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        logger.info(\"[✓] NLTK 'punkt' data is available.\")\n",
    "    except LookupError:\n",
    "        logger.warning(\"[!] NLTK 'punkt' data not found. Attempting to download...\")\n",
    "        try:\n",
    "            # Use default SSL context; no unverified workaround\n",
    "            nltk.download(required_package, download_dir=NLTK_DATA_PATH)\n",
    "            logger.info(f\"[✓] '{required_package}' downloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download NLTK data: {e}\")\n",
    "            logger.error(\"Ensure internet access and proper SSL configuration. Alternatively, pre-download NLTK data.\")\n",
    "            raise\n",
    "    logger.info(\"--- NLTK setup complete ---\")\n",
    "\n",
    "# --- DFD Components Schema for Validation ---\n",
    "class DataFlow(BaseModel):\n",
    "    source: str\n",
    "    destination: str\n",
    "    data_description: str\n",
    "\n",
    "class DFDComponents(BaseModel):\n",
    "    assets: list[str]\n",
    "    processes: list[str]\n",
    "    data_flows: list[DataFlow]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Document & Image Loading with partition_docx ---\n",
    "full_document_text = \"\"\n",
    "elements = []\n",
    "\n",
    "try:\n",
    "    # Ensure directories exist\n",
    "    os.makedirs(IMAGE_OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    ensure_nltk_data()\n",
    "\n",
    "    logger.info(f\"--- Loading document and extracting images from: {DOCUMENT_PATH} ---\")\n",
    "    # Use partition_docx to extract both text and images\n",
    "    # Note: For security, consider scanning the file with an antivirus tool before processing in production.\n",
    "    elements = partition_docx(\n",
    "        filename=DOCUMENT_PATH,\n",
    "        extract_images_in_document=True,\n",
    "        image_output_dir_path=IMAGE_OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "    # Reconstruct the document text from elements for the LLM\n",
    "    # and confirm image extractions.\n",
    "    for element in elements:\n",
    "        if hasattr(element, 'text'):\n",
    "            full_document_text += element.text + \"\\n\\n\"\n",
    "        elif \"Image\" in str(type(element)):\n",
    "            if element.metadata.image_path:\n",
    "                logger.info(f\"[✓] Extracted image saved to: {element.metadata.image_path}\")\n",
    "\n",
    "    if not full_document_text.strip():\n",
    "        raise ValueError(\"Document processed but no text content was found.\")\n",
    "\n",
    "    logger.info(\"--- Successfully processed document ---\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"--- FATAL ERROR: Input document not found at '{DOCUMENT_PATH}' ---\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- FATAL ERROR: An unexpected error occurred while processing the document ---\")\n",
    "    logger.error(f\"Error details: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Prompt Engineering ---\n",
    "prompt_template = \"\"\"\n",
    "You are an expert cybersecurity architect specializing in threat modeling, with knowledge of current standards as of 2025, including OWASP Threat Modeling Cheat Sheet and NIST SP 800-53.\n",
    "\n",
    "Your task is to carefully read the provided system design document and decompose it into core components for a Data Flow Diagram (DFD) Level 0 or 1. Use a Chain-of-Thought approach: \n",
    "1. First, identify all mentioned components, data stores, and interactions from the document.\n",
    "2. Classify them systematically: Assets (data at rest, e.g., databases, files), Processes (data transformation, e.g., servers, services), External Entities (e.g., users, third-party APIs).\n",
    "3. Map data flows between them, describing the data exchanged and any protocols or security controls mentioned.\n",
    "4. Identify trust boundaries (e.g., zones like public, DMZ, internal) and any external dependencies.\n",
    "\n",
    "Output a valid JSON object with the following keys:\n",
    "- 'external_entities': Array of strings (e.g., users or external systems not under control).\n",
    "- 'assets': Array of strings (data stores where data rests, e.g., database, cache, log file).\n",
    "- 'processes': Array of strings (components that act on or transform data, e.g., API, microservice, web server).\n",
    "- 'data_flows': Array of objects, each with 'source' (from external_entities, assets, or processes), 'destination' (same), 'data_description' (what data is exchanged, e.g., \"user credentials\"), 'protocol' (e.g., \"HTTPS\", if mentioned).\n",
    "- 'trust_boundaries': Array of strings describing zones or boundaries (e.g., \"Public Zone to Edge Zone\").\n",
    "\n",
    "Stick strictly to the document content—do not hallucinate or add unmentioned elements. If the document includes diagrams or images (via extracted text), incorporate their descriptions.\n",
    "\n",
    "System Design Document:\n",
    "---\n",
    "{document_text}\n",
    "---\n",
    "\n",
    "Generate ONLY the JSON object, with no additional text, explanations, or formatting.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# --- Chain Construction with JSON Output Parser ---\n",
    "output_parser = JsonOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\")\n",
    "output_path = os.path.join(OUTPUT_DIR, \"dfd_components.json\")\n",
    "\n",
    "try:\n",
    "    # Parameterize prompt to mitigate injection (though document_text is trusted here)\n",
    "    response_dict = chain.invoke({\"document_text\": full_document_text})\n",
    "    \n",
    "    # Add metadata\n",
    "    response_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_document\": DOCUMENT_PATH\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema\n",
    "    try:\n",
    "        validated = DFDComponents(**response_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    logger.info(\"\\n--- LLM Output (Parsed JSON) ---\")\n",
    "    print(json.dumps(response_dict, indent=2))\n",
    "    \n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(response_dict, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"\\n--- DFD components successfully saved to '{output_path}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during chain invocation or parsing ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This may be due to the LLM not returning a well-formed JSON object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58d2a7-0a06-48fc-bdae-b7efe4f3b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 07:03:29,127 - INFO - --- Loading DFD components from './output/dfd_components.json' ---\n",
      "2025-07-27 07:03:29,127 - INFO - --- DFD components loaded successfully ---\n",
      "2025-07-27 07:03:29,128 - INFO - \n",
      "--- Invoking Local LLM Chain (Mixtral) to generate STRIDE threats ---\n",
      "2025-07-27 07:03:45,117 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-27 07:04:09,795 - INFO - --- JSON output validated successfully ---\n",
      "2025-07-27 07:04:09,799 - INFO - \n",
      "--- LLM Output (Identified Threats) ---\n",
      "2025-07-27 07:04:09,800 - INFO - \n",
      "--- Identified threats successfully saved to './output/identified_threats.json' ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"threats\": [\n",
      "    {\n",
      "      \"component_name\": \"User\",\n",
      "      \"stride_category\": \"Spoofing\",\n",
      "      \"threat_description\": \"An attacker may impersonate a legitimate user to gain unauthorized access.\",\n",
      "      \"mitigation_suggestion\": \"Implement multi-factor authentication and enforce strong password policies.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"CDN / WAF\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Sensitive data may be exposed due to misconfigured or weak security settings in the CDN/WAF.\",\n",
      "      \"mitigation_suggestion\": \"Regularly audit and configure CDN/WAF settings according to best practices.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Load Balancer\",\n",
      "      \"stride_category\": \"Denial of Service\",\n",
      "      \"threat_description\": \"A DDoS attack can overload the load balancer, making services unavailable.\",\n",
      "      \"mitigation_suggestion\": \"Implement a robust DDoS mitigation strategy, including traffic filtering and rate limiting.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Web Server Farm\",\n",
      "      \"stride_category\": \"Tampering\",\n",
      "      \"threat_description\": \"An attacker can manipulate data in transit between the web server farm and primary database.\",\n",
      "      \"mitigation_suggestion\": \"Use strong encryption protocols like JDBC/ODBC over TLS and regularly monitor logs for unusual activities.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Primary Database\",\n",
      "      \"stride_category\": \"Elevation of Privilege\",\n",
      "      \"threat_description\": \"Unauthorized access to the primary database can lead to privilege escalation.\",\n",
      "      \"mitigation_suggestion\": \"Implement principle of least privilege (PoLP) and regularly review user permissions.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Message Queue Server\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Data transmitted through the message queue server might be intercepted or exposed.\",\n",
      "      \"mitigation_suggestion\": \"Use encrypted protocols like AMQP over TLS and ensure proper server configuration.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Worker Service\",\n",
      "      \"stride_category\": \"Repudiation\",\n",
      "      \"threat_description\": \"An attacker can perform unauthorized actions without leaving a trace.\",\n",
      "      \"mitigation_suggestion\": \"Implement robust auditing and logging mechanisms to record user activities.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Backup System\",\n",
      "      \"stride_category\": \"Tampering\",\n",
      "      \"threat_description\": \"Backup data may be manipulated by an attacker, leading to data loss or unavailability.\",\n",
      "      \"mitigation_suggestion\": \"Encrypt backup data and use secure backup protocols over TLS. Regularly validate the integrity of backups.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Admin\",\n",
      "      \"stride_category\": \"Spoofing\",\n",
      "      \"threat_description\": \"An attacker may impersonate an admin to gain unauthorized access or elevated privileges.\",\n",
      "      \"mitigation_suggestion\": \"Implement multi-factor authentication and enforce strong password policies for admin accounts.\"\n",
      "    },\n",
      "    {\n",
      "      \"component_name\": \"Admin Portal\",\n",
      "      \"stride_category\": \"Information Disclosure\",\n",
      "      \"threat_description\": \"Sensitive information in the admin portal may be exposed due to misconfigured or weak security settings.\",\n",
      "      \"mitigation_suggestion\": \"Regularly audit and configure admin portal settings according to best practices. Use VPN for secure connections.\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T07:04:09.795355\",\n",
      "    \"source_dfd\": \"./output/dfd_components.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama python-dotenv pydantic logging\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use environment variables for paths and settings\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"mixtral\")\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./output\")\n",
    "DFD_INPUT_PATH = os.getenv(\"DFD_INPUT_PATH\", os.path.join(INPUT_DIR, \"dfd_components.json\"))\n",
    "THREATS_OUTPUT_PATH = os.getenv(\"THREATS_OUTPUT_PATH\", os.path.join(INPUT_DIR, \"identified_threats.json\"))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL)\n",
    "\n",
    "# --- Threat Schema for Validation ---\n",
    "class Threat(BaseModel):\n",
    "    component_name: str\n",
    "    stride_category: str\n",
    "    threat_description: str\n",
    "    mitigation_suggestion: str\n",
    "\n",
    "class ThreatsOutput(BaseModel):\n",
    "    threats: list[Threat]\n",
    "    metadata: dict\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "logger.info(f\"--- Loading DFD components from '{DFD_INPUT_PATH}' ---\")\n",
    "try:\n",
    "    with open(DFD_INPUT_PATH, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    logger.info(\"--- DFD components loaded successfully ---\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"--- FATAL ERROR: Input file not found at '{DFD_INPUT_PATH}' ---\")\n",
    "    logger.error(\"Please run the first script (to generate DFD components) before running this one.\")\n",
    "    exit(1)\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"--- FATAL ERROR: Could not parse JSON from '{DFD_INPUT_PATH}' ---\")\n",
    "    logger.error(\"The file may be corrupted or empty.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"--- FATAL ERROR: An unexpected error occurred while loading DFD components ---\")\n",
    "    logger.error(f\"Error details: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Prompt Engineering for Threat Generation ---\n",
    "threat_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), aligned with 2025 standards like OWASP Top 10, NIST SP 800-53, and MITRE ATT&CK.\n",
    "\n",
    "Based on the provided Data Flow Diagram (DFD) components in JSON format, perform a comprehensive threat analysis. Use Chain-of-Thought reasoning:\n",
    "1. For each external entity, asset, process, and data flow, systematically apply all STRIDE categories where applicable.\n",
    "2. Describe threats considering trust boundaries, protocols, and potential attack vectors (e.g., injection, misconfiguration).\n",
    "3. Suggest mitigations with references to standards (e.g., \"NIST AC-6 for least privilege\").\n",
    "4. Assess impact (Low/Medium/High based on potential damage) and likelihood (Low/Medium/High based on exploitability).\n",
    "\n",
    "For each threat, include:\n",
    "- 'component_name': Affected asset, process, data flow, or entity.\n",
    "- 'stride_category': One STRIDE category.\n",
    "- 'threat_description': Clear, specific description (e.g., \"Attacker intercepts unencrypted data in transit leading to disclosure\").\n",
    "- 'mitigation_suggestion': Practical, actionable mitigation (e.g., \"Implement TLS 1.3 with certificate pinning\").\n",
    "- 'impact': Low/Medium/High.\n",
    "- 'likelihood': Low/Medium/High.\n",
    "- 'references': Array of strings (e.g., [\"OWASP A01:2021\", \"NIST SI-2\"]).\n",
    "\n",
    "DFD Components:\n",
    "---\n",
    "{dfd_json}\n",
    "---\n",
    "\n",
    "Generate a JSON object with a key 'threats' (array of threat objects). Output ONLY the JSON, with no additional commentary or formatting.\n",
    "\"\"\"\n",
    "\n",
    "threat_prompt = ChatPromptTemplate.from_template(threat_prompt_template)\n",
    "\n",
    "# --- Chain Construction with JSON Output Parser ---\n",
    "threat_parser = JsonOutputParser()\n",
    "threat_chain = threat_prompt | llm | threat_parser\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "logger.info(\"\\n--- Invoking Local LLM Chain (Mixtral) to generate STRIDE threats ---\")\n",
    "try:\n",
    "    # Convert the loaded DFD dictionary back to a JSON string for the prompt\n",
    "    dfd_json_string = json.dumps(dfd_data, indent=2)\n",
    "\n",
    "    # Invoke the threat analysis chain\n",
    "    threats_dict = threat_chain.invoke({\"dfd_json\": dfd_json_string})\n",
    "    \n",
    "    # Add metadata\n",
    "    threats_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_dfd\": DFD_INPUT_PATH\n",
    "    }\n",
    "    \n",
    "    # Validate the output against schema\n",
    "    try:\n",
    "        validated = ThreatsOutput(**threats_dict)\n",
    "        logger.info(\"--- JSON output validated successfully ---\")\n",
    "    except ValidationError as ve:\n",
    "        logger.error(f\"--- JSON validation failed: {ve} ---\")\n",
    "        raise\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save the threats to a new file\n",
    "    with open(THREATS_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(threats_dict, f, indent=2)\n",
    "        \n",
    "    logger.info(\"\\n--- LLM Output (Identified Threats) ---\")\n",
    "    print(json.dumps(threats_dict, indent=2))\n",
    "    logger.info(f\"\\n--- Identified threats successfully saved to '{THREATS_OUTPUT_PATH}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n--- An error occurred during threat generation ---\")\n",
    "    logger.error(f\"Error: {e}\")\n",
    "    logger.error(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb0223-adbe-438a-b6b3-eb6ce3a9f852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
