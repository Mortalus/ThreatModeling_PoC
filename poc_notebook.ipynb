{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying NLTK data packages... ---\n",
      "  [✓] NLTK 'punkt' data is available.\n",
      "--- NLTK setup complete. ---\n",
      "--- Successfully loaded document: ./docs/designdoc.docx ---\n",
      "\n",
      "--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\n",
      "\n",
      "--- LLM Output (Parsed JSON) ---\n",
      "{\n",
      "  \"assets\": [\n",
      "    \"Azure Database for MySQL\",\n",
      "    \"Azure Blob Storage\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"Browser\",\n",
      "    \"Azure App Service (WordPress/WooCommerce)\",\n",
      "    \"Azure Database for MySQL\",\n",
      "    \"Stripe/PayPal API\",\n",
      "    \"Admin Browser\",\n",
      "    \"/wp-admin\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"Browser\",\n",
      "      \"destination\": \"Azure App Service (WordPress/WooCommerce)\",\n",
      "      \"data_description\": \"Customer browsing products\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Azure App Service (WordPress/WooCommerce)\",\n",
      "      \"destination\": \"Azure Database for MySQL\",\n",
      "      \"data_description\": \"Product data, customer actions\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Browser\",\n",
      "      \"destination\": \"Stripe/PayPal API\",\n",
      "      \"data_description\": \"Payment information\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Stripe/PayPal API\",\n",
      "      \"destination\": \"Azure Database for MySQL\",\n",
      "      \"data_description\": \"Order creation data\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"/wp-admin\",\n",
      "      \"destination\": \"Azure Database for MySQL\",\n",
      "      \"data_description\": \"Admin product and order management actions\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"timestamp\": \"2025-07-27T05:44:15.002042\",\n",
      "    \"source_document\": \"./docs/designdoc.docx\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--- DFD components successfully saved to './output/dfd_components_20250727_054405.json' ---\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama unstructured[docx] nltk\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ssl  # Imported to handle SSL certificate verification issues\n",
    "import nltk  # Required for the data download function\n",
    "from urllib.error import URLError  # To catch the specific download error\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from datetime import datetime\n",
    "\n",
    "# --- NLTK Data Management ---\n",
    "# This function checks for necessary NLTK data and downloads it if missing.\n",
    "# Unstructured typically requires 'punkt' for sentence tokenization.\n",
    "def ensure_nltk_data():\n",
    "    \"\"\"\n",
    "    Checks for and downloads required NLTK data, handling potential SSL errors.\n",
    "    \"\"\"\n",
    "    required_package = 'punkt'  # Minimal requirement for Unstructured\n",
    "    nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "\n",
    "    if not os.path.exists(nltk_data_path):\n",
    "        os.makedirs(nltk_data_path)\n",
    "\n",
    "    if nltk_data_path not in nltk.data.path:\n",
    "        nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "    print(\"--- Verifying NLTK data packages... ---\")\n",
    "    try:\n",
    "        # Check for 'punkt'\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        print(f\"  [✓] NLTK 'punkt' data is available.\")\n",
    "    except LookupError:\n",
    "        print(f\"  [!] NLTK 'punkt' data not found. Attempting to download...\")\n",
    "        try:\n",
    "            # First, try downloading normally.\n",
    "            nltk.download(required_package, download_dir=nltk_data_path)\n",
    "        except URLError as e:\n",
    "            # If a URLError with an SSL certificate issue occurs, apply the workaround.\n",
    "            if \"CERTIFICATE_VERIFY_FAILED\" in str(e):\n",
    "                print(\"  [!] SSL certificate verification failed. Applying workaround...\")\n",
    "                # Create an unverified SSL context.\n",
    "                ssl._create_default_https_context = ssl._create_unverified_context\n",
    "                # Retry the download with the unverified context.\n",
    "                nltk.download(required_package, download_dir=nltk_data_path)\n",
    "                print(f\"  [✓] '{required_package}' downloaded successfully using SSL workaround.\")\n",
    "            else:\n",
    "                # If it's a different URLError, re-raise it.\n",
    "                raise\n",
    "        print(f\"  [✓] '{required_package}' downloaded successfully.\")\n",
    "\n",
    "    print(\"--- NLTK setup complete. ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "llm = ChatOllama(model=\"mixtral\")\n",
    "\n",
    "# --- NLTK Data Path Configuration ---\n",
    "ensure_nltk_data()  # No need to store the path, as it's appended to nltk.data.path\n",
    "\n",
    "# --- Document Loading with Error Handling ---\n",
    "document_path = \"./docs/designdoc.docx\"\n",
    "document = None\n",
    "\n",
    "try:\n",
    "    loader = UnstructuredWordDocumentLoader(document_path)\n",
    "    document = loader.load()\n",
    "    print(f\"--- Successfully loaded document: {document_path} ---\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- FATAL ERROR: Input document not found at '{document_path}' ---\")\n",
    "    print(\"Please ensure the 'designdoc.docx' file exists in a 'docs' subdirectory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"--- FATAL ERROR: An unexpected error occurred while loading the document. ---\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit()\n",
    "\n",
    "if not document or not document[0].page_content:\n",
    "    print(\"--- FATAL ERROR: Document loaded but is empty. ---\")\n",
    "    exit()\n",
    "\n",
    "# --- Prompt Engineering ---\n",
    "prompt_template = \"\"\"\n",
    "You are an expert cybersecurity architect specializing in threat modeling.\n",
    "Your task is to read the provided system design document and extract the core components\n",
    "for a Data Flow Diagram (DFD) in a valid JSON format.\n",
    "\n",
    "The JSON output must contain three keys: 'assets', 'processes', and 'data_flows'.\n",
    "- An 'asset' is a data store where data rests (e.g., a database, a cache, a log file). List as an array of strings.\n",
    "- A 'process' is a component that acts on or transforms data (e.g., an API, a microservice, a user-facing application). List as an array of strings.\n",
    "- A 'data_flow' is an array of objects, each with 'source', 'destination', and 'data_description'. The 'source' and 'destination'\n",
    "  must be one of the previously identified processes or assets.\n",
    "\n",
    "System Design Document:\n",
    "---\n",
    "{document_text}\n",
    "---\n",
    "\n",
    "Now, generate the JSON object based on the document. Output ONLY the JSON object itself, with no\n",
    "additional commentary, explanations, or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# --- Chain Construction with JSON Output Parser ---\n",
    "output_parser = JsonOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "print(\"\\n--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\")\n",
    "output_dir = \"./output\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#output_path = os.path.join(output_dir, f\"dfd_components_{timestamp}.json\")\n",
    "output_path = os.path.join(output_dir, f\"dfd_components.json\")\n",
    "\n",
    "try:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    response_dict = chain.invoke({\"document_text\": document[0].page_content})\n",
    "    \n",
    "    # Add timestamp to the JSON data itself\n",
    "    response_dict[\"metadata\"] = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source_document\": document_path\n",
    "    }\n",
    "    \n",
    "    print(\"\\n--- LLM Output (Parsed JSON) ---\")\n",
    "    print(json.dumps(response_dict, indent=2))\n",
    "    \n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(response_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n--- DFD components successfully saved to '{output_path}' ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An error occurred during chain invocation or parsing ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This may be due to the LLM not returning a well-formed JSON object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58d2a7-0a06-48fc-bdae-b7efe4f3b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependencies ---\n",
    "import os\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# --- Configuration ---\n",
    "llm = ChatOllama(model=\"mixtral\")\n",
    "input_dir = \"./output\"\n",
    "dfd_input_path = os.path.join(input_dir, \"dfd_components.json\")\n",
    "threats_output_path = os.path.join(input_dir, \"identified_threats.json\")\n",
    "\n",
    "# --- Load DFD Components ---\n",
    "print(f\"--- Loading DFD components from '{dfd_input_path}' ---\")\n",
    "try:\n",
    "    with open(dfd_input_path, 'r') as f:\n",
    "        dfd_data = json.load(f)\n",
    "    print(\"--- DFD components loaded successfully. ---\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- FATAL ERROR: Input file not found at '{dfd_input_path}' ---\")\n",
    "    print(\"Please run the first script (to generate DFD components) before running this one.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"--- FATAL ERROR: Could not parse JSON from '{dfd_input_path}'. ---\")\n",
    "    print(\"The file may be corrupted or empty.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prompt Engineering for Threat Generation ---\n",
    "threat_prompt_template = \"\"\"\n",
    "You are a senior cybersecurity analyst specializing in threat modeling using the STRIDE methodology.\n",
    "Based on the provided Data Flow Diagram (DFD) components in JSON format, identify potential threats for each data flow, process, and asset.\n",
    "\n",
    "For each identified threat, provide the following details:\n",
    "- 'component_name': The name of the affected asset, process, or data flow.\n",
    "- 'stride_category': The relevant STRIDE category (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege).\n",
    "- 'threat_description': A clear and concise description of the specific threat.\n",
    "- 'mitigation_suggestion': A practical suggestion for mitigating the threat.\n",
    "\n",
    "DFD Components:\n",
    "---\n",
    "{dfd_json}\n",
    "---\n",
    "\n",
    "Generate a JSON object with a single key 'threats', which contains a list of all identified threats.\n",
    "Output ONLY the JSON object itself, with no additional commentary or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "threat_prompt = ChatPromptTemplate.from_template(threat_prompt_template)\n",
    "\n",
    "# --- Chain Construction with JSON Output Parser ---\n",
    "threat_parser = JsonOutputParser()\n",
    "threat_chain = threat_prompt | llm | threat_parser\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "print(\"\\n--- Invoking Local LLM Chain (Mixtral) to generate STRIDE threats ---\")\n",
    "try:\n",
    "    # Convert the loaded DFD dictionary back to a JSON string for the prompt\n",
    "    dfd_json_string = json.dumps(dfd_data, indent=2)\n",
    "\n",
    "    # Invoke the threat analysis chain\n",
    "    threats_dict = threat_chain.invoke({\"dfd_json\": dfd_json_string})\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the threats to a new file\n",
    "    with open(threats_output_path, 'w') as f:\n",
    "        json.dump(threats_dict, f, indent=2)\n",
    "        \n",
    "    print(\"\\n--- LLM Output (Identified Threats) ---\")\n",
    "    print(json.dumps(threats_dict, indent=2))\n",
    "    print(f\"\\n--- Identified threats successfully saved to '{threats_output_path}' ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An error occurred during threat generation ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This could be due to the LLM not returning a well-formed JSON object or an issue with the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb0223-adbe-438a-b6b3-eb6ce3a9f852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
