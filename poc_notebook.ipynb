{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c045a38-4208-462a-b050-3fab89383e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying NLTK data packages... ---\n",
      "  [!] NLTK 'punkt' data not found. Attempting to download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeffreyvonrotz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [✓] 'punkt' downloaded successfully.\n",
      "--- NLTK setup complete. ---\n",
      "--- Successfully loaded document: ./docs/designdoc.docx ---\n",
      "\n",
      "--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\n",
      "\n",
      "--- LLM Output (Parsed JSON) ---\n",
      "{\n",
      "  \"assets\": [\n",
      "    \"Azure Database for MySQL\",\n",
      "    \"Azure Blob Storage\"\n",
      "  ],\n",
      "  \"processes\": [\n",
      "    \"Customer Browser\",\n",
      "    \"Azure App Service (WordPress/WooCommerce)\",\n",
      "    \"Azure Database for MySQL\",\n",
      "    \"Stripe/PayPal API\",\n",
      "    \"Admin Browser\",\n",
      "    \"/wp-admin\"\n",
      "  ],\n",
      "  \"data_flows\": [\n",
      "    {\n",
      "      \"source\": \"Customer Browser\",\n",
      "      \"destination\": \"Azure App Service (WordPress/WooCommerce)\",\n",
      "      \"data_description\": \"Browsing, product catalog\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Azure App Service (WordPress/WooCommerce)\",\n",
      "      \"destination\": \"Azure Database for MySQL\",\n",
      "      \"data_description\": \"Products, cart, orders, user data\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Customer Browser\",\n",
      "      \"destination\": \"Stripe/PayPal API\",\n",
      "      \"data_description\": \"Payment information\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Stripe/PayPal API\",\n",
      "      \"destination\": \"Azure Database for MySQL\",\n",
      "      \"data_description\": \"Order creation\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Admin Browser\",\n",
      "      \"destination\": \"/wp-admin\",\n",
      "      \"data_description\": \"Product management, order management, inventory control, reporting\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- JSON Validation: Success ---\n",
      "The LLM output was successfully parsed into a dictionary.\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# Ensure you have these packages installed. You can install them using pip:\n",
    "# pip install langchain langchain-community langchain-ollama unstructured[docx] nltk\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ssl  # Imported to handle SSL certificate verification issues\n",
    "import nltk  # Required for the data download function\n",
    "from urllib.error import URLError  # To catch the specific download error\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "# --- NLTK Data Management ---\n",
    "# This function checks for necessary NLTK data and downloads it if missing.\n",
    "# Unstructured typically requires 'punkt' for sentence tokenization.\n",
    "def ensure_nltk_data():\n",
    "    \"\"\"\n",
    "    Checks for and downloads required NLTK data, handling potential SSL errors.\n",
    "    \"\"\"\n",
    "    required_package = 'punkt'  # Minimal requirement for Unstructured\n",
    "    nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "\n",
    "    if not os.path.exists(nltk_data_path):\n",
    "        os.makedirs(nltk_data_path)\n",
    "\n",
    "    if nltk_data_path not in nltk.data.path:\n",
    "        nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "    print(\"--- Verifying NLTK data packages... ---\")\n",
    "    try:\n",
    "        # Check for 'punkt'\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        print(f\"  [✓] NLTK 'punkt' data is available.\")\n",
    "    except LookupError:\n",
    "        print(f\"  [!] NLTK 'punkt' data not found. Attempting to download...\")\n",
    "        try:\n",
    "            # First, try downloading normally.\n",
    "            nltk.download(required_package, download_dir=nltk_data_path)\n",
    "        except URLError as e:\n",
    "            # If a URLError with an SSL certificate issue occurs, apply the workaround.\n",
    "            if \"CERTIFICATE_VERIFY_FAILED\" in str(e):\n",
    "                print(\"  [!] SSL certificate verification failed. Applying workaround...\")\n",
    "                # Create an unverified SSL context.\n",
    "                ssl._create_default_https_context = ssl._create_unverified_context\n",
    "                # Retry the download with the unverified context.\n",
    "                nltk.download(required_package, download_dir=nltk_data_path)\n",
    "                print(f\"  [✓] '{required_package}' downloaded successfully using SSL workaround.\")\n",
    "            else:\n",
    "                # If it's a different URLError, re-raise it.\n",
    "                raise\n",
    "        print(f\"  [✓] '{required_package}' downloaded successfully.\")\n",
    "\n",
    "    print(\"--- NLTK setup complete. ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "llm = ChatOllama(model=\"mixtral\")\n",
    "\n",
    "# --- NLTK Data Path Configuration ---\n",
    "ensure_nltk_data()  # No need to store the path, as it's appended to nltk.data.path\n",
    "\n",
    "# --- Document Loading with Error Handling ---\n",
    "document_path = \"./docs/designdoc.docx\"\n",
    "document = None\n",
    "\n",
    "try:\n",
    "    loader = UnstructuredWordDocumentLoader(document_path)\n",
    "    document = loader.load()\n",
    "    print(f\"--- Successfully loaded document: {document_path} ---\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- FATAL ERROR: Input document not found at '{document_path}' ---\")\n",
    "    print(\"Please ensure the 'designdoc.docx' file exists in a 'docs' subdirectory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"--- FATAL ERROR: An unexpected error occurred while loading the document. ---\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit()\n",
    "\n",
    "if not document or not document[0].page_content:\n",
    "    print(\"--- FATAL ERROR: Document loaded but is empty. ---\")\n",
    "    exit()\n",
    "\n",
    "# --- Prompt Engineering ---\n",
    "prompt_template = \"\"\"\n",
    "You are an expert cybersecurity architect specializing in threat modeling.\n",
    "Your task is to read the provided system design document and extract the core components\n",
    "for a Data Flow Diagram (DFD) in a valid JSON format.\n",
    "\n",
    "The JSON output must contain three keys: 'assets', 'processes', and 'data_flows'.\n",
    "- An 'asset' is a data store where data rests (e.g., a database, a cache, a log file). List as an array of strings.\n",
    "- A 'process' is a component that acts on or transforms data (e.g., an API, a microservice, a user-facing application). List as an array of strings.\n",
    "- A 'data_flow' is an array of objects, each with 'source', 'destination', and 'data_description'. The 'source' and 'destination'\n",
    "  must be one of the previously identified processes or assets.\n",
    "\n",
    "System Design Document:\n",
    "---\n",
    "{document_text}\n",
    "---\n",
    "\n",
    "Now, generate the JSON object based on the document. Output ONLY the JSON object itself, with no\n",
    "additional commentary, explanations, or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# --- Chain Construction with JSON Output Parser ---\n",
    "output_parser = JsonOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# --- Invocation and Output ---\n",
    "print(\"\\n--- Invoking Local LLM Chain (Mixtral) to extract DFD components ---\")\n",
    "try:\n",
    "    response_dict = chain.invoke({\"document_text\": document[0].page_content})\n",
    "    print(\"\\n--- LLM Output (Parsed JSON) ---\")\n",
    "    print(json.dumps(response_dict, indent=2))\n",
    "    print(\"\\n--- JSON Validation: Success ---\")\n",
    "    print(\"The LLM output was successfully parsed into a dictionary.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An error occurred during chain invocation or parsing ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This may be due to the LLM not returning a well-formed JSON object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58d2a7-0a06-48fc-bdae-b7efe4f3b825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb0223-adbe-438a-b6b3-eb6ce3a9f852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47710-d7ff-4428-8e2e-ed52320ff48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
